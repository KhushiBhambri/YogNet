{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhushiBhambri/YogNet/blob/55_Khushi/RGBStream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "AnGMhJE6eqZ6",
        "outputId": "0d0ae7bd-43c6-4492-92a4-3323d908011c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a85bc942d03>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# This will prompt for authorization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "\n",
        "################### RUN THIS CELL\n",
        "\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import initializers\n",
        "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
        "from keras.models import load_model\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/MyDrive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os,cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data_path = '../MyDrive/MyDrive/Videos'\n",
        "\n",
        "\n",
        "labels=[]\n",
        "for folder in os.listdir(data_path):\n",
        "    labels.append(folder)\n",
        "labels.sort() #len = 107\n",
        "\n",
        "\n",
        "\n",
        "train_videos=[]\n",
        "train_labels=[]\n",
        "import traceback\n",
        "import random\n",
        "\n",
        "num = 1\n",
        "\n",
        "for i,folder in enumerate(labels):\n",
        "    try:\n",
        "      for video in os.listdir(data_path+'/'+folder):\n",
        "        print(num)\n",
        "        num+=1\n",
        "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Select a random starting frame number\n",
        "        start_frame = random.randint(0, total_frames - 16)\n",
        "\n",
        "        # Set the number of frames to be selected\n",
        "        num_frames = 16\n",
        "\n",
        "        # Set the frame number to start with\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        # Loop through the frames and save them\n",
        "        frames = []\n",
        "        for j in range(num_frames):\n",
        "            ret, image = cap.read()\n",
        "            if ret:\n",
        "              frames.append(cv2.resize(image,(112,112)))\n",
        "\n",
        "        # Release the video file\n",
        "        cap.release()\n",
        "\n",
        "        train_videos.append(frames)\n",
        "        train_labels.append(i)\n",
        "\n",
        "    except Exception:\n",
        "      traceback.print_exc()\n",
        "train_videos = np.asarray(train_videos)\n",
        "train_labels = np.asarray(train_labels).astype('int64')\n",
        "\n",
        "# import numpy as np\n",
        "# arr_reshaped = train_videos.reshape(train_videos.shape[0], -1) #1151\n",
        "# np.savetxt(\"dataset.txt\", arr_reshaped)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
        "\n",
        "input_shape =(32,16,112,112,3)\n",
        "output_shape = 20\n",
        "\n",
        "tf.keras.regularizers.L1L2(\n",
        "    l1=0.01, l2=0.1\n",
        ")\n",
        "\n",
        "RGBmodel = Sequential()\n",
        "\n",
        "# model.add(RandomFlip(\"horizontal\"))\n",
        "# model.add(RandomRotation(0.1))\n",
        "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
        "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
        "\n",
        "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
        "\n",
        "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "RGBmodel.add(Flatten())\n",
        "\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
        "\n",
        "\n",
        "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
        "\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.0001)s\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
        "\n",
        "Print the summary of the model\n",
        "RGBmodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################### RUN THIS CELL\n",
        "\n",
        "############ Manaaaaan\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import initializers\n",
        "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
        "from keras.models import load_model\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/MyDrive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os,cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data_path = '../MyDrive/MyDrive/Videos'\n",
        "\n",
        "\n",
        "labels=[]\n",
        "for folder in os.listdir(data_path):\n",
        "    labels.append(folder)\n",
        "labels.sort() #len = 107\n",
        "\n",
        "\n",
        "\n",
        "train_videos=[]\n",
        "train_labels=[]\n",
        "import traceback\n",
        "import random\n",
        "\n",
        "num = 1\n",
        "\n",
        "for i,folder in enumerate(labels):\n",
        "    try:\n",
        "      for video in os.listdir(data_path+'/'+folder):\n",
        "        print(num)\n",
        "        num+=1\n",
        "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Select a random starting frame number\n",
        "        start_frame = random.randint(0, total_frames - 16)\n",
        "\n",
        "        # Set the number of frames to be selected\n",
        "        num_frames = 16\n",
        "\n",
        "        # Set the frame number to start with\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        # Loop through the frames and save them\n",
        "        frames = []\n",
        "        for j in range(num_frames):\n",
        "            ret, image = cap.read()\n",
        "            if ret:\n",
        "              frames.append(cv2.resize(image,(112,112)))\n",
        "\n",
        "        # Release the video file\n",
        "        cap.release()\n",
        "\n",
        "        train_videos.append(frames)\n",
        "        train_labels.append(i)\n",
        "\n",
        "    except Exception:\n",
        "      traceback.print_exc()\n",
        "train_videos = np.asarray(train_videos)\n",
        "train_labels = np.asarray(train_labels).astype('int64')\n",
        "\n",
        "# import numpy as np\n",
        "# arr_reshaped = train_videos.reshape(train_videos.shape[0], -1) #1151\n",
        "# np.savetxt(\"dataset.txt\", arr_reshaped)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
        "\n",
        "input_shape =(32,16,112,112,3)\n",
        "output_shape = 20\n",
        "\n",
        "tf.keras.regularizers.L1L2(\n",
        "    l1=0.01, l2=0.1\n",
        ")\n",
        "\n",
        "RGBmodel = Sequential()\n",
        "\n",
        "# model.add(RandomFlip(\"horizontal\"))\n",
        "# model.add(RandomRotation(0.1))\n",
        "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
        "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
        "\n",
        "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
        "\n",
        "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "RGBmodel.add(Flatten())\n",
        "\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
        "\n",
        "\n",
        "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# define your model here\n",
        "\n",
        "\n",
        "# set up the checkpointing callback\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", save_weights_only=True)\n",
        "\n",
        "# train the model for 100 epochs and save the weights\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "# load the saved weights and continue training for another 100 epochs\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "# repeat the previous step until you have trained for 500 epochs\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "opt = SGD(learning_rate=0.0001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "\n",
        "# Print the summary of the model\n",
        "RGBmodel.summary()"
      ],
      "metadata": {
        "id": "HoCLpQgMlXBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6779329d-5ac9-4a83-9052-beee4ae81cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /MyDrive; to attempt to forcibly remount, call drive.mount(\"/MyDrive\", force_remount=True).\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "Epoch 1/100\n",
            "22/22 [==============================] - 84s 3s/step - loss: 2311.3818 - accuracy: 0.0536 - val_loss: 2291.4685 - val_accuracy: 0.0578\n",
            "Epoch 2/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2274.4072 - accuracy: 0.0449 - val_loss: 2254.9414 - val_accuracy: 0.0347\n",
            "Epoch 3/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2238.0171 - accuracy: 0.0580 - val_loss: 2218.7317 - val_accuracy: 0.0347\n",
            "Epoch 4/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2201.9360 - accuracy: 0.0565 - val_loss: 2182.8433 - val_accuracy: 0.0347\n",
            "Epoch 5/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2166.1821 - accuracy: 0.0507 - val_loss: 2147.2271 - val_accuracy: 0.0462\n",
            "Epoch 6/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2130.7161 - accuracy: 0.0536 - val_loss: 2111.9294 - val_accuracy: 0.0347\n",
            "Epoch 7/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2095.5471 - accuracy: 0.0580 - val_loss: 2076.9312 - val_accuracy: 0.0636\n",
            "Epoch 8/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2060.7002 - accuracy: 0.0551 - val_loss: 2042.2335 - val_accuracy: 0.0289\n",
            "Epoch 9/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 2026.1377 - accuracy: 0.0565 - val_loss: 2007.8280 - val_accuracy: 0.0694\n",
            "Epoch 10/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1991.8665 - accuracy: 0.0565 - val_loss: 1973.7341 - val_accuracy: 0.0462\n",
            "Epoch 11/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1957.9039 - accuracy: 0.0551 - val_loss: 1939.9353 - val_accuracy: 0.0520\n",
            "Epoch 12/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1924.2440 - accuracy: 0.0652 - val_loss: 1906.4321 - val_accuracy: 0.0405\n",
            "Epoch 13/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1890.8931 - accuracy: 0.0565 - val_loss: 1873.2285 - val_accuracy: 0.0347\n",
            "Epoch 14/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1857.8158 - accuracy: 0.0681 - val_loss: 1840.3237 - val_accuracy: 0.0578\n",
            "Epoch 15/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1825.0497 - accuracy: 0.0681 - val_loss: 1807.7209 - val_accuracy: 0.0520\n",
            "Epoch 16/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1792.5936 - accuracy: 0.0652 - val_loss: 1775.4183 - val_accuracy: 0.0578\n",
            "Epoch 17/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1760.4374 - accuracy: 0.0754 - val_loss: 1743.4086 - val_accuracy: 0.0462\n",
            "Epoch 18/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1728.5560 - accuracy: 0.0739 - val_loss: 1711.7102 - val_accuracy: 0.0405\n",
            "Epoch 19/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1696.9969 - accuracy: 0.0464 - val_loss: 1680.3036 - val_accuracy: 0.0751\n",
            "Epoch 20/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1665.7151 - accuracy: 0.0638 - val_loss: 1649.1960 - val_accuracy: 0.0578\n",
            "Epoch 21/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1634.7698 - accuracy: 0.0638 - val_loss: 1618.3958 - val_accuracy: 0.0520\n",
            "Epoch 22/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1604.0966 - accuracy: 0.0667 - val_loss: 1587.8870 - val_accuracy: 0.0636\n",
            "Epoch 23/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1573.7277 - accuracy: 0.0725 - val_loss: 1557.6785 - val_accuracy: 0.0578\n",
            "Epoch 24/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1543.6625 - accuracy: 0.0681 - val_loss: 1527.7699 - val_accuracy: 0.0462\n",
            "Epoch 25/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1513.9089 - accuracy: 0.0507 - val_loss: 1498.1674 - val_accuracy: 0.0520\n",
            "Epoch 26/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1484.4469 - accuracy: 0.0710 - val_loss: 1468.8704 - val_accuracy: 0.0751\n",
            "Epoch 27/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1455.2773 - accuracy: 0.0783 - val_loss: 1439.8655 - val_accuracy: 0.0636\n",
            "Epoch 28/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1426.4221 - accuracy: 0.0580 - val_loss: 1411.1594 - val_accuracy: 0.0809\n",
            "Epoch 29/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1397.8495 - accuracy: 0.0826 - val_loss: 1382.7565 - val_accuracy: 0.0578\n",
            "Epoch 30/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1369.5844 - accuracy: 0.0725 - val_loss: 1354.6532 - val_accuracy: 0.0636\n",
            "Epoch 31/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1341.6190 - accuracy: 0.0797 - val_loss: 1326.8490 - val_accuracy: 0.0694\n",
            "Epoch 32/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 1313.9639 - accuracy: 0.0725 - val_loss: 1299.3488 - val_accuracy: 0.0751\n",
            "Epoch 33/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1286.6088 - accuracy: 0.0696 - val_loss: 1272.1453 - val_accuracy: 0.0867\n",
            "Epoch 34/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1259.5482 - accuracy: 0.0754 - val_loss: 1245.2443 - val_accuracy: 0.0867\n",
            "Epoch 35/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1232.7712 - accuracy: 0.0812 - val_loss: 1218.6427 - val_accuracy: 0.0867\n",
            "Epoch 36/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1206.3198 - accuracy: 0.0841 - val_loss: 1192.3400 - val_accuracy: 0.0751\n",
            "Epoch 37/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1180.1490 - accuracy: 0.1014 - val_loss: 1166.3374 - val_accuracy: 0.0578\n",
            "Epoch 38/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1154.2997 - accuracy: 0.0855 - val_loss: 1140.6366 - val_accuracy: 0.0809\n",
            "Epoch 39/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1128.7375 - accuracy: 0.0826 - val_loss: 1115.2340 - val_accuracy: 0.0751\n",
            "Epoch 40/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1103.4677 - accuracy: 0.0870 - val_loss: 1090.1294 - val_accuracy: 0.0809\n",
            "Epoch 41/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1078.4919 - accuracy: 0.0870 - val_loss: 1065.3273 - val_accuracy: 0.1040\n",
            "Epoch 42/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 1053.8527 - accuracy: 0.0870 - val_loss: 1040.8296 - val_accuracy: 0.0925\n",
            "Epoch 43/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 1029.4916 - accuracy: 0.0754 - val_loss: 1016.6238 - val_accuracy: 0.1098\n",
            "Epoch 44/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 1005.4167 - accuracy: 0.0783 - val_loss: 992.7164 - val_accuracy: 0.1098\n",
            "Epoch 45/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 981.6546 - accuracy: 0.0986 - val_loss: 969.1083 - val_accuracy: 0.1272\n",
            "Epoch 46/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 958.1772 - accuracy: 0.1174 - val_loss: 945.8020 - val_accuracy: 0.1156\n",
            "Epoch 47/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 935.0189 - accuracy: 0.1043 - val_loss: 922.7905 - val_accuracy: 0.1272\n",
            "Epoch 48/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 912.1718 - accuracy: 0.0841 - val_loss: 900.0787 - val_accuracy: 0.1214\n",
            "Epoch 49/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 889.5881 - accuracy: 0.0884 - val_loss: 877.6709 - val_accuracy: 0.1098\n",
            "Epoch 50/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 867.3101 - accuracy: 0.1101 - val_loss: 855.5562 - val_accuracy: 0.1329\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 845.3549 - accuracy: 0.1000 - val_loss: 833.7490 - val_accuracy: 0.1445\n",
            "Epoch 52/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 823.6778 - accuracy: 0.1130 - val_loss: 812.2393 - val_accuracy: 0.1272\n",
            "Epoch 53/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 802.3036 - accuracy: 0.1188 - val_loss: 791.0123 - val_accuracy: 0.1098\n",
            "Epoch 54/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 781.2461 - accuracy: 0.1087 - val_loss: 770.1003 - val_accuracy: 0.1156\n",
            "Epoch 55/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 760.4551 - accuracy: 0.1290 - val_loss: 749.4651 - val_accuracy: 0.1214\n",
            "Epoch 56/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 739.9889 - accuracy: 0.1290 - val_loss: 729.1372 - val_accuracy: 0.1214\n",
            "Epoch 57/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 719.8002 - accuracy: 0.1319 - val_loss: 709.1282 - val_accuracy: 0.1272\n",
            "Epoch 58/100\n",
            "22/22 [==============================] - 47s 2s/step - loss: 699.9155 - accuracy: 0.1406 - val_loss: 689.3815 - val_accuracy: 0.1503\n",
            "Epoch 59/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 680.3352 - accuracy: 0.1652 - val_loss: 669.9341 - val_accuracy: 0.1329\n",
            "Epoch 60/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 661.0383 - accuracy: 0.1290 - val_loss: 650.8246 - val_accuracy: 0.1965\n",
            "Epoch 61/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 642.1033 - accuracy: 0.1246 - val_loss: 632.0315 - val_accuracy: 0.1445\n",
            "Epoch 62/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 623.3806 - accuracy: 0.1435 - val_loss: 613.5479 - val_accuracy: 0.1618\n",
            "Epoch 63/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 605.0329 - accuracy: 0.1667 - val_loss: 595.2570 - val_accuracy: 0.1618\n",
            "Epoch 64/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 586.9357 - accuracy: 0.1783 - val_loss: 577.3138 - val_accuracy: 0.1561\n",
            "Epoch 65/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 569.1627 - accuracy: 0.1783 - val_loss: 559.6879 - val_accuracy: 0.1214\n",
            "Epoch 66/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 551.6685 - accuracy: 0.1826 - val_loss: 542.4105 - val_accuracy: 0.2139\n",
            "Epoch 67/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 534.4785 - accuracy: 0.1870 - val_loss: 525.3145 - val_accuracy: 0.2428\n",
            "Epoch 68/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 517.6304 - accuracy: 0.1899 - val_loss: 508.6920 - val_accuracy: 0.1965\n",
            "Epoch 69/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 500.9989 - accuracy: 0.2232 - val_loss: 492.3513 - val_accuracy: 0.1098\n",
            "Epoch 70/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 484.7809 - accuracy: 0.1899 - val_loss: 476.2034 - val_accuracy: 0.1792\n",
            "Epoch 71/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 468.7554 - accuracy: 0.2377 - val_loss: 460.3888 - val_accuracy: 0.1618\n",
            "Epoch 72/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 453.1233 - accuracy: 0.2174 - val_loss: 444.7850 - val_accuracy: 0.3006\n",
            "Epoch 73/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 437.7193 - accuracy: 0.2362 - val_loss: 429.7722 - val_accuracy: 0.1850\n",
            "Epoch 74/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 422.6374 - accuracy: 0.2362 - val_loss: 414.7770 - val_accuracy: 0.2428\n",
            "Epoch 75/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 407.9064 - accuracy: 0.2507 - val_loss: 400.2524 - val_accuracy: 0.1792\n",
            "Epoch 76/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 393.3943 - accuracy: 0.2768 - val_loss: 386.0368 - val_accuracy: 0.1272\n",
            "Epoch 77/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 379.2628 - accuracy: 0.2565 - val_loss: 371.8065 - val_accuracy: 0.2486\n",
            "Epoch 78/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 365.4177 - accuracy: 0.2696 - val_loss: 358.4207 - val_accuracy: 0.1965\n",
            "Epoch 79/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 351.8518 - accuracy: 0.2884 - val_loss: 344.6989 - val_accuracy: 0.2717\n",
            "Epoch 80/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 338.5795 - accuracy: 0.2754 - val_loss: 331.6960 - val_accuracy: 0.2543\n",
            "Epoch 81/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 325.6340 - accuracy: 0.2623 - val_loss: 318.9985 - val_accuracy: 0.2197\n",
            "Epoch 82/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 312.9617 - accuracy: 0.2986 - val_loss: 306.5136 - val_accuracy: 0.2775\n",
            "Epoch 83/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 300.6145 - accuracy: 0.2942 - val_loss: 294.2588 - val_accuracy: 0.2775\n",
            "Epoch 84/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 288.5797 - accuracy: 0.2725 - val_loss: 282.2340 - val_accuracy: 0.3295\n",
            "Epoch 85/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 276.8133 - accuracy: 0.3130 - val_loss: 270.7363 - val_accuracy: 0.2948\n",
            "Epoch 86/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 265.3932 - accuracy: 0.3072 - val_loss: 259.4830 - val_accuracy: 0.3179\n",
            "Epoch 87/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 254.2301 - accuracy: 0.3029 - val_loss: 248.3571 - val_accuracy: 0.3988\n",
            "Epoch 88/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 243.3923 - accuracy: 0.3130 - val_loss: 237.7254 - val_accuracy: 0.3815\n",
            "Epoch 89/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 232.8781 - accuracy: 0.3000 - val_loss: 227.4118 - val_accuracy: 0.2659\n",
            "Epoch 90/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 222.6311 - accuracy: 0.3043 - val_loss: 217.3257 - val_accuracy: 0.3295\n",
            "Epoch 91/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 212.7030 - accuracy: 0.3072 - val_loss: 207.4452 - val_accuracy: 0.3642\n",
            "Epoch 92/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 203.0408 - accuracy: 0.3116 - val_loss: 198.0988 - val_accuracy: 0.3353\n",
            "Epoch 93/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 193.7088 - accuracy: 0.2971 - val_loss: 189.1293 - val_accuracy: 0.2081\n",
            "Epoch 94/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 184.6908 - accuracy: 0.3101 - val_loss: 180.2527 - val_accuracy: 0.2312\n",
            "Epoch 95/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 175.9477 - accuracy: 0.3145 - val_loss: 171.7884 - val_accuracy: 0.2139\n",
            "Epoch 96/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 167.5706 - accuracy: 0.3232 - val_loss: 163.2959 - val_accuracy: 0.2832\n",
            "Epoch 97/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 159.4014 - accuracy: 0.3232 - val_loss: 155.4776 - val_accuracy: 0.3353\n",
            "Epoch 98/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 151.6334 - accuracy: 0.3304 - val_loss: 147.8121 - val_accuracy: 0.2312\n",
            "Epoch 99/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 144.0332 - accuracy: 0.3507 - val_loss: 140.5321 - val_accuracy: 0.2832\n",
            "Epoch 100/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 136.8503 - accuracy: 0.3246 - val_loss: 133.3329 - val_accuracy: 0.2717\n",
            "Epoch 1/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 130.0090 - accuracy: 0.2783 - val_loss: 126.4009 - val_accuracy: 0.3584\n",
            "Epoch 2/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 123.2372 - accuracy: 0.3522 - val_loss: 120.3018 - val_accuracy: 0.2717\n",
            "Epoch 3/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 117.0390 - accuracy: 0.3130 - val_loss: 113.8132 - val_accuracy: 0.2948\n",
            "Epoch 4/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 110.9769 - accuracy: 0.3304 - val_loss: 107.9664 - val_accuracy: 0.3468\n",
            "Epoch 5/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 105.3014 - accuracy: 0.3319 - val_loss: 102.3759 - val_accuracy: 0.3642\n",
            "Epoch 6/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 99.8651 - accuracy: 0.3406 - val_loss: 97.0943 - val_accuracy: 0.3584\n",
            "Epoch 7/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 94.7989 - accuracy: 0.3333 - val_loss: 92.4999 - val_accuracy: 0.2543\n",
            "Epoch 8/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 89.9896 - accuracy: 0.3159 - val_loss: 87.9299 - val_accuracy: 0.2543\n",
            "Epoch 9/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 85.4994 - accuracy: 0.3145 - val_loss: 83.5261 - val_accuracy: 0.3006\n",
            "Epoch 10/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 81.3457 - accuracy: 0.3391 - val_loss: 79.7727 - val_accuracy: 0.2139\n",
            "Epoch 11/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 77.4228 - accuracy: 0.3130 - val_loss: 75.8927 - val_accuracy: 0.3237\n",
            "Epoch 12/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 73.9084 - accuracy: 0.3232 - val_loss: 72.7192 - val_accuracy: 0.2312\n",
            "Epoch 13/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 70.6815 - accuracy: 0.3058 - val_loss: 69.2429 - val_accuracy: 0.3006\n",
            "Epoch 14/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 67.6085 - accuracy: 0.3014 - val_loss: 66.6828 - val_accuracy: 0.2717\n",
            "Epoch 15/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 64.9937 - accuracy: 0.3348 - val_loss: 64.3047 - val_accuracy: 0.2139\n",
            "Epoch 16/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 62.7272 - accuracy: 0.2957 - val_loss: 61.3256 - val_accuracy: 0.3757\n",
            "Epoch 17/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 60.5420 - accuracy: 0.3116 - val_loss: 59.9785 - val_accuracy: 0.2312\n",
            "Epoch 18/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 58.8452 - accuracy: 0.2942 - val_loss: 58.5628 - val_accuracy: 0.1561\n",
            "Epoch 19/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 57.4588 - accuracy: 0.2667 - val_loss: 57.0352 - val_accuracy: 0.2312\n",
            "Epoch 20/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 56.2162 - accuracy: 0.2812 - val_loss: 56.0968 - val_accuracy: 0.2254\n",
            "Epoch 21/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 55.3842 - accuracy: 0.2623 - val_loss: 55.1260 - val_accuracy: 0.3064\n",
            "Epoch 22/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 54.6874 - accuracy: 0.2391 - val_loss: 54.5941 - val_accuracy: 0.2543\n",
            "Epoch 23/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 54.2727 - accuracy: 0.2261 - val_loss: 54.0152 - val_accuracy: 0.2832\n",
            "Epoch 24/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.9718 - accuracy: 0.2594 - val_loss: 54.1197 - val_accuracy: 0.2254\n",
            "Epoch 25/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.7900 - accuracy: 0.2478 - val_loss: 53.7820 - val_accuracy: 0.3064\n",
            "Epoch 26/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.6245 - accuracy: 0.2435 - val_loss: 54.1263 - val_accuracy: 0.1965\n",
            "Epoch 27/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.4816 - accuracy: 0.2348 - val_loss: 53.5645 - val_accuracy: 0.3121\n",
            "Epoch 28/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 53.3589 - accuracy: 0.2725 - val_loss: 53.5595 - val_accuracy: 0.2659\n",
            "Epoch 29/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.3289 - accuracy: 0.2406 - val_loss: 53.3004 - val_accuracy: 0.3410\n",
            "Epoch 30/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.1876 - accuracy: 0.2609 - val_loss: 53.5510 - val_accuracy: 0.2832\n",
            "Epoch 31/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.0845 - accuracy: 0.2725 - val_loss: 53.3050 - val_accuracy: 0.3237\n",
            "Epoch 32/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 53.1116 - accuracy: 0.2609 - val_loss: 52.9938 - val_accuracy: 0.2832\n",
            "Epoch 33/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.9207 - accuracy: 0.2696 - val_loss: 53.1535 - val_accuracy: 0.2659\n",
            "Epoch 34/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.9016 - accuracy: 0.2638 - val_loss: 53.1408 - val_accuracy: 0.2023\n",
            "Epoch 35/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.8296 - accuracy: 0.2783 - val_loss: 52.8797 - val_accuracy: 0.2370\n",
            "Epoch 36/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 52.7388 - accuracy: 0.2797 - val_loss: 52.8965 - val_accuracy: 0.2428\n",
            "Epoch 37/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.6269 - accuracy: 0.2957 - val_loss: 52.8481 - val_accuracy: 0.2601\n",
            "Epoch 38/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.5855 - accuracy: 0.2986 - val_loss: 53.6333 - val_accuracy: 0.2139\n",
            "Epoch 39/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.6392 - accuracy: 0.2957 - val_loss: 53.0312 - val_accuracy: 0.2428\n",
            "Epoch 40/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.5414 - accuracy: 0.2710 - val_loss: 52.4115 - val_accuracy: 0.3295\n",
            "Epoch 41/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.4197 - accuracy: 0.3029 - val_loss: 52.5679 - val_accuracy: 0.3410\n",
            "Epoch 42/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 52.3479 - accuracy: 0.2928 - val_loss: 52.3270 - val_accuracy: 0.2890\n",
            "Epoch 43/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.3255 - accuracy: 0.2812 - val_loss: 52.4475 - val_accuracy: 0.3006\n",
            "Epoch 44/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.2336 - accuracy: 0.3130 - val_loss: 52.5077 - val_accuracy: 0.2717\n",
            "Epoch 45/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 52.1902 - accuracy: 0.2435 - val_loss: 52.3002 - val_accuracy: 0.2832\n",
            "Epoch 46/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.0708 - accuracy: 0.3043 - val_loss: 52.3513 - val_accuracy: 0.2543\n",
            "Epoch 47/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.0628 - accuracy: 0.3217 - val_loss: 52.0186 - val_accuracy: 0.2890\n",
            "Epoch 48/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 52.0153 - accuracy: 0.3203 - val_loss: 52.0988 - val_accuracy: 0.3410\n",
            "Epoch 49/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.9704 - accuracy: 0.3377 - val_loss: 52.0104 - val_accuracy: 0.3179\n",
            "Epoch 50/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.9665 - accuracy: 0.2884 - val_loss: 51.9498 - val_accuracy: 0.3410\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.8082 - accuracy: 0.3290 - val_loss: 51.9370 - val_accuracy: 0.2486\n",
            "Epoch 52/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 51.7253 - accuracy: 0.3174 - val_loss: 52.0975 - val_accuracy: 0.2543\n",
            "Epoch 53/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 51.7847 - accuracy: 0.3188 - val_loss: 51.6927 - val_accuracy: 0.2832\n",
            "Epoch 54/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.6662 - accuracy: 0.3348 - val_loss: 51.7778 - val_accuracy: 0.2543\n",
            "Epoch 55/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 51.6358 - accuracy: 0.3159 - val_loss: 51.7098 - val_accuracy: 0.3815\n",
            "Epoch 56/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.5880 - accuracy: 0.3087 - val_loss: 51.8027 - val_accuracy: 0.2717\n",
            "Epoch 57/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.4711 - accuracy: 0.3565 - val_loss: 51.6941 - val_accuracy: 0.3757\n",
            "Epoch 58/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.4723 - accuracy: 0.3435 - val_loss: 51.4817 - val_accuracy: 0.3526\n",
            "Epoch 59/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.4153 - accuracy: 0.3000 - val_loss: 51.8407 - val_accuracy: 0.2428\n",
            "Epoch 60/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 51.3876 - accuracy: 0.3304 - val_loss: 51.3303 - val_accuracy: 0.4162\n",
            "Epoch 61/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 51.2774 - accuracy: 0.3217 - val_loss: 51.3834 - val_accuracy: 0.2717\n",
            "Epoch 62/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.2672 - accuracy: 0.3058 - val_loss: 51.1648 - val_accuracy: 0.3295\n",
            "Epoch 63/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.1698 - accuracy: 0.3478 - val_loss: 51.3669 - val_accuracy: 0.3353\n",
            "Epoch 64/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.1288 - accuracy: 0.3493 - val_loss: 51.3584 - val_accuracy: 0.3988\n",
            "Epoch 65/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.0382 - accuracy: 0.3710 - val_loss: 51.3198 - val_accuracy: 0.2543\n",
            "Epoch 66/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.0910 - accuracy: 0.3725 - val_loss: 50.9786 - val_accuracy: 0.3642\n",
            "Epoch 67/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.0238 - accuracy: 0.3783 - val_loss: 51.0506 - val_accuracy: 0.3006\n",
            "Epoch 68/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.9994 - accuracy: 0.3507 - val_loss: 50.9235 - val_accuracy: 0.3237\n",
            "Epoch 69/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.9008 - accuracy: 0.3522 - val_loss: 50.9892 - val_accuracy: 0.3757\n",
            "Epoch 70/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 50.8808 - accuracy: 0.3594 - val_loss: 51.1947 - val_accuracy: 0.3353\n",
            "Epoch 71/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.7670 - accuracy: 0.3928 - val_loss: 50.7273 - val_accuracy: 0.4104\n",
            "Epoch 72/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.7659 - accuracy: 0.3667 - val_loss: 50.6971 - val_accuracy: 0.3179\n",
            "Epoch 73/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 50.6998 - accuracy: 0.3420 - val_loss: 51.0815 - val_accuracy: 0.3353\n",
            "Epoch 74/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 50.6894 - accuracy: 0.3710 - val_loss: 50.7402 - val_accuracy: 0.4104\n",
            "Epoch 75/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.6473 - accuracy: 0.3783 - val_loss: 50.6821 - val_accuracy: 0.3931\n",
            "Epoch 76/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.5516 - accuracy: 0.3884 - val_loss: 50.7310 - val_accuracy: 0.4104\n",
            "Epoch 77/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.5544 - accuracy: 0.3638 - val_loss: 50.5029 - val_accuracy: 0.3699\n",
            "Epoch 78/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.4096 - accuracy: 0.3986 - val_loss: 50.7027 - val_accuracy: 0.3410\n",
            "Epoch 79/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.4315 - accuracy: 0.3841 - val_loss: 50.5072 - val_accuracy: 0.4335\n",
            "Epoch 80/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.3736 - accuracy: 0.4072 - val_loss: 50.3844 - val_accuracy: 0.3757\n",
            "Epoch 81/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 50.3416 - accuracy: 0.4217 - val_loss: 50.5311 - val_accuracy: 0.3468\n",
            "Epoch 82/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.2810 - accuracy: 0.4043 - val_loss: 50.3916 - val_accuracy: 0.3757\n",
            "Epoch 83/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.2355 - accuracy: 0.3667 - val_loss: 50.2092 - val_accuracy: 0.4104\n",
            "Epoch 84/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.2242 - accuracy: 0.3812 - val_loss: 50.1998 - val_accuracy: 0.3584\n",
            "Epoch 85/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.0870 - accuracy: 0.3768 - val_loss: 50.2966 - val_accuracy: 0.3237\n",
            "Epoch 86/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.0336 - accuracy: 0.4058 - val_loss: 50.2143 - val_accuracy: 0.3699\n",
            "Epoch 87/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.0552 - accuracy: 0.4145 - val_loss: 50.2542 - val_accuracy: 0.3468\n",
            "Epoch 88/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 50.0218 - accuracy: 0.4116 - val_loss: 50.2611 - val_accuracy: 0.3468\n",
            "Epoch 89/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.9393 - accuracy: 0.4116 - val_loss: 49.8973 - val_accuracy: 0.3815\n",
            "Epoch 90/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.9017 - accuracy: 0.3768 - val_loss: 50.0159 - val_accuracy: 0.4162\n",
            "Epoch 91/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.8541 - accuracy: 0.4087 - val_loss: 50.0141 - val_accuracy: 0.3642\n",
            "Epoch 92/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.8645 - accuracy: 0.3986 - val_loss: 49.8969 - val_accuracy: 0.3873\n",
            "Epoch 93/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.7231 - accuracy: 0.4101 - val_loss: 49.8313 - val_accuracy: 0.4046\n",
            "Epoch 94/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.6951 - accuracy: 0.3957 - val_loss: 49.8194 - val_accuracy: 0.3179\n",
            "Epoch 95/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.6641 - accuracy: 0.4058 - val_loss: 49.7172 - val_accuracy: 0.3642\n",
            "Epoch 96/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.5809 - accuracy: 0.4217 - val_loss: 49.4824 - val_accuracy: 0.4509\n",
            "Epoch 97/100\n",
            "22/22 [==============================] - 45s 2s/step - loss: 49.4924 - accuracy: 0.4029 - val_loss: 49.4337 - val_accuracy: 0.4104\n",
            "Epoch 98/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.5284 - accuracy: 0.4290 - val_loss: 49.4707 - val_accuracy: 0.3873\n",
            "Epoch 99/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.4170 - accuracy: 0.4116 - val_loss: 49.5060 - val_accuracy: 0.4393\n",
            "Epoch 100/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.3995 - accuracy: 0.4217 - val_loss: 49.5706 - val_accuracy: 0.3873\n",
            "Epoch 1/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.3897 - accuracy: 0.4159 - val_loss: 49.3081 - val_accuracy: 0.4451\n",
            "Epoch 2/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.2869 - accuracy: 0.4217 - val_loss: 49.3321 - val_accuracy: 0.4220\n",
            "Epoch 3/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.2233 - accuracy: 0.4275 - val_loss: 49.4637 - val_accuracy: 0.3988\n",
            "Epoch 4/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.2107 - accuracy: 0.4348 - val_loss: 49.2697 - val_accuracy: 0.4393\n",
            "Epoch 5/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.1600 - accuracy: 0.4464 - val_loss: 49.4123 - val_accuracy: 0.4335\n",
            "Epoch 6/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.1860 - accuracy: 0.4203 - val_loss: 49.2779 - val_accuracy: 0.4220\n",
            "Epoch 7/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.0612 - accuracy: 0.4551 - val_loss: 49.2160 - val_accuracy: 0.4104\n",
            "Epoch 8/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 49.0578 - accuracy: 0.4275 - val_loss: 49.6326 - val_accuracy: 0.4046\n",
            "Epoch 9/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 49.0070 - accuracy: 0.4449 - val_loss: 48.9365 - val_accuracy: 0.4393\n",
            "Epoch 10/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 48.9483 - accuracy: 0.4435 - val_loss: 49.1399 - val_accuracy: 0.4566\n",
            "Epoch 11/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 48.9002 - accuracy: 0.4435 - val_loss: 49.1865 - val_accuracy: 0.3353\n",
            "Epoch 12/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.9209 - accuracy: 0.4232 - val_loss: 48.9938 - val_accuracy: 0.4277\n",
            "Epoch 13/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.8246 - accuracy: 0.4319 - val_loss: 49.0638 - val_accuracy: 0.3699\n",
            "Epoch 14/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.7624 - accuracy: 0.4507 - val_loss: 49.0479 - val_accuracy: 0.4220\n",
            "Epoch 15/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.7423 - accuracy: 0.4623 - val_loss: 48.8490 - val_accuracy: 0.4104\n",
            "Epoch 16/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.7039 - accuracy: 0.4536 - val_loss: 48.6080 - val_accuracy: 0.4624\n",
            "Epoch 17/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.6574 - accuracy: 0.4449 - val_loss: 48.6755 - val_accuracy: 0.5202\n",
            "Epoch 18/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.5966 - accuracy: 0.4594 - val_loss: 48.6159 - val_accuracy: 0.4220\n",
            "Epoch 19/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.5637 - accuracy: 0.4841 - val_loss: 49.1111 - val_accuracy: 0.3584\n",
            "Epoch 20/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 48.5080 - accuracy: 0.4870 - val_loss: 48.8398 - val_accuracy: 0.4104\n",
            "Epoch 21/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.4931 - accuracy: 0.4754 - val_loss: 48.6677 - val_accuracy: 0.4451\n",
            "Epoch 22/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.4462 - accuracy: 0.4594 - val_loss: 48.5278 - val_accuracy: 0.4509\n",
            "Epoch 23/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.4294 - accuracy: 0.4681 - val_loss: 48.4854 - val_accuracy: 0.5029\n",
            "Epoch 24/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.2488 - accuracy: 0.5101 - val_loss: 48.3719 - val_accuracy: 0.4451\n",
            "Epoch 25/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.2832 - accuracy: 0.4551 - val_loss: 48.2828 - val_accuracy: 0.4451\n",
            "Epoch 26/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.2127 - accuracy: 0.4696 - val_loss: 48.2815 - val_accuracy: 0.4682\n",
            "Epoch 27/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.1124 - accuracy: 0.5072 - val_loss: 48.7723 - val_accuracy: 0.4046\n",
            "Epoch 28/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.1666 - accuracy: 0.4623 - val_loss: 48.4282 - val_accuracy: 0.3584\n",
            "Epoch 29/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 48.0860 - accuracy: 0.4725 - val_loss: 48.2834 - val_accuracy: 0.4624\n",
            "Epoch 30/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.0821 - accuracy: 0.4812 - val_loss: 48.0791 - val_accuracy: 0.4624\n",
            "Epoch 31/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.9984 - accuracy: 0.5043 - val_loss: 48.2310 - val_accuracy: 0.4971\n",
            "Epoch 32/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.0093 - accuracy: 0.4986 - val_loss: 47.9298 - val_accuracy: 0.4566\n",
            "Epoch 33/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.8992 - accuracy: 0.4768 - val_loss: 48.1495 - val_accuracy: 0.4509\n",
            "Epoch 34/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 47.9447 - accuracy: 0.4667 - val_loss: 47.9220 - val_accuracy: 0.4566\n",
            "Epoch 35/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.7874 - accuracy: 0.4899 - val_loss: 48.1063 - val_accuracy: 0.4335\n",
            "Epoch 36/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.8551 - accuracy: 0.4826 - val_loss: 48.1497 - val_accuracy: 0.4046\n",
            "Epoch 37/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.7220 - accuracy: 0.5246 - val_loss: 47.8954 - val_accuracy: 0.3815\n",
            "Epoch 38/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 47.7107 - accuracy: 0.4942 - val_loss: 47.7536 - val_accuracy: 0.4913\n",
            "Epoch 39/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.6501 - accuracy: 0.5130 - val_loss: 47.9500 - val_accuracy: 0.4913\n",
            "Epoch 40/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.6084 - accuracy: 0.5101 - val_loss: 47.5635 - val_accuracy: 0.4855\n",
            "Epoch 41/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.6120 - accuracy: 0.4783 - val_loss: 47.7865 - val_accuracy: 0.4971\n",
            "Epoch 42/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.4659 - accuracy: 0.5101 - val_loss: 47.4251 - val_accuracy: 0.4509\n",
            "Epoch 43/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.4199 - accuracy: 0.4971 - val_loss: 47.7673 - val_accuracy: 0.4624\n",
            "Epoch 44/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.4624 - accuracy: 0.5043 - val_loss: 47.5074 - val_accuracy: 0.4740\n",
            "Epoch 45/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.3599 - accuracy: 0.5087 - val_loss: 47.4310 - val_accuracy: 0.4046\n",
            "Epoch 46/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 47.3492 - accuracy: 0.4768 - val_loss: 47.2478 - val_accuracy: 0.4913\n",
            "Epoch 47/100\n",
            "22/22 [==============================] - 44s 2s/step - loss: 47.2661 - accuracy: 0.4899 - val_loss: 47.4018 - val_accuracy: 0.5145\n",
            "Epoch 48/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.2593 - accuracy: 0.4986 - val_loss: 47.5528 - val_accuracy: 0.4740\n",
            "Epoch 49/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.2242 - accuracy: 0.5261 - val_loss: 47.3604 - val_accuracy: 0.4451\n",
            "Epoch 50/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.1510 - accuracy: 0.5261 - val_loss: 47.3614 - val_accuracy: 0.4855\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 46s 2s/step - loss: 47.1231 - accuracy: 0.5087 - val_loss: 47.2120 - val_accuracy: 0.4740\n",
            "Epoch 52/100\n",
            "20/22 [==========================>...] - ETA: 3s - loss: 47.0196 - accuracy: 0.5281"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RGBmodel = Sequential()\n",
        "\n",
        "# model.add(RandomFlip(\"horizontal\"))\n",
        "# model.add(RandomRotation(0.1))\n",
        "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
        "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
        "\n",
        "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
        "\n",
        "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "RGBmodel.add(Flatten())\n",
        "\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
        "\n",
        "\n",
        "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
        "\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.0001)s\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
        "RGBmodel.save('RGBmodel.h5') "
      ],
      "metadata": {
        "id": "3phz1CUCZfY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# define your model here\n",
        "\n",
        "\n",
        "# set up the checkpointing callback\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", save_weights_only=True)\n",
        "\n",
        "# train the model for 100 epochs and save the weights\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "# load the saved weights and continue training for another 100 epochs\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "# repeat the previous step until you have trained for 500 epochs\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
        "\n",
        "opt = SGD(learning_rate=0.0001)s\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.load_weights(\"model_weights.h5\")\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "id": "UEAbUHYqZOTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PCoZzxq8HB6"
      },
      "outputs": [],
      "source": [
        "RGBmodel.save('RGBmodel.h5') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################### RUN THIS CELL\n",
        "\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import initializers\n",
        "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
        "from keras.models import load_model\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/MyDrive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os,cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data_path = '../MyDrive/MyDrive/Videos'\n",
        "\n",
        "\n",
        "labels=[]\n",
        "for folder in os.listdir(data_path):\n",
        "    labels.append(folder)\n",
        "labels.sort() #len = 107\n",
        "\n",
        "\n",
        "\n",
        "train_videos=[]\n",
        "train_labels=[]\n",
        "import traceback\n",
        "import random\n",
        "\n",
        "num = 1\n",
        "\n",
        "for i,folder in enumerate(labels):\n",
        "    try:\n",
        "      for video in os.listdir(data_path+'/'+folder):\n",
        "        print(num)\n",
        "        num+=1\n",
        "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Select a random starting frame number\n",
        "        start_frame = random.randint(0, total_frames - 16)\n",
        "\n",
        "        # Set the number of frames to be selected\n",
        "        num_frames = 16\n",
        "\n",
        "        # Set the frame number to start with\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        # Loop through the frames and save them\n",
        "        frames = []\n",
        "        for j in range(num_frames):\n",
        "            ret, image = cap.read()\n",
        "            if ret:\n",
        "              frames.append(cv2.resize(image,(112,112)))\n",
        "\n",
        "        # Release the video file\n",
        "        cap.release()\n",
        "\n",
        "        train_videos.append(frames)\n",
        "        train_labels.append(i)\n",
        "\n",
        "    except Exception:\n",
        "      traceback.print_exc()\n",
        "train_videos = np.asarray(train_videos)\n",
        "train_labels = np.asarray(train_labels).astype('int64')\n",
        "\n",
        "# import numpy as np\n",
        "# arr_reshaped = train_videos.reshape(train_videos.shape[0], -1) #1151\n",
        "# np.savetxt(\"dataset.txt\", arr_reshaped)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
        "\n",
        "input_shape =(32,16,112,112,3)\n",
        "output_shape = 20\n",
        "\n",
        "tf.keras.regularizers.L1L2(\n",
        "    l1=0.01, l2=0.1\n",
        ")\n",
        "\n",
        "RGBmodel = Sequential()\n",
        "\n",
        "# model.add(RandomFlip(\"horizontal\"))\n",
        "# model.add(RandomRotation(0.1))\n",
        "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
        "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
        "\n",
        "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
        "\n",
        "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "RGBmodel.add(Flatten())\n",
        "\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
        "\n",
        "\n",
        "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
        "\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.0001)s\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
        "\n",
        "# Print the summary of the model\n",
        "RGBmodel.summary()"
      ],
      "metadata": {
        "id": "wLWUM-aXeYNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAEOyxysewFw"
      },
      "outputs": [],
      "source": [
        "\n",
        "################### RUN THIS CELL\n",
        "\n",
        "############ college gpu\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import initializers\n",
        "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
        "from keras.models import load_model\n",
        "\n",
        "# This will prompt for authorization.\n",
        "# drive.mount('/MyDrive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os,cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# data_path = '../MyDrive/MyDrive/Videos'\n",
        "\n",
        "data_path = \"...\"\n",
        "\n",
        "labels=[]\n",
        "for folder in os.listdir(data_path):\n",
        "    labels.append(folder)\n",
        "labels.sort() #len = 107\n",
        "\n",
        "\n",
        "\n",
        "train_videos=[]\n",
        "train_labels=[]\n",
        "import traceback\n",
        "import random\n",
        "\n",
        "num = 1\n",
        "\n",
        "for i,folder in enumerate(labels):\n",
        "    try:\n",
        "      for video in os.listdir(data_path+'/'+folder):\n",
        "        print(num)\n",
        "        num+=1\n",
        "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video)\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Select a random starting frame number\n",
        "        start_frame = random.randint(0, total_frames - 16)\n",
        "\n",
        "        # Set the number of frames to be selected\n",
        "        num_frames = 16\n",
        "\n",
        "        # Set the frame number to start with\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        # Loop through the frames and save them\n",
        "        frames = []\n",
        "        for j in range(num_frames):\n",
        "            ret, image = cap.read()\n",
        "            if ret:\n",
        "              frames.append(cv2.resize(image,(112,112)))\n",
        "\n",
        "        # Release the video file\n",
        "        cap.release()\n",
        "\n",
        "        train_videos.append(frames)\n",
        "        train_labels.append(i)\n",
        "\n",
        "    except Exception:\n",
        "      traceback.print_exc()\n",
        "train_videos = np.asarray(train_videos)\n",
        "train_labels = np.asarray(train_labels).astype('int64')\n",
        "\n",
        "# import numpy as np\n",
        "# arr_reshaped = train_videos.reshape(train_videos.shape[0], -1) #1151\n",
        "# np.savetxt(\"dataset.txt\", arr_reshaped)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
        "\n",
        "input_shape =(32,16,112,112,3)\n",
        "output_shape = 20\n",
        "\n",
        "tf.keras.regularizers.L1L2(\n",
        "    l1=0.01, l2=0.1\n",
        ")\n",
        "\n",
        "RGBmodel = Sequential()\n",
        "\n",
        "# model.add(RandomFlip(\"horizontal\"))\n",
        "# model.add(RandomRotation(0.1))\n",
        "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
        "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
        "\n",
        "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
        "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
        "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
        "\n",
        "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
        "RGBmodel.add(Flatten())\n",
        "\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
        "RGBmodel.add(Dropout(rate=0.5))\n",
        "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
        "\n",
        "\n",
        "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
        "\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.0001)\n",
        "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
        "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
        "\n",
        "# Print the summary of the model\n",
        "RGBmodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf. __version__) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVbAnyo3p8ak",
        "outputId": "8ef7a828-9d71-492c-9abb-a657da2f70be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ‐‐version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p2n4KtZ2KJu",
        "outputId": "2a644827-ffc7-4018-89df-303829b91013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/‐‐version': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIN9pqD-gx8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}