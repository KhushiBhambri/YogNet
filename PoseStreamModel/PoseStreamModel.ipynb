{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhushiBhambri/YogNet/blob/main/PoseStreamModel/PoseStreamModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mq1eifL7RQCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4cce6d-7c74-46d0-fe09-76f4de6ff9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Major_Project_Cse\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/Major_Project_Cse')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Major_Project_Cse\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numOfKeypoints = 35\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# read the data from the CSV file\n",
        "data = pd.read_csv('PoseStream_train_data.csv')\n",
        "\n",
        "# load the keypoints and labels into separate variables\n",
        "train_keypoints=[]\n",
        "train_labels = list(data['label'])\n",
        "\n",
        "for keypoints_str in data['keypoints']:\n",
        "    keypoints_list = ast.literal_eval(keypoints_str)\n",
        "    train_keypoints.append(keypoints_list)"
      ],
      "metadata": {
        "id": "ofsjDXnca5RZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_keypoints[10]"
      ],
      "metadata": {
        "id": "Slu2qiW9e9jA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import initializers\n",
        "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
        "from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "YlIQnLkKScSg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_keypoints),len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPuAXk9Ktb2e",
        "outputId": "af95d750-55e5-49bd-895a-f5e120eca325"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1213 1213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the center of the object in each frame\n",
        "centers = []\n",
        "for keypoints in train_keypoints:\n",
        "    frame_centers = []\n",
        "    for frame_keypoints in keypoints:\n",
        "        # assume the center is the mean of all keypoints\n",
        "       center_x = np.nanmean([kp[0] for kp in frame_keypoints if kp[0] is not None])\n",
        "       center_y = np.nanmean([kp[1] for kp in frame_keypoints if kp[1] is not None])\n",
        "       frame_centers.append((center_x, center_y))\n",
        "    centers.append(frame_centers)\n",
        "\n",
        "train_relative_keypoints = train_keypoints\n",
        "# make the keypoints relative to the center\n",
        "for i in range(len(train_relative_keypoints)):\n",
        "    for j in range(len(train_relative_keypoints[i])):\n",
        "        for k in range(len(train_relative_keypoints[i][j])):\n",
        "            if train_relative_keypoints[i][j][k][0] is not None:\n",
        "               train_relative_keypoints[i][j][k][0] -= centers[i][j][0]\n",
        "            if train_relative_keypoints[i][j][k][1] is not None:\n",
        "              train_relative_keypoints[i][j][k][1] -= centers[i][j][1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BnE-CJ7rSXT",
        "outputId": "d69932b2-1582-49b7-90b8-d5944eb5a6ac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-a6c48355f099>:7: RuntimeWarning: Mean of empty slice\n",
            "  center_x = np.nanmean([kp[0] for kp in frame_keypoints if kp[0] is not None])\n",
            "<ipython-input-34-a6c48355f099>:8: RuntimeWarning: Mean of empty slice\n",
            "  center_y = np.nanmean([kp[1] for kp in frame_keypoints if kp[1] is not None])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ReplacebyAverage(index,keypoints):\n",
        "  \n",
        "  group_indices = keypoint_groups[index]\n",
        "  group_keypoints = [keypoints[i] for i in group_indices]\n",
        "  valid_keypoints = [k for k in group_keypoints if k is not None]\n",
        "  if len(valid_keypoints) > 0:\n",
        "     group_average_x = np.mean([k[0] for k in valid_keypoints if k[0] is not None])\n",
        "     group_average_y = np.mean([k[1] for k in valid_keypoints if k[1] is not None])\n",
        "    #  keypoints[index][0]=group_average_x\n",
        "    #  keypoints[index][1]=group_average_y\n",
        "  else:\n",
        "    # If all keypoints in the group are missing, replace with (0,0) or another default value\n",
        "    group_average_x, group_average_y = 0, 0\n",
        "  print(group_average_x,group_average_y)\n",
        "  return [group_average_x,group_average_y]"
      ],
      "metadata": {
        "id": "1sbfZ7IbYKan"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "new_train_keypoints = []\n",
        "new_train_labels = []\n",
        "for i in range(len(train_relative_keypoints)):\n",
        "    # flag=1\n",
        "    frames_keypoints=[] \n",
        "    for j in range(len(train_relative_keypoints[i])):\n",
        "        flag=1\n",
        "        for k in range(len(train_relative_keypoints[i][j])):\n",
        "            if train_relative_keypoints[i][j][k][0] is None or train_relative_keypoints[i][j][k][1] is None:\n",
        "              count+=1\n",
        "              flag=0\n",
        "              break\n",
        "        if(flag==1):\n",
        "          frames_keypoints.append(train_relative_keypoints[i][j])\n",
        "    if(len(frames_keypoints)>0):\n",
        "      kk = [[-1.0,-1.0] for _ in range(numOfKeypoints)]\n",
        "      for x in range (16-len(frames_keypoints)):\n",
        "          frames_keypoints.append(kk)\n",
        "      # if(len(frames_keypoints)!=16):\n",
        "      #   print(\"2 : \",len(frames_keypoints))\n",
        "      new_train_keypoints.append(frames_keypoints)\n",
        "      new_train_labels.append(train_labels[i])"
      ],
      "metadata": {
        "id": "52M9wU4uESQP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(new_train_keypoints),len(new_train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThJnaYFbbxl",
        "outputId": "7b48d6a4-2f00-41c3-bcff-ab245144b2d9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1031 1031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_keypoints[0]"
      ],
      "metadata": {
        "id": "egja4TfifLaN",
        "outputId": "620aa7e2-5be4-47eb-f1ef-474dcd8d38de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[-0.029326708827699943, -0.026793485028403174],\n",
              "  [-0.034212203536714836, -0.02457404647554673],\n",
              "  [-0.0346262173993247, -0.02584773812975205],\n",
              "  [-0.034937770877565666, -0.027047460419791114],\n",
              "  [-0.034913213763918205, -0.024447863442557227],\n",
              "  [-0.035796137366976066, -0.025694852215903174],\n",
              "  [-0.03663066199847631, -0.026806061608450782],\n",
              "  [-0.039403291259493156, -0.04115552221025742],\n",
              "  [-0.04029411247798376, -0.041398470742361915],\n",
              "  [-0.028009744201387687, -0.03802711282457627],\n",
              "  [-0.028571100745882316, -0.03803718600954331],\n",
              "  [-0.03417179158755712, -0.06857920203890122],\n",
              "  [-0.0349885540349143, -0.07739919934953965],\n",
              "  [-0.03415009549685888, 0.01500725235257827],\n",
              "  [-0.03543076089450292, -0.005169873578207862],\n",
              "  [-0.038290949378694816, 0.10855608667646133],\n",
              "  [-0.0407183485371726, 0.06685149158750259],\n",
              "  [-0.04855254462787084, 0.11788504804883682],\n",
              "  [-0.05034718087741308, 0.07137697424207412],\n",
              "  [-0.052036555324281974, 0.11120289053235732],\n",
              "  [-0.049935998235430046, 0.07524793829236709],\n",
              "  [-0.04700926116534643, 0.10907345499311172],\n",
              "  [-0.045573146854128166, 0.0752803632191249],\n",
              "  [0.04380646177700587, -0.1716392687388829],\n",
              "  [0.036204366173062996, -0.18421185527529038],\n",
              "  [0.07120677658489771, -0.027203564984457862],\n",
              "  [0.06077286430767603, -0.04535699401582993],\n",
              "  [0.12497624584606715, 0.0729992934635707],\n",
              "  [0.10506281086376734, 0.034033352988106835],\n",
              "  [0.14043840595654078, 0.09185754741941177],\n",
              "  [0.1123149675982339, 0.044763917582375634],\n",
              "  [0.1038921160357339, 0.10605644668851577],\n",
              "  [0.08382609316280909, 0.07011246170316421],\n",
              "  [0.04000541397503443, -0.17792556200708665],\n",
              "  [-0.03458017281123571, -0.07298920069422044]],\n",
              " [[-0.029378457580293893, -0.023542214291436325],\n",
              "  [-0.03454969695636201, -0.022071171658379685],\n",
              "  [-0.0350556807858603, -0.02357732142720903],\n",
              "  [-0.03541473916598725, -0.02492474402700151],\n",
              "  [-0.03503487876483369, -0.02217351283345903],\n",
              "  [-0.035856051955904245, -0.02376948680196489],\n",
              "  [-0.03661815694400239, -0.025160957234246384],\n",
              "  [-0.039654596362795114, -0.04009251679692949],\n",
              "  [-0.0401845710618155, -0.04022567357335771],\n",
              "  [-0.028054936443056344, -0.035178471463067185],\n",
              "  [-0.02843104175158906, -0.03533213223729814],\n",
              "  [-0.03398643306323457, -0.06661306704793657],\n",
              "  [-0.03306208423205781, -0.07679663981710161],\n",
              "  [-0.033872528587068795, 0.014083217723029007],\n",
              "  [-0.0348192887646811, -0.0036121496132441866],\n",
              "  [-0.03881917766162324, 0.10707158957208907],\n",
              "  [-0.04167245200702119, 0.06898571167673384],\n",
              "  [-0.048592104230608224, 0.11182249699320113],\n",
              "  [-0.0510701375348227, 0.07330812130655562],\n",
              "  [-0.05203090480395722, 0.1061157694884709],\n",
              "  [-0.05069659522601533, 0.0773866287299565],\n",
              "  [-0.04761440924235749, 0.10404814396585738],\n",
              "  [-0.04660282901355195, 0.0772272459098271],\n",
              "  [0.04248066374233794, -0.17065738524709428],\n",
              "  [0.03595133253506255, -0.18598585213933672],\n",
              "  [0.07047076651028228, -0.026447761910302292],\n",
              "  [0.061263815845762015, -0.043428350346429],\n",
              "  [0.12404735514095855, 0.07346738491739546],\n",
              "  [0.10269649454525542, 0.032835256201880325],\n",
              "  [0.13932527729443145, 0.09115989123071944],\n",
              "  [0.12014027067593169, 0.02451772604669844],\n",
              "  [0.10428179928234649, 0.10567427788461958],\n",
              "  [0.08472223707607818, 0.07191241894449507],\n",
              "  [0.03921599813870025, -0.1783216186932155],\n",
              "  [-0.03352425864764619, -0.07170485343251909]],\n",
              " [[-0.028994955335344574, -0.025071700981685097],\n",
              "  [-0.03410384825297763, -0.02375461714608329],\n",
              "  [-0.03457949331828525, -0.025257667473384315],\n",
              "  [-0.03492591551371982, -0.026656171253749306],\n",
              "  [-0.034703172956194184, -0.023730119637080604],\n",
              "  [-0.035577573095049164, -0.02521284478051322],\n",
              "  [-0.03641120365687778, -0.026568015984126503],\n",
              "  [-0.03919578364917209, -0.0416664685521807],\n",
              "  [-0.03990945986339023, -0.04146971361977714],\n",
              "  [-0.027703739915575287, -0.036321183613368446],\n",
              "  [-0.02815214565822055, -0.036578020027705604],\n",
              "  [-0.03399149349757602, -0.06923278229577201],\n",
              "  [-0.03437284401484897, -0.07632895367486137],\n",
              "  [-0.03381524256297519, 0.01292000157492501],\n",
              "  [-0.03501717022487094, -0.004628678730555946],\n",
              "  [-0.03859780005046298, 0.10610995633261544],\n",
              "  [-0.04113063982554843, 0.07067773682730538],\n",
              "  [-0.05025441220828464, 0.11448703152792794],\n",
              "  [-0.05104447177478244, 0.07419751031058175],\n",
              "  [-0.05375752619334628, 0.10863206727164132],\n",
              "  [-0.05058337024280002, 0.07779101473944527],\n",
              "  [-0.04859025052615573, 0.10533086402075631],\n",
              "  [-0.045873739038194916, 0.07767073256628854],\n",
              "  [0.04323979445866177, -0.1738456095967974],\n",
              "  [0.03666665383747647, -0.18756755249840873],\n",
              "  [0.07284607716969083, -0.028652748039790565],\n",
              "  [0.06290753909519742, -0.04723223107201713],\n",
              "  [0.12469639607838223, 0.0753308926309858],\n",
              "  [0.1042480094092233, 0.034593621322086876],\n",
              "  [0.13983961173466275, 0.09077940327780587],\n",
              "  [0.11186935731342862, 0.046554604598454064],\n",
              "  [0.10361953803471158, 0.10673777205603463],\n",
              "  [0.08558221885136197, 0.07144931895392281],\n",
              "  [0.03995322414806912, -0.18070658104760307],\n",
              "  [-0.034182168756212494, -0.07278086798531669]],\n",
              " [[-0.02820696660450528, -0.024709746667316956],\n",
              "  [-0.03310319014957974, -0.023395166226795716],\n",
              "  [-0.03354956933430264, -0.024904534646442933],\n",
              "  [-0.03392668792179654, -0.02630476696150641],\n",
              "  [-0.03376152345112393, -0.023328051396778626],\n",
              "  [-0.03465809651783536, -0.024801060983112855],\n",
              "  [-0.03554191419056485, -0.026155993768147034],\n",
              "  [-0.0387040002005441, -0.04101918680327277],\n",
              "  [-0.03946074077061246, -0.040883049794605775],\n",
              "  [-0.02704843112400601, -0.03574292404311041],\n",
              "  [-0.027623198713575103, -0.03585325224058966],\n",
              "  [-0.03298904725483487, -0.06971280319350104],\n",
              "  [-0.03490372725895474, -0.07556270105498175],\n",
              "  [-0.03528561421803067, 0.013659014872142272],\n",
              "  [-0.036771736826215484, -0.0036677573408399056],\n",
              "  [-0.03958704301289151, 0.10730542199952264],\n",
              "  [-0.04270692893436978, 0.07057268874985834],\n",
              "  [-0.05029635855129788, 0.11318899648530145],\n",
              "  [-0.0524148566382272, 0.07479061143738885],\n",
              "  [-0.053647421087537506, 0.10802139299256464],\n",
              "  [-0.051832429851804473, 0.07832904117447992],\n",
              "  [-0.048876844133649566, 0.10532464044434686],\n",
              "  [-0.04740720220974515, 0.07821835534913202],\n",
              "  [0.04202262333461215, -0.17080102903502326],\n",
              "  [0.0345459359032767, -0.18592558843748908],\n",
              "  [0.07191566399165561, -0.02619551164763312],\n",
              "  [0.061926402364458344, -0.04428623659270148],\n",
              "  [0.12465647629329135, 0.07398374336106439],\n",
              "  [0.10388336351939609, 0.03517718570572992],\n",
              "  [0.14026770762034824, 0.09202815549714227],\n",
              "  [0.1202606337411063, 0.025631323031016784],\n",
              "  [0.10376874378749301, 0.1056093718324389],\n",
              "  [0.08471808603831699, 0.07241047876221796],\n",
              "  [0.03828427961894443, -0.17836330873625617],\n",
              "  [-0.033946387256894806, -0.0726377521242414]],\n",
              " [[-0.02879583409854347, -0.024369876725333084],\n",
              "  [-0.03364741376468117, -0.022943716389792312],\n",
              "  [-0.03416847756930763, -0.024286072594778885],\n",
              "  [-0.03455036452838356, -0.025498490674155105],\n",
              "  [-0.03423511556216652, -0.02316306148256575],\n",
              "  [-0.03517919353076393, -0.02457771812166487],\n",
              "  [-0.03606366685458595, -0.025792460782187332],\n",
              "  [-0.03919565251895363, -0.04012308631624495],\n",
              "  [-0.03986459544726784, -0.03991476808275496],\n",
              "  [-0.027665968452181167, -0.03506372485842024],\n",
              "  [-0.028028543506349868, -0.03516010556902205],\n",
              "  [-0.033284838710512465, -0.06725476299013411],\n",
              "  [-0.03324120811053688, -0.07415715966905867],\n",
              "  [-0.035114462886537856, 0.014413613932473313],\n",
              "  [-0.03539526036807472, -0.005746345860617508],\n",
              "  [-0.03898202947207863, 0.10381688560758318],\n",
              "  [-0.041296209607805556, 0.06806262220655168],\n",
              "  [-0.0498383598668235, 0.11139549698148454],\n",
              "  [-0.05150892904826576, 0.07231863226209367],\n",
              "  [-0.05343740752765114, 0.10592319454465593],\n",
              "  [-0.05103399923869545, 0.07488866533551897],\n",
              "  [-0.04903560451098854, 0.10299076523099626],\n",
              "  [-0.04648726752826149, 0.07460703338895525],\n",
              "  [0.04072314926556175, -0.17244802032198225],\n",
              "  [0.03382856079510277, -0.1863828114100865],\n",
              "  [0.07342714497021263, -0.026289086682455887],\n",
              "  [0.06322330662182396, -0.041767220837729324],\n",
              "  [0.12273657747677391, 0.07539119209562029],\n",
              "  [0.10153735109737938, 0.03757389272962297],\n",
              "  [0.13930976816586083, 0.09116460766111101],\n",
              "  [0.11649990507534569, 0.027321238177163254],\n",
              "  [0.10336614080837792, 0.10793175186429704],\n",
              "  [0.09138566681316918, 0.07726027454648698],\n",
              "  [0.03727585503033226, -0.17941541586603438],\n",
              "  [-0.03326302341052467, -0.07070596132959639]],\n",
              " [[-0.029257222584315734, -0.02620769313403537],\n",
              "  [-0.03407607930047174, -0.024540253196443818],\n",
              "  [-0.03454427378518243, -0.025933273349489472],\n",
              "  [-0.0348899807248797, -0.02713096908160617],\n",
              "  [-0.03467927830559869, -0.024705834899629853],\n",
              "  [-0.03556047337395807, -0.026256211314882538],\n",
              "  [-0.03638391154153009, -0.027588136707033417],\n",
              "  [-0.039335235527583556, -0.04145521691867282],\n",
              "  [-0.04004393475396295, -0.042099662337984345],\n",
              "  [-0.02821384327752252, -0.037377305541719696],\n",
              "  [-0.028738066128322082, -0.037553973708833954],\n",
              "  [-0.03446696656090875, -0.06772477201053073],\n",
              "  [-0.03357677119118829, -0.07753778270312717],\n",
              "  [-0.03476326125008722, 0.013160995926175811],\n",
              "  [-0.03561268704278131, -0.0055065827710287785],\n",
              "  [-0.03929547922951837, 0.10622619816235135],\n",
              "  [-0.04154084580285211, 0.0685635132449014],\n",
              "  [-0.049395188263484435, 0.11285757252148221],\n",
              "  [-0.051044448784419494, 0.07314043470791409],\n",
              "  [-0.052903666666575866, 0.10625570246151517],\n",
              "  [-0.051096304825374084, 0.07661145159176419],\n",
              "  [-0.04828344242913385, 0.10363613792828152],\n",
              "  [-0.04696808712823053, 0.07662671038082669],\n",
              "  [0.042757824489048524, -0.17059589198657443],\n",
              "  [0.03675110680716376, -0.18513048461505344],\n",
              "  [0.07198102814810614, -0.024715133224214814],\n",
              "  [0.0637055192674909, -0.04272235206195285],\n",
              "  [0.12442763192313055, 0.0757431311266763],\n",
              "  [0.1066122804369245, 0.03555112310818265],\n",
              "  [0.1399616752352033, 0.09213041492870877],\n",
              "  [0.11597152096884589, 0.047542504753385284],\n",
              "  [0.1013891850199018, 0.10562973448208401],\n",
              "  [0.085379079410008, 0.07160036989620755],\n",
              "  [0.03975446564810614, -0.17786318830081393],\n",
              "  [-0.03402186887604852, -0.07263127735682895]],\n",
              " [[-0.029586427552359407, -0.024891373940876593],\n",
              "  [-0.03419219766344339, -0.02251738054411756],\n",
              "  [-0.03460937057222635, -0.023808715173176398],\n",
              "  [-0.03492801700319559, -0.02494597179549085],\n",
              "  [-0.03487079654421121, -0.02255099756377088],\n",
              "  [-0.03571289096559793, -0.023893294164112677],\n",
              "  [-0.03651570592607767, -0.02496629697935926],\n",
              "  [-0.03934656892504007, -0.039735314675739875],\n",
              "  [-0.04016088758196146, -0.03993493063109266],\n",
              "  [-0.028481059414999788, -0.03621303779738294],\n",
              "  [-0.02897744689668924, -0.036111233064106574],\n",
              "  [-0.03487818752016336, -0.06753831846373426],\n",
              "  [-0.0353041223117283, -0.07618528349058973],\n",
              "  [-0.03361510549272806, 0.014501276186534295],\n",
              "  [-0.03529410873140604, -0.005380031892231574],\n",
              "  [-0.03885542665209085, 0.1041008855615343],\n",
              "  [-0.04192721162523538, 0.06542545812470568],\n",
              "  [-0.04944672499384195, 0.11102664726121081],\n",
              "  [-0.05174409661974222, 0.07001191633088244],\n",
              "  [-0.05297529016222269, 0.10435313241822375],\n",
              "  [-0.05160465155329019, 0.0731754328523363],\n",
              "  [-0.04825639043535501, 0.10196674125535143],\n",
              "  [-0.0472631386348179, 0.07323265331132067],\n",
              "  [0.044068879740578826, -0.17429244262831556],\n",
              "  [0.03759068931852072, -0.18728834135191785],\n",
              "  [0.07347632135663718, -0.026591834851673712],\n",
              "  [0.06482250179563254, -0.04470628244536268],\n",
              "  [0.12559945072446554, 0.07889557140214098],\n",
              "  [0.10544473614011496, 0.03531587379319323],\n",
              "  [0.1410976716450283, 0.09533584373337878],\n",
              "  [0.11413485492978781, 0.047413470915385614],\n",
              "  [0.10183937038694113, 0.10768568771226061],\n",
              "  [0.08473271812711447, 0.07176268356186999],\n",
              "  [0.04082978452954977, -0.1807903919901167],\n",
              "  [-0.03509115491594583, -0.071861800977162]],\n",
              " [[-0.030006499801363273, -0.024220240967614326],\n",
              "  [-0.034444840465273185, -0.0217900403908321],\n",
              "  [-0.03482625058719091, -0.02296663607869831],\n",
              "  [-0.03510865739413671, -0.02393235053334919],\n",
              "  [-0.03513905576297216, -0.0221373566559383],\n",
              "  [-0.03598871997424535, -0.023651016610009346],\n",
              "  [-0.036796362910951896, -0.024918270962578926],\n",
              "  [-0.03939601949283056, -0.03830505694661823],\n",
              "  [-0.040495069537843986, -0.039964331047875556],\n",
              "  [-0.028798671279634758, -0.03558696593557087],\n",
              "  [-0.029393644843782707, -0.035996926682336006],\n",
              "  [-0.03494212201663427, -0.06556357230458942],\n",
              "  [-0.035216720615114494, -0.07693661536489216],\n",
              "  [-0.03417584470340185, 0.013858305556433526],\n",
              "  [-0.034984858546938224, -0.005845797913415107],\n",
              "  [-0.03910353950091772, 0.10413889799799236],\n",
              "  [-0.041404934440340324, 0.06589453135217938],\n",
              "  [-0.0482564287526267, 0.10910521660532269],\n",
              "  [-0.05072549155780248, 0.07074867401804241],\n",
              "  [-0.052070827995027824, 0.10226856384958538],\n",
              "  [-0.0509797649724143, 0.07400386248316082],\n",
              "  [-0.04806611112185888, 0.1004481187888554],\n",
              "  [-0.047004373584474846, 0.07395361576761517],\n",
              "  [0.0429344458239419, -0.1723695525101253],\n",
              "  [0.03662112184933253, -0.1856459506920406],\n",
              "  [0.07268139549664088, -0.026183439152581367],\n",
              "  [0.06325486132076807, -0.04561783160482136],\n",
              "  [0.12463351913860865, 0.07567523632730755],\n",
              "  [0.10419586130550929, 0.033321546656744805],\n",
              "  [0.14046504923275538, 0.09155909929956707],\n",
              "  [0.11283841558865138, 0.04545264158930096],\n",
              "  [0.10259750315121241, 0.10439567480768475],\n",
              "  [0.09240427442959376, 0.07706581268991741],\n",
              "  [0.039777783836637215, -0.17900775160108295],\n",
              "  [-0.03507942131587438, -0.07125009383474079]],\n",
              " [[-0.029009308985301407, -0.025472405978611556],\n",
              "  [-0.033824410608836564, -0.023357633181980697],\n",
              "  [-0.03427740590912953, -0.024782780238560287],\n",
              "  [-0.03459426420075551, -0.02596050841467723],\n",
              "  [-0.034562494925090226, -0.023565593787602035],\n",
              "  [-0.0355233814035143, -0.025191846915653793],\n",
              "  [-0.03641357677323476, -0.026560846396854965],\n",
              "  [-0.03900727289063588, -0.04060304505484447],\n",
              "  [-0.04001250522477284, -0.041423801013401595],\n",
              "  [-0.02792796151978627, -0.037107769080570785],\n",
              "  [-0.02856185691697255, -0.03728908640997752],\n",
              "  [-0.03363421218735829, -0.0677110586847578],\n",
              "  [-0.0342111255441393, -0.07635993106024608],\n",
              "  [-0.03383597390992299, 0.012852009705134781],\n",
              "  [-0.034953560999461564, -0.005495670863560287],\n",
              "  [-0.03885569827897206, 0.10486071961266652],\n",
              "  [-0.04151335018021718, 0.06813811915261403],\n",
              "  [-0.048709359339305314, 0.11123305218560353],\n",
              "  [-0.05107810752732411, 0.07280933516366139],\n",
              "  [-0.052181121281215104, 0.1047006811414446],\n",
              "  [-0.05109083311898366, 0.07624464886529103],\n",
              "  [-0.04772689597947255, 0.10218411343438283],\n",
              "  [-0.04681455748421803, 0.07614069836480275],\n",
              "  [0.04386499864714488, -0.1706991825784956],\n",
              "  [0.03672322971480235, -0.1842201386179243],\n",
              "  [0.07258144361632213, -0.02419400555746898],\n",
              "  [0.06182858688490733, -0.04485142571585521],\n",
              "  [0.12440130455153331, 0.0773571695600237],\n",
              "  [0.10405305368559703, 0.03264760630471364],\n",
              "  [0.1398797963346754, 0.09357392447335378],\n",
              "  [0.11287638885634288, 0.044274326733180436],\n",
              "  [0.10131773693220958, 0.1062846745763506],\n",
              "  [0.08442125064986095, 0.07104080574853078],\n",
              "  [0.04029411418097362, -0.17745966059820995],\n",
              "  [-0.033922668865748795, -0.07203549487250194]],\n",
              " [[-0.02942447151456562, -0.023569942372185793],\n",
              "  [-0.03426031555448261, -0.021806240933281984],\n",
              "  [-0.034727735178811225, -0.02315086211477002],\n",
              "  [-0.03506944860730854, -0.024363101380211916],\n",
              "  [-0.0348571368626186, -0.02198898877416333],\n",
              "  [-0.03572658981595722, -0.023574055092675295],\n",
              "  [-0.03655145849500385, -0.024951697247368898],\n",
              "  [-0.03938846077237812, -0.039901078598839845],\n",
              "  [-0.040180755513055, -0.040515960114342775],\n",
              "  [-0.027947766440255317, -0.03503632630620679],\n",
              "  [-0.02847389663968769, -0.03526580418859204],\n",
              "  [-0.03485207046781269, -0.06672060574804028],\n",
              "  [-0.03451584066663471, -0.07603395070348462],\n",
              "  [-0.03418288912091938, 0.013558088881628905],\n",
              "  [-0.03472040380750385, -0.004665733235222902],\n",
              "  [-0.03892652477536884, 0.1036077133246831],\n",
              "  [-0.04158471311841694, 0.06637024794306079],\n",
              "  [-0.04770265306745258, 0.1087993374892644],\n",
              "  [-0.05099846209798542, 0.07093489084924975],\n",
              "  [-0.05168033923421589, 0.10296213541712085],\n",
              "  [-0.051271510975701484, 0.074565886599677],\n",
              "  [-0.047868801014763984, 0.10100829516138354],\n",
              "  [-0.04718242372785297, 0.07439106617655078],\n",
              "  [0.043229418141501275, -0.1718521722725459],\n",
              "  [0.037129121167319146, -0.18590605344091138],\n",
              "  [0.07169123206819805, -0.028022409336907472],\n",
              "  [0.06393017087663921, -0.04509067620549878],\n",
              "  [0.12447257552828106, 0.07589036141123096],\n",
              "  [0.1057671478816441, 0.03504890118326465],\n",
              "  [0.1399541071483067, 0.09332013045038501],\n",
              "  [0.11437590633119854, 0.047924636942999754],\n",
              "  [0.10127462659563335, 0.1035837522574834],\n",
              "  [0.08477504764284405, 0.07070660505976001],\n",
              "  [0.04017926965441021, -0.17887911285672864],\n",
              "  [-0.0346839555672237, -0.07137727822576245]],\n",
              " [[-0.02909575871058878, -0.026410395758492577],\n",
              "  [-0.03406380585261759, -0.024987215655190576],\n",
              "  [-0.034570147309984534, -0.026503021376473535],\n",
              "  [-0.03494750431605753, -0.027725929873330224],\n",
              "  [-0.03460483721324381, -0.024924392359597314],\n",
              "  [-0.035462011609758703, -0.026451940195901025],\n",
              "  [-0.036278476033892004, -0.027708584921700585],\n",
              "  [-0.039526631150926916, -0.04220449413572036],\n",
              "  [-0.04011483958789286, -0.042385990279061425],\n",
              "  [-0.02809052637645182, -0.037521416800362695],\n",
              "  [-0.028502990518297522, -0.03732805933271133],\n",
              "  [-0.03404711655208048, -0.06973373379026138],\n",
              "  [-0.03237097433635172, -0.0781599232128688],\n",
              "  [-0.03558914831706461, 0.013063257081168067],\n",
              "  [-0.036273052011217444, -0.005482370512826074],\n",
              "  [-0.03943597248622355, 0.10608047757829941],\n",
              "  [-0.041689117465700476, 0.06841087852205552],\n",
              "  [-0.049708654199327795, 0.11318493400301255],\n",
              "  [-0.051449705873216955, 0.072238450390952],\n",
              "  [-0.05308066776820597, 0.10680795226778306],\n",
              "  [-0.051188101087297766, 0.07562822614397324],\n",
              "  [-0.048441668067659704, 0.10423607145036973],\n",
              "  [-0.04711844495364603, 0.07632148776735581],\n",
              "  [0.04184998103550497, -0.1698749014309474],\n",
              "  [0.037606785978589685, -0.18560909714017593],\n",
              "  [0.07130540439060751, -0.02471005405698501],\n",
              "  [0.06418384143284384, -0.042924697058541406],\n",
              "  [0.1239308220999581, 0.0778762153216771],\n",
              "  [0.10622991153172079, 0.03581649575914658],\n",
              "  [0.14017767735889974, 0.09291554008211411],\n",
              "  [0.11633087226322714, 0.04917842660631455],\n",
              "  [0.10102028676441732, 0.10646945749010361],\n",
              "  [0.08649523087910238, 0.07410717521395005],\n",
              "  [0.03972838350704733, -0.17774199928556167],\n",
              "  [-0.0332090454442161, -0.07394682850156509]],\n",
              " [[-0.02796172244208195, -0.02664371558598111],\n",
              "  [-0.03279446704047062, -0.02429463693073819],\n",
              "  [-0.033220223018101236, -0.02571281024387906],\n",
              "  [-0.033539227076939127, -0.026930592741285064],\n",
              "  [-0.033584109374454996, -0.024494908537183502],\n",
              "  [-0.03456669194357731, -0.026052914346967437],\n",
              "  [-0.035498312541416666, -0.02742036410740445],\n",
              "  [-0.038511684962681314, -0.04122634955814908],\n",
              "  [-0.03969906909125187, -0.041709504808698394],\n",
              "  [-0.026931694575718423, -0.03831347056797574],\n",
              "  [-0.02756070239203312, -0.038772307123456695],\n",
              "  [-0.03241842133658268, -0.07125522920063565],\n",
              "  [-0.033963373729160806, -0.07682641574314664],\n",
              "  [-0.03576331479208805, 0.012541987214769623],\n",
              "  [-0.03607820613043644, -0.005965016569410064],\n",
              "  [-0.03971152646200993, 0.10632900169917514],\n",
              "  [-0.0413251434053693, 0.06801680496760776],\n",
              "  [-0.05074863774435856, 0.11459783485957553],\n",
              "  [-0.051145038434437295, 0.07163367441722324],\n",
              "  [-0.05439921362059452, 0.1084282772881644],\n",
              "  [-0.050889125892094156, 0.07450745276042392],\n",
              "  [-0.049818030425480386, 0.1052626746041434],\n",
              "  [-0.046293399163654825, 0.07463607958384921],\n",
              "  [0.04102624314171932, -0.1734972221510751],\n",
              "  [0.034533449581691245, -0.18675573893955777],\n",
              "  [0.07249457495553158, -0.024023614610944488],\n",
              "  [0.06267977612359188, -0.042889736379895904],\n",
              "  [0.12404627459389828, 0.07937268189021518],\n",
              "  [0.10388106959206722, 0.03702733686992099],\n",
              "  [0.13979757683617733, 0.09411410263606479],\n",
              "  [0.11489708083016537, 0.04812226465770175],\n",
              "  [0.10247964518410824, 0.10905907324382236],\n",
              "  [0.08599669592721126, 0.07330260447093417],\n",
              "  [0.03777984636170528, -0.18012648054531644],\n",
              "  [-0.033190897532871744, -0.07404082247189114]],\n",
              " [[-0.030175210748400016, -0.024183799539293616],\n",
              "  [-0.03463852575847082, -0.021801759515489905],\n",
              "  [-0.03499573639460973, -0.022986521039690344],\n",
              "  [-0.03522670439311437, -0.02391337326594767],\n",
              "  [-0.035364391122545524, -0.0221861498696464],\n",
              "  [-0.03620338610240392, -0.023747731958116858],\n",
              "  [-0.036982061181749626, -0.025044490609850256],\n",
              "  [-0.03924313357898168, -0.03805725744792399],\n",
              "  [-0.040351928983415886, -0.040088881765093176],\n",
              "  [-0.02886200121470861, -0.03556488922664103],\n",
              "  [-0.02949202230998449, -0.03606360128947672],\n",
              "  [-0.034250499520983024, -0.06642251185008463],\n",
              "  [-0.034669341359819694, -0.07805072239467081],\n",
              "  [-0.03422880343028478, 0.01444078513554159],\n",
              "  [-0.035115958963121696, -0.006357063565935461],\n",
              "  [-0.038279058252062126, 0.10487986632755819],\n",
              "  [-0.04075950554439001, 0.06636966296604696],\n",
              "  [-0.047872843061174675, 0.11100501843861166],\n",
              "  [-0.04996305874415807, 0.07132197448185507],\n",
              "  [-0.05166030100413732, 0.10335315295628134],\n",
              "  [-0.050021083865846916, 0.07428325244358602],\n",
              "  [-0.047360004697527214, 0.10122383662632528],\n",
              "  [-0.04604211619922094, 0.07434726783207479],\n",
              "  [0.04380047151020594, -0.17153619698115763],\n",
              "  [0.03732788392475672, -0.1844308870179313],\n",
              "  [0.07300352879932948, -0.025135804925646155],\n",
              "  [0.06300574370792933, -0.046141316209520666],\n",
              "  [0.1252575499670846, 0.07609010764530721],\n",
              "  [0.10313373633793421, 0.031925330843244226],\n",
              "  [0.1411938054221017, 0.09234191008976522],\n",
              "  [0.1120320541518075, 0.043316553320203455],\n",
              "  [0.10251098700932093, 0.10638059207371298],\n",
              "  [0.08438765832356043, 0.07065380641392294],\n",
              "  [0.04056417771748133, -0.17798354199954447],\n",
              "  [-0.03445992044040136, -0.07223661712237772]],\n",
              " [[-0.02919720326151165, -0.01999918563025338],\n",
              "  [-0.03407924090112957, -0.020481446811131065],\n",
              "  [-0.03451286469187054, -0.02221612078802926],\n",
              "  [-0.034723686320441094, -0.02378515345709664],\n",
              "  [-0.03460072193826946, -0.02001742465155465],\n",
              "  [-0.03535060797418865, -0.0216308627809797],\n",
              "  [-0.035986291510718194, -0.023011008330753868],\n",
              "  [-0.03832428370203289, -0.038737157412937706],\n",
              "  [-0.03861187611307415, -0.037911394664219444],\n",
              "  [-0.02773432646478924, -0.030142048427036827],\n",
              "  [-0.02810751114572796, -0.03013787610190255],\n",
              "  [-0.03191845331873211, -0.06655857903616769],\n",
              "  [-0.028906332595007744, -0.07328228099005563],\n",
              "  [-0.03506539974893841, 0.014012953213282997],\n",
              "  [-0.03518884096826824, -0.004412034579685753],\n",
              "  [-0.03893448625292095, 0.10158648150307792],\n",
              "  [-0.042024867875235405, 0.06165172713143485],\n",
              "  [-0.050152229411261406, 0.11327268736703056],\n",
              "  [-0.05309944067682537, 0.06894608395440238],\n",
              "  [-0.05284945879663738, 0.1066693867955889],\n",
              "  [-0.053070800645010796, 0.07027413504464286],\n",
              "  [-0.048798607928412285, 0.10295935528618949],\n",
              "  [-0.04882236037935528, 0.07007296936852592],\n",
              "  [0.04061616744313923, -0.17579928977148873],\n",
              "  [0.034527195351464424, -0.1905824933733259],\n",
              "  [0.07216192569051472, -0.02691362244742257],\n",
              "  [0.06537295665059772, -0.039215127059391564],\n",
              "  [0.12177283849034992, 0.07344134705407279],\n",
              "  [0.10180939521108356, 0.037711938789912636],\n",
              "  [0.13827790107045856, 0.0882781948362078],\n",
              "  [0.11722947444234577, 0.031466206482478554],\n",
              "  [0.10430599536214558, 0.10532101052148002],\n",
              "  [0.08682675446782795, 0.0722799505506243],\n",
              "  [0.037571681397301826, -0.1831908915724073],\n",
              "  [-0.030412392956869927, -0.06992043001311166]],\n",
              " [[-0.02857132468904766, -0.020505502394267516],\n",
              "  [-0.03341318879808697, -0.020403757265635925],\n",
              "  [-0.033704715115683404, -0.021995022467204528],\n",
              "  [-0.03386391912187847, -0.023362233809062438],\n",
              "  [-0.03440834794725689, -0.02027864711625238],\n",
              "  [-0.03536291633333477, -0.021850481203624206],\n",
              "  [-0.03621573959078106, -0.023223414591380553],\n",
              "  [-0.037802474839346734, -0.038999929598399596],\n",
              "  [-0.038658099515097466, -0.038491621187755065],\n",
              "  [-0.026934342724936333, -0.031131222418376403],\n",
              "  [-0.027669983250754204, -0.031564250162669616],\n",
              "  [-0.03085680518831524, -0.07118768947465082],\n",
              "  [-0.032613711697714654, -0.07318921344620843],\n",
              "  [-0.03529878173555645, 0.012444004842213197],\n",
              "  [-0.03615094934191021, -0.004291072062083678],\n",
              "  [-0.039319949490683404, 0.1042718742574964],\n",
              "  [-0.041464703423636284, 0.06810973627226691],\n",
              "  [-0.05005337510790142, 0.11729840976851325],\n",
              "  [-0.05239935432161602, 0.07250361187117438],\n",
              "  [-0.053448157651083794, 0.11183963758604865],\n",
              "  [-0.05232472930635723, 0.07397197229521613],\n",
              "  [-0.04985775266374859, 0.10750024063246588],\n",
              "  [-0.04770909462656292, 0.07431952697890143],\n",
              "  [0.04130719389234272, -0.17680074231965204],\n",
              "  [0.0323430725506374, -0.18874140041215082],\n",
              "  [0.07175187553678242, -0.030980899504252868],\n",
              "  [0.06049708809171406, -0.04540444867951532],\n",
              "  [0.12304000343595234, 0.07062749607222418],\n",
              "  [0.10069773878370014, 0.03364239675658087],\n",
              "  [0.139744085924966, 0.08590523941176276],\n",
              "  [0.11822484220777241, 0.024973139592579408],\n",
              "  [0.1049630471638271, 0.10440914375441412],\n",
              "  [0.09044359411512104, 0.07554464084761481],\n",
              "  [0.03682513322149006, -0.18277107136590143],\n",
              "  [-0.031735258443014946, -0.07218845146042963]],\n",
              " [[-0.028694931949887925, -0.02095707654953005],\n",
              "  [-0.03347665497234886, -0.02091106176376345],\n",
              "  [-0.033952359642301255, -0.02257450819015505],\n",
              "  [-0.03422272631100243, -0.024042153358459495],\n",
              "  [-0.033981327499662095, -0.020552062988281272],\n",
              "  [-0.03476179071835106, -0.0220476031303406],\n",
              "  [-0.03543824383190697, -0.023318552970886253],\n",
              "  [-0.03833115526608055, -0.039017224311828635],\n",
              "  [-0.03845632502010887, -0.03789212703704836],\n",
              "  [-0.027444247688565904, -0.03127976655960085],\n",
              "  [-0.02770549484661644, -0.031241083145141624],\n",
              "  [-0.032642309154782945, -0.06920948028564455],\n",
              "  [-0.028445486511502915, -0.07489105463027956],\n",
              "  [-0.036354069198880845, 0.012973582744598366],\n",
              "  [-0.035357837166104966, -0.004960620403289817],\n",
              "  [-0.039751206125531846, 0.10436288118362425],\n",
              "  [-0.040320311273847276, 0.06581804752349851],\n",
              "  [-0.050441925014768296, 0.11628100872039793],\n",
              "  [-0.05059424468449181, 0.07239822149276731],\n",
              "  [-0.05333215423992699, 0.11055943965911863],\n",
              "  [-0.050795887197766953, 0.07370487451553343],\n",
              "  [-0.04953590461185997, 0.10705140829086301],\n",
              "  [-0.04656625219753807, 0.07353279590606687],\n",
              "  [0.04010170272418434, -0.17676236629486086],\n",
              "  [0.0336829381329673, -0.19003602266311648],\n",
              "  [0.07199471763202125, -0.029149198532104514],\n",
              "  [0.0635211425168174, -0.04165580272674563],\n",
              "  [0.12155568173953468, 0.07027623653411863],\n",
              "  [0.10113083890506203, 0.036891615390777566],\n",
              "  [0.13867294362613136, 0.08546063899993894],\n",
              "  [0.11567556432315285, 0.03234598636627195],\n",
              "  [0.10321574977466041, 0.10275248289108274],\n",
              "  [0.08470314315387184, 0.07153800725936887],\n",
              "  [0.03689232042857582, -0.18339919447898867],\n",
              "  [-0.03054389783314293, -0.07205026745796206]]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_videos = np.asarray(train_videos)\n",
        "new_train_keypoints = np.asarray(new_train_keypoints)\n",
        "new_train_labels = np.asarray(new_train_labels).astype('int32')\n",
        "\n",
        "# train_keypoints = tf.convert_to_tensor(train_keypoints, dtype=tf.float32)\n",
        "# # yKey_train = tf.cast(yKey_train , dtype=tf.float32)\n",
        "# train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
        "\n",
        "# train_keypoints = np.array([np.array(val) for val in train_keypoints])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "XKey_train,XKey_test,yKey_train,yKey_test = train_test_split(new_train_keypoints,new_train_labels,test_size=0.25,shuffle=True)"
      ],
      "metadata": {
        "id": "o2u4v1FP1NmE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(XKey_train),len(XKey_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZFr3mqdw0ai",
        "outputId": "181f9806-4f3d-4853-baea-e3c2dbdf02e0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "773 258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XKey_train = tf.convert_to_tensor(XKey_train, dtype=tf.float32)\n",
        "yKey_train = tf.convert_to_tensor(yKey_train, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "PHGmTIzi26pF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mediapipe as mp\n",
        "\n",
        "# landmarks = mp_pose.PoseLandmark.__members__\n",
        "# print(landmarks)\n"
      ],
      "metadata": {
        "id": "PyUPZNVDDyHy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models, layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, TimeDistributed, GlobalAveragePooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import GlorotNormal\n",
        "from keras.regularizers import L1L2\n",
        "\n",
        "yKey_train = np.squeeze(yKey_train)\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Define model input shape\n",
        "input_shape = (16, 35, 2)\n",
        "\n",
        "# Build model architecture\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, activation='relu'),input_shape = input_shape))\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model.add(TimeDistributed(Conv1D(filters=50, kernel_size=3, activation='relu')))\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "\n",
        "# flatten output from CNN layers\n",
        "model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
        "\n",
        "# add LSTM layer\n",
        "# model.add(LSTM(100, dropout=0.0, recurrent_dropout=0.0, kernel_initializer=GlorotNormal(seed=42), recurrent_regularizer=L1L2(l1=0.0, l2=0.001), bias_regularizer=L1L2(l1=0.0, l2=0.001), recurrent_initializer=GlorotNormal(seed=42), return_sequences=False))\n",
        "model.add(LSTM(100, recurrent_regularizer=L1L2(l1=0.01, l2=0.001), bias_regularizer=L1L2(l1=0.01, l2=0.001), recurrent_initializer=GlorotNormal(seed=42), return_sequences=False))\n",
        "\n",
        "# TODO: forget bias \n",
        "# TODO: L1 = 0.01\n",
        "\n",
        "\n",
        "# add fully connected layers\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(80, activation='relu'))\n",
        "\n",
        "# add Softmax layer\n",
        "model.add(Dense(20, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "adam = Adam(learning_rate=0.001) \n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# print model summary\n",
        "# print(model.summary())\n",
        "\n",
        "history = model.fit(XKey_train,yKey_train,batch_size=32,epochs = 400,validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIenRNsKJaz9",
        "outputId": "2ccae0dc-d1a6-46e0-e05e-d805e5e3c2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 21.7000 - accuracy: 0.0615 - val_loss: 18.0037 - val_accuracy: 0.0387\n",
            "Epoch 2/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 15.4152 - accuracy: 0.0534 - val_loss: 12.6308 - val_accuracy: 0.0516\n",
            "Epoch 3/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 10.7310 - accuracy: 0.1100 - val_loss: 8.6997 - val_accuracy: 0.1161\n",
            "Epoch 4/400\n",
            "20/20 [==============================] - 3s 165ms/step - loss: 7.3103 - accuracy: 0.1521 - val_loss: 5.8877 - val_accuracy: 0.3097\n",
            "Epoch 5/400\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 5.0454 - accuracy: 0.2298 - val_loss: 4.1440 - val_accuracy: 0.3806\n",
            "Epoch 6/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 3.6963 - accuracy: 0.3042 - val_loss: 3.2563 - val_accuracy: 0.3097\n",
            "Epoch 7/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 2.9551 - accuracy: 0.3641 - val_loss: 2.7889 - val_accuracy: 0.4194\n",
            "Epoch 8/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 2.6993 - accuracy: 0.4207 - val_loss: 2.6578 - val_accuracy: 0.3613\n",
            "Epoch 9/400\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 2.5853 - accuracy: 0.4239 - val_loss: 2.5312 - val_accuracy: 0.4516\n",
            "Epoch 10/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 2.4839 - accuracy: 0.4482 - val_loss: 2.4482 - val_accuracy: 0.4710\n",
            "Epoch 11/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 2.3988 - accuracy: 0.4757 - val_loss: 2.4136 - val_accuracy: 0.4645\n",
            "Epoch 12/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 2.3522 - accuracy: 0.4693 - val_loss: 2.3521 - val_accuracy: 0.4323\n",
            "Epoch 13/400\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 2.2349 - accuracy: 0.5049 - val_loss: 2.1922 - val_accuracy: 0.5548\n",
            "Epoch 14/400\n",
            "20/20 [==============================] - 3s 148ms/step - loss: 2.1790 - accuracy: 0.5113 - val_loss: 2.1816 - val_accuracy: 0.5613\n",
            "Epoch 15/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 2.2147 - accuracy: 0.4935 - val_loss: 2.2876 - val_accuracy: 0.4194\n",
            "Epoch 16/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 2.1386 - accuracy: 0.5146 - val_loss: 2.1288 - val_accuracy: 0.5355\n",
            "Epoch 17/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 2.1043 - accuracy: 0.5097 - val_loss: 2.0535 - val_accuracy: 0.6000\n",
            "Epoch 18/400\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 1.9814 - accuracy: 0.5599 - val_loss: 2.0071 - val_accuracy: 0.5742\n",
            "Epoch 19/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 1.9235 - accuracy: 0.5485 - val_loss: 2.0654 - val_accuracy: 0.5161\n",
            "Epoch 20/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 1.8426 - accuracy: 0.5696 - val_loss: 1.9048 - val_accuracy: 0.5484\n",
            "Epoch 21/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 1.8548 - accuracy: 0.5971 - val_loss: 1.8825 - val_accuracy: 0.5806\n",
            "Epoch 22/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 1.8457 - accuracy: 0.5841 - val_loss: 1.7751 - val_accuracy: 0.6645\n",
            "Epoch 23/400\n",
            "20/20 [==============================] - 3s 163ms/step - loss: 1.7492 - accuracy: 0.6036 - val_loss: 1.8092 - val_accuracy: 0.6258\n",
            "Epoch 24/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 1.7516 - accuracy: 0.6036 - val_loss: 1.9118 - val_accuracy: 0.4968\n",
            "Epoch 25/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 1.6891 - accuracy: 0.6165 - val_loss: 1.6594 - val_accuracy: 0.7032\n",
            "Epoch 26/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 1.6047 - accuracy: 0.6634 - val_loss: 1.6659 - val_accuracy: 0.6516\n",
            "Epoch 27/400\n",
            "20/20 [==============================] - 3s 155ms/step - loss: 1.5992 - accuracy: 0.6294 - val_loss: 1.6383 - val_accuracy: 0.7226\n",
            "Epoch 28/400\n",
            "20/20 [==============================] - 3s 147ms/step - loss: 1.6438 - accuracy: 0.6230 - val_loss: 1.7259 - val_accuracy: 0.5677\n",
            "Epoch 29/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 1.6463 - accuracy: 0.6068 - val_loss: 1.6135 - val_accuracy: 0.6839\n",
            "Epoch 30/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 1.5141 - accuracy: 0.6650 - val_loss: 1.5751 - val_accuracy: 0.6387\n",
            "Epoch 31/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 1.4843 - accuracy: 0.6715 - val_loss: 1.5020 - val_accuracy: 0.6710\n",
            "Epoch 32/400\n",
            "20/20 [==============================] - 3s 173ms/step - loss: 1.4460 - accuracy: 0.6828 - val_loss: 1.4890 - val_accuracy: 0.7226\n",
            "Epoch 33/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 1.4030 - accuracy: 0.6667 - val_loss: 1.4937 - val_accuracy: 0.6387\n",
            "Epoch 34/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 1.3947 - accuracy: 0.6926 - val_loss: 1.4187 - val_accuracy: 0.6452\n",
            "Epoch 35/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 1.4156 - accuracy: 0.6634 - val_loss: 1.3408 - val_accuracy: 0.6710\n",
            "Epoch 36/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 1.2821 - accuracy: 0.7071 - val_loss: 1.2880 - val_accuracy: 0.6903\n",
            "Epoch 37/400\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 1.2529 - accuracy: 0.7023 - val_loss: 1.2930 - val_accuracy: 0.6839\n",
            "Epoch 38/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 1.1659 - accuracy: 0.7427 - val_loss: 1.3035 - val_accuracy: 0.6968\n",
            "Epoch 39/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 1.1436 - accuracy: 0.7443 - val_loss: 1.1940 - val_accuracy: 0.7290\n",
            "Epoch 40/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 1.1927 - accuracy: 0.7104 - val_loss: 1.2250 - val_accuracy: 0.7355\n",
            "Epoch 41/400\n",
            "20/20 [==============================] - 3s 159ms/step - loss: 1.1006 - accuracy: 0.7492 - val_loss: 1.1257 - val_accuracy: 0.7484\n",
            "Epoch 42/400\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 1.0820 - accuracy: 0.7460 - val_loss: 1.1990 - val_accuracy: 0.7032\n",
            "Epoch 43/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 1.0941 - accuracy: 0.7282 - val_loss: 1.0768 - val_accuracy: 0.7548\n",
            "Epoch 44/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 1.0296 - accuracy: 0.7330 - val_loss: 1.0585 - val_accuracy: 0.7161\n",
            "Epoch 45/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 1.0241 - accuracy: 0.7508 - val_loss: 1.1147 - val_accuracy: 0.6710\n",
            "Epoch 46/400\n",
            "20/20 [==============================] - 3s 175ms/step - loss: 0.9641 - accuracy: 0.7492 - val_loss: 0.9908 - val_accuracy: 0.7548\n",
            "Epoch 47/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.9367 - accuracy: 0.7589 - val_loss: 1.0019 - val_accuracy: 0.7355\n",
            "Epoch 48/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.9110 - accuracy: 0.7540 - val_loss: 0.9076 - val_accuracy: 0.8000\n",
            "Epoch 49/400\n",
            "20/20 [==============================] - 2s 126ms/step - loss: 0.8706 - accuracy: 0.7718 - val_loss: 0.9324 - val_accuracy: 0.7548\n",
            "Epoch 50/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.8662 - accuracy: 0.7589 - val_loss: 1.0219 - val_accuracy: 0.6968\n",
            "Epoch 51/400\n",
            "20/20 [==============================] - 3s 166ms/step - loss: 0.7861 - accuracy: 0.7864 - val_loss: 0.8086 - val_accuracy: 0.8194\n",
            "Epoch 52/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.7058 - accuracy: 0.7977 - val_loss: 0.8759 - val_accuracy: 0.7613\n",
            "Epoch 53/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.7325 - accuracy: 0.7945 - val_loss: 0.8623 - val_accuracy: 0.7677\n",
            "Epoch 54/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.7202 - accuracy: 0.7945 - val_loss: 0.7680 - val_accuracy: 0.8194\n",
            "Epoch 55/400\n",
            "20/20 [==============================] - 3s 162ms/step - loss: 0.7514 - accuracy: 0.7702 - val_loss: 0.7720 - val_accuracy: 0.7806\n",
            "Epoch 56/400\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.6976 - accuracy: 0.7896 - val_loss: 0.9162 - val_accuracy: 0.7097\n",
            "Epoch 57/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.7866 - accuracy: 0.7832 - val_loss: 0.8323 - val_accuracy: 0.8000\n",
            "Epoch 58/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.7807 - accuracy: 0.7573 - val_loss: 0.8033 - val_accuracy: 0.7806\n",
            "Epoch 59/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.7685 - accuracy: 0.7735 - val_loss: 0.8050 - val_accuracy: 0.7484\n",
            "Epoch 60/400\n",
            "20/20 [==============================] - 4s 180ms/step - loss: 0.7526 - accuracy: 0.7832 - val_loss: 0.8868 - val_accuracy: 0.7290\n",
            "Epoch 61/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.8280 - accuracy: 0.7508 - val_loss: 0.8808 - val_accuracy: 0.7742\n",
            "Epoch 62/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.7602 - accuracy: 0.7686 - val_loss: 0.8031 - val_accuracy: 0.7806\n",
            "Epoch 63/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.6960 - accuracy: 0.7929 - val_loss: 0.7844 - val_accuracy: 0.7935\n",
            "Epoch 64/400\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.7002 - accuracy: 0.8074 - val_loss: 0.7446 - val_accuracy: 0.8065\n",
            "Epoch 65/400\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.7566 - accuracy: 0.7670 - val_loss: 0.7862 - val_accuracy: 0.7871\n",
            "Epoch 66/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.6834 - accuracy: 0.8285 - val_loss: 0.7282 - val_accuracy: 0.8129\n",
            "Epoch 67/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.6582 - accuracy: 0.8188 - val_loss: 0.7172 - val_accuracy: 0.8065\n",
            "Epoch 68/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.6007 - accuracy: 0.8414 - val_loss: 0.6808 - val_accuracy: 0.8323\n",
            "Epoch 69/400\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.5880 - accuracy: 0.8333 - val_loss: 0.6839 - val_accuracy: 0.8000\n",
            "Epoch 70/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.6293 - accuracy: 0.8301 - val_loss: 0.6699 - val_accuracy: 0.8452\n",
            "Epoch 71/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.6088 - accuracy: 0.8366 - val_loss: 0.7013 - val_accuracy: 0.8258\n",
            "Epoch 72/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6383 - accuracy: 0.8333 - val_loss: 0.7020 - val_accuracy: 0.8194\n",
            "Epoch 73/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6625 - accuracy: 0.8172 - val_loss: 0.7173 - val_accuracy: 0.8387\n",
            "Epoch 74/400\n",
            "20/20 [==============================] - 3s 165ms/step - loss: 0.6297 - accuracy: 0.8252 - val_loss: 0.7070 - val_accuracy: 0.8387\n",
            "Epoch 75/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6524 - accuracy: 0.8074 - val_loss: 0.7259 - val_accuracy: 0.8258\n",
            "Epoch 76/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.6616 - accuracy: 0.8188 - val_loss: 0.7596 - val_accuracy: 0.7871\n",
            "Epoch 77/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.5990 - accuracy: 0.8511 - val_loss: 0.7124 - val_accuracy: 0.8065\n",
            "Epoch 78/400\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.6210 - accuracy: 0.8172 - val_loss: 0.7943 - val_accuracy: 0.7871\n",
            "Epoch 79/400\n",
            "20/20 [==============================] - 3s 151ms/step - loss: 0.7445 - accuracy: 0.7654 - val_loss: 0.7911 - val_accuracy: 0.7935\n",
            "Epoch 80/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.6708 - accuracy: 0.8123 - val_loss: 0.7574 - val_accuracy: 0.7806\n",
            "Epoch 81/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6007 - accuracy: 0.8252 - val_loss: 0.7158 - val_accuracy: 0.8065\n",
            "Epoch 82/400\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.6091 - accuracy: 0.8091 - val_loss: 0.7951 - val_accuracy: 0.7806\n",
            "Epoch 83/400\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.6310 - accuracy: 0.8074 - val_loss: 0.7394 - val_accuracy: 0.8129\n",
            "Epoch 84/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.5799 - accuracy: 0.8414 - val_loss: 0.7315 - val_accuracy: 0.7871\n",
            "Epoch 85/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6872 - accuracy: 0.7945 - val_loss: 0.7234 - val_accuracy: 0.7871\n",
            "Epoch 86/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5789 - accuracy: 0.8382 - val_loss: 0.6571 - val_accuracy: 0.8065\n",
            "Epoch 87/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5823 - accuracy: 0.8285 - val_loss: 0.6661 - val_accuracy: 0.8129\n",
            "Epoch 88/400\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.5598 - accuracy: 0.8625 - val_loss: 0.6898 - val_accuracy: 0.8065\n",
            "Epoch 89/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.5080 - accuracy: 0.8576 - val_loss: 0.6601 - val_accuracy: 0.8387\n",
            "Epoch 90/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.5625 - accuracy: 0.8479 - val_loss: 0.6685 - val_accuracy: 0.8387\n",
            "Epoch 91/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5553 - accuracy: 0.8495 - val_loss: 0.6096 - val_accuracy: 0.8581\n",
            "Epoch 92/400\n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.5254 - accuracy: 0.8608 - val_loss: 0.6777 - val_accuracy: 0.8452\n",
            "Epoch 93/400\n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.6095 - accuracy: 0.8220 - val_loss: 0.6310 - val_accuracy: 0.8516\n",
            "Epoch 94/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.5913 - accuracy: 0.8398 - val_loss: 0.8152 - val_accuracy: 0.7419\n",
            "Epoch 95/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.6141 - accuracy: 0.8479 - val_loss: 0.6560 - val_accuracy: 0.8258\n",
            "Epoch 96/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5327 - accuracy: 0.8576 - val_loss: 0.6493 - val_accuracy: 0.8387\n",
            "Epoch 97/400\n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.4755 - accuracy: 0.8851 - val_loss: 0.6611 - val_accuracy: 0.8323\n",
            "Epoch 98/400\n",
            "20/20 [==============================] - 3s 137ms/step - loss: 0.5187 - accuracy: 0.8592 - val_loss: 0.8001 - val_accuracy: 0.7742\n",
            "Epoch 99/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.6316 - accuracy: 0.8285 - val_loss: 0.7267 - val_accuracy: 0.8129\n",
            "Epoch 100/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.5836 - accuracy: 0.8414 - val_loss: 0.6815 - val_accuracy: 0.8516\n",
            "Epoch 101/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.5815 - accuracy: 0.8463 - val_loss: 0.6702 - val_accuracy: 0.8258\n",
            "Epoch 102/400\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.6007 - accuracy: 0.8220 - val_loss: 0.7205 - val_accuracy: 0.8194\n",
            "Epoch 103/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.5427 - accuracy: 0.8657 - val_loss: 0.6387 - val_accuracy: 0.8516\n",
            "Epoch 104/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.4942 - accuracy: 0.8706 - val_loss: 0.6306 - val_accuracy: 0.8387\n",
            "Epoch 105/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5690 - accuracy: 0.8236 - val_loss: 0.7063 - val_accuracy: 0.8258\n",
            "Epoch 106/400\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.5320 - accuracy: 0.8479 - val_loss: 0.6701 - val_accuracy: 0.8581\n",
            "Epoch 107/400\n",
            "20/20 [==============================] - 3s 157ms/step - loss: 0.4682 - accuracy: 0.8819 - val_loss: 0.6739 - val_accuracy: 0.8387\n",
            "Epoch 108/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.4426 - accuracy: 0.8770 - val_loss: 0.6601 - val_accuracy: 0.8452\n",
            "Epoch 109/400\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.6032 - accuracy: 0.8382 - val_loss: 0.7212 - val_accuracy: 0.8129\n",
            "Epoch 110/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.5150 - accuracy: 0.8706 - val_loss: 0.6549 - val_accuracy: 0.8452\n",
            "Epoch 111/400\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.5614 - accuracy: 0.8414 - val_loss: 0.7042 - val_accuracy: 0.8194\n",
            "Epoch 112/400\n",
            "20/20 [==============================] - 3s 139ms/step - loss: 0.4627 - accuracy: 0.8786 - val_loss: 0.6894 - val_accuracy: 0.8323\n",
            "Epoch 113/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4582 - accuracy: 0.8819 - val_loss: 0.6827 - val_accuracy: 0.8323\n",
            "Epoch 114/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.4266 - accuracy: 0.9045 - val_loss: 0.6676 - val_accuracy: 0.8258\n",
            "Epoch 115/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.5221 - accuracy: 0.8689 - val_loss: 0.6756 - val_accuracy: 0.8452\n",
            "Epoch 116/400\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.5308 - accuracy: 0.8689 - val_loss: 0.7006 - val_accuracy: 0.8194\n",
            "Epoch 117/400\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.5170 - accuracy: 0.8625 - val_loss: 0.7020 - val_accuracy: 0.8065\n",
            "Epoch 118/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.4748 - accuracy: 0.8754 - val_loss: 0.6831 - val_accuracy: 0.8323\n",
            "Epoch 119/400\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.4286 - accuracy: 0.8948 - val_loss: 0.6520 - val_accuracy: 0.8581\n",
            "Epoch 120/400\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.4201 - accuracy: 0.8867 - val_loss: 0.6238 - val_accuracy: 0.8323\n",
            "Epoch 121/400\n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.5540 - accuracy: 0.8592 - val_loss: 0.7026 - val_accuracy: 0.8387\n",
            "Epoch 122/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.5560 - accuracy: 0.8495 - val_loss: 0.6598 - val_accuracy: 0.8258\n",
            "Epoch 123/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4159 - accuracy: 0.8948 - val_loss: 0.6274 - val_accuracy: 0.8387\n",
            "Epoch 124/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.4051 - accuracy: 0.9045 - val_loss: 0.6532 - val_accuracy: 0.8581\n",
            "Epoch 125/400\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.5126 - accuracy: 0.8641 - val_loss: 0.6536 - val_accuracy: 0.8387\n",
            "Epoch 126/400\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.4467 - accuracy: 0.9013 - val_loss: 0.6298 - val_accuracy: 0.8387\n",
            "Epoch 127/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.4304 - accuracy: 0.8964 - val_loss: 0.6234 - val_accuracy: 0.8516\n",
            "Epoch 128/400\n",
            "20/20 [==============================] - 2s 124ms/step - loss: 0.4319 - accuracy: 0.8964 - val_loss: 0.5741 - val_accuracy: 0.8581\n",
            "Epoch 129/400\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.4315 - accuracy: 0.8803 - val_loss: 0.6754 - val_accuracy: 0.8258\n",
            "Epoch 130/400\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.5199 - accuracy: 0.8608 - val_loss: 0.6927 - val_accuracy: 0.8516\n",
            "Epoch 131/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.4515 - accuracy: 0.8867 - val_loss: 0.6756 - val_accuracy: 0.8258\n",
            "Epoch 132/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.4279 - accuracy: 0.8997 - val_loss: 0.6975 - val_accuracy: 0.8452\n",
            "Epoch 133/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.4130 - accuracy: 0.8964 - val_loss: 0.6497 - val_accuracy: 0.8452\n",
            "Epoch 134/400\n",
            "20/20 [==============================] - 3s 139ms/step - loss: 0.4426 - accuracy: 0.8819 - val_loss: 0.7318 - val_accuracy: 0.8129\n",
            "Epoch 135/400\n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.4455 - accuracy: 0.8770 - val_loss: 0.6418 - val_accuracy: 0.8387\n",
            "Epoch 136/400\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.4384 - accuracy: 0.8964 - val_loss: 0.7183 - val_accuracy: 0.8387\n",
            "Epoch 137/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.3857 - accuracy: 0.8981 - val_loss: 0.6993 - val_accuracy: 0.8516\n",
            "Epoch 138/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.3888 - accuracy: 0.9078 - val_loss: 0.6812 - val_accuracy: 0.8258\n",
            "Epoch 139/400\n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.4050 - accuracy: 0.8867 - val_loss: 0.6547 - val_accuracy: 0.8581\n",
            "Epoch 140/400\n",
            "20/20 [==============================] - 3s 141ms/step - loss: 0.6071 - accuracy: 0.8511 - val_loss: 0.6922 - val_accuracy: 0.8258\n",
            "Epoch 141/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.5224 - accuracy: 0.8592 - val_loss: 0.6762 - val_accuracy: 0.8452\n",
            "Epoch 142/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.4688 - accuracy: 0.8673 - val_loss: 0.6569 - val_accuracy: 0.8323\n",
            "Epoch 143/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4329 - accuracy: 0.8867 - val_loss: 0.6163 - val_accuracy: 0.8645\n",
            "Epoch 144/400\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.3670 - accuracy: 0.9159 - val_loss: 0.6351 - val_accuracy: 0.8387\n",
            "Epoch 145/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.3788 - accuracy: 0.8964 - val_loss: 0.5973 - val_accuracy: 0.8516\n",
            "Epoch 146/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.3393 - accuracy: 0.9126 - val_loss: 0.6818 - val_accuracy: 0.8323\n",
            "Epoch 147/400\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4716 - accuracy: 0.8867 - val_loss: 0.6010 - val_accuracy: 0.8581\n",
            "Epoch 148/400\n",
            "20/20 [==============================] - 3s 137ms/step - loss: 0.4238 - accuracy: 0.8900 - val_loss: 0.6406 - val_accuracy: 0.8387\n",
            "Epoch 149/400\n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.4448 - accuracy: 0.8786 - val_loss: 0.7106 - val_accuracy: 0.8323\n",
            "Epoch 150/400\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.4068 - accuracy: 0.8981 - val_loss: 0.6042 - val_accuracy: 0.8581\n",
            "Epoch 151/400\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.3672 - accuracy: 0.9223 - val_loss: 0.6138 - val_accuracy: 0.8581\n",
            "Epoch 152/400\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.4393 - accuracy: 0.8867 - val_loss: 0.7757 - val_accuracy: 0.8258\n",
            "Epoch 153/400\n",
            "20/20 [==============================] - 4s 186ms/step - loss: 0.4641 - accuracy: 0.8641 - val_loss: 0.6203 - val_accuracy: 0.8710\n",
            "Epoch 154/400\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.4550 - accuracy: 0.8948 - val_loss: 0.7099 - val_accuracy: 0.8323\n",
            "Epoch 155/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.4386 - accuracy: 0.8786 - val_loss: 0.6257 - val_accuracy: 0.8710\n",
            "Epoch 156/400\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.3529 - accuracy: 0.9239 - val_loss: 0.6675 - val_accuracy: 0.8194\n",
            "Epoch 157/400\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.3493 - accuracy: 0.9094 - val_loss: 0.6398 - val_accuracy: 0.8645\n",
            "Epoch 158/400\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.3248 - accuracy: 0.9272 - val_loss: 0.5823 - val_accuracy: 0.8516\n",
            "Epoch 159/400\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.3506 - accuracy: 0.9223 - val_loss: 0.6173 - val_accuracy: 0.8452\n",
            "Epoch 160/400\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.3442 - accuracy: 0.9029 - val_loss: 0.6322 - val_accuracy: 0.8258\n",
            "Epoch 161/400\n",
            " 2/20 [==>...........................] - ETA: 2s - loss: 0.3093 - accuracy: 0.9219"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get model summary\n",
        "# model.summary()\n",
        "\n",
        "# # Get training and validation accuracy\n",
        "# train_accuracy = history.history['accuracy']\n",
        "# val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# # Get training and validation loss\n",
        "# train_loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaQTq9CoSq3F",
        "outputId": "ffe310fe-7942-44ec-dab6-e67b6d2ce521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_7 (TimeDis  (None, 16, 32, 16)       112       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 16, 16, 16)       0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 16, 14, 128)      6272      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_10 (TimeDi  (None, 16, 7, 128)       0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_11 (TimeDi  (None, 16, 5, 50)        19250     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_12 (TimeDi  (None, 16, 2, 50)        0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_13 (TimeDi  (None, 16, 50)           0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               60400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 80)                8080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                1620      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95,734\n",
            "Trainable params: 95,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(XKey_test,yKey_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN9KVXyOiewC",
        "outputId": "b89ae554-2937-4069-c511-80adeb576311"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 53ms/step - loss: 0.5969 - accuracy: 0.9070\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5968587398529053, 0.9069767594337463]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "model.save('PoseStreamFrame.h5')\n",
        "\n",
        "# Load the saved model from the file\n",
        "loaded_model = load_model('PoseStreamFrame.h5')"
      ],
      "metadata": {
        "id": "Yfsu-uUIjN13"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.evaluate(XKey_test,yKey_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_SttZ58j9MJ",
        "outputId": "78738c5c-6417-43e2-f5ac-7fce1a247217"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 1s 65ms/step - loss: 0.7392 - accuracy: 0.8643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.739200234413147, 0.8643410801887512]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = new_train_keypoints[0]\n",
        "# Reshape the input data to include the batch size dimension\n",
        "input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "# Make a prediction with the model\n",
        "output = model.predict(input_data)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzW5u9b4k18g",
        "outputId": "19985010-266d-4420-f73d-c27c64c4c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 299ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.99708712e-01, 2.62607158e-11, 8.06678546e-09, 1.93838652e-08,\n",
              "        4.37154535e-09, 3.91757027e-07, 7.37984745e-19, 7.65237303e-07,\n",
              "        2.77937128e-04, 2.42370291e-11, 1.20226105e-05, 3.20784697e-12,\n",
              "        1.26892900e-07, 1.34451783e-09, 6.71775569e-13, 3.91274763e-10,\n",
              "        7.01205912e-13, 3.00378666e-10, 9.27593319e-15, 4.94650870e-13]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "GPUzTSH7k50O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638bb919-beaa-4264-bae2-ba0358e24a55"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [21.697357177734375,\n",
              "  15.334752082824707,\n",
              "  10.374938011169434,\n",
              "  6.945734977722168,\n",
              "  4.801348686218262,\n",
              "  3.59242844581604,\n",
              "  2.8934285640716553,\n",
              "  2.7839713096618652,\n",
              "  2.6048779487609863,\n",
              "  2.5306413173675537,\n",
              "  2.5051467418670654,\n",
              "  2.4700725078582764,\n",
              "  2.3183608055114746,\n",
              "  2.2186343669891357,\n",
              "  2.219428062438965,\n",
              "  2.204704523086548,\n",
              "  2.1441214084625244,\n",
              "  2.101477861404419,\n",
              "  2.064624547958374,\n",
              "  2.0200462341308594,\n",
              "  1.9735335111618042,\n",
              "  1.9881101846694946,\n",
              "  1.8807616233825684,\n",
              "  1.8255115747451782,\n",
              "  1.796167016029358,\n",
              "  1.7502628564834595,\n",
              "  1.8021289110183716,\n",
              "  1.7942311763763428,\n",
              "  1.7058643102645874,\n",
              "  1.6513512134552002,\n",
              "  1.6281332969665527,\n",
              "  1.5865105390548706,\n",
              "  1.5486098527908325,\n",
              "  1.5636802911758423,\n",
              "  1.539646863937378,\n",
              "  1.5715489387512207,\n",
              "  1.4929057359695435,\n",
              "  1.4360692501068115,\n",
              "  1.3862661123275757,\n",
              "  1.3283048868179321,\n",
              "  1.2949881553649902,\n",
              "  1.3075218200683594,\n",
              "  1.1945618391036987,\n",
              "  1.2245590686798096,\n",
              "  1.1564655303955078,\n",
              "  1.1601319313049316,\n",
              "  1.0828495025634766,\n",
              "  1.1008566617965698,\n",
              "  1.0794758796691895,\n",
              "  1.168844223022461,\n",
              "  1.0477032661437988,\n",
              "  1.0381593704223633,\n",
              "  0.9589099287986755,\n",
              "  0.9939118027687073,\n",
              "  0.9336200952529907,\n",
              "  0.9132868051528931,\n",
              "  0.8782263994216919,\n",
              "  0.9002148509025574,\n",
              "  0.9267409443855286,\n",
              "  0.9430218935012817,\n",
              "  0.85881108045578,\n",
              "  0.8634685277938843,\n",
              "  0.8309898972511292,\n",
              "  0.8282844424247742,\n",
              "  0.8224249482154846,\n",
              "  0.8390000462532043,\n",
              "  0.8491878509521484,\n",
              "  0.7867288589477539,\n",
              "  0.7768445611000061,\n",
              "  0.7740469574928284,\n",
              "  0.7689253091812134,\n",
              "  0.7428708076477051,\n",
              "  0.7162260413169861,\n",
              "  0.755881130695343,\n",
              "  0.7215197682380676,\n",
              "  0.7885614037513733,\n",
              "  0.7469620704650879,\n",
              "  0.7325073480606079,\n",
              "  0.7476467490196228,\n",
              "  0.726381242275238,\n",
              "  0.7400230765342712,\n",
              "  0.6962136626243591,\n",
              "  0.6924495100975037,\n",
              "  0.7571061253547668,\n",
              "  0.8176351189613342,\n",
              "  0.7080281376838684,\n",
              "  0.6549909710884094,\n",
              "  0.6217132806777954,\n",
              "  0.6379427909851074,\n",
              "  0.6925114393234253,\n",
              "  0.6487424373626709,\n",
              "  0.87381911277771,\n",
              "  0.7444082498550415,\n",
              "  0.6815469861030579,\n",
              "  0.6126934885978699,\n",
              "  0.5704524517059326,\n",
              "  0.6238189935684204,\n",
              "  0.6662655472755432,\n",
              "  0.6754522323608398,\n",
              "  0.6477757692337036,\n",
              "  0.6766358613967896,\n",
              "  0.6636713743209839,\n",
              "  0.6698733568191528,\n",
              "  0.6437652111053467,\n",
              "  0.6149874925613403,\n",
              "  0.6033655405044556,\n",
              "  0.5993136167526245,\n",
              "  0.5971485376358032,\n",
              "  0.5335411429405212,\n",
              "  0.5577492117881775,\n",
              "  0.5500048995018005,\n",
              "  0.6290122866630554,\n",
              "  0.5639044046401978,\n",
              "  0.5651640892028809,\n",
              "  0.5365751385688782,\n",
              "  0.5446491837501526,\n",
              "  0.6468196511268616,\n",
              "  0.573089599609375,\n",
              "  0.5387172102928162,\n",
              "  0.5343040823936462,\n",
              "  0.5330759286880493,\n",
              "  0.5376212000846863,\n",
              "  0.4708009958267212,\n",
              "  0.5000197291374207,\n",
              "  0.5025792717933655,\n",
              "  0.5326360464096069,\n",
              "  0.599067211151123,\n",
              "  0.6987951397895813,\n",
              "  0.548374593257904,\n",
              "  0.49204114079475403,\n",
              "  0.48358431458473206,\n",
              "  0.47242096066474915,\n",
              "  0.46838831901550293,\n",
              "  0.4851023256778717,\n",
              "  0.4703926742076874,\n",
              "  0.4651968777179718,\n",
              "  0.4499592185020447,\n",
              "  0.4860785901546478,\n",
              "  0.4557152986526489,\n",
              "  0.47623318433761597,\n",
              "  0.49524345993995667,\n",
              "  0.4391935169696808,\n",
              "  0.4547309875488281,\n",
              "  0.48559120297431946,\n",
              "  0.4658733904361725,\n",
              "  0.45631143450737,\n",
              "  0.4699416160583496,\n",
              "  0.5325895547866821,\n",
              "  0.4350830912590027,\n",
              "  0.4460660517215729,\n",
              "  0.4492628574371338,\n",
              "  0.44659388065338135,\n",
              "  0.3928404450416565,\n",
              "  0.4646459221839905,\n",
              "  0.47893986105918884,\n",
              "  0.49122747778892517,\n",
              "  0.48529502749443054,\n",
              "  0.44619855284690857,\n",
              "  0.4623434841632843,\n",
              "  0.44675636291503906,\n",
              "  0.3982484042644501,\n",
              "  0.440786749124527,\n",
              "  0.44253501296043396,\n",
              "  0.4070582985877991,\n",
              "  0.38638728857040405,\n",
              "  0.419202595949173,\n",
              "  0.4936775863170624,\n",
              "  0.400267094373703,\n",
              "  0.4678627550601959,\n",
              "  0.3672412633895874,\n",
              "  0.3812061548233032,\n",
              "  0.4296463131904602,\n",
              "  0.3865025043487549,\n",
              "  0.3924350142478943,\n",
              "  0.449181467294693,\n",
              "  0.4395279884338379,\n",
              "  0.37706470489501953,\n",
              "  0.3670618534088135,\n",
              "  0.387008935213089,\n",
              "  0.3610098361968994,\n",
              "  0.39432963728904724,\n",
              "  0.3510315716266632,\n",
              "  0.394959032535553,\n",
              "  0.4517383277416229,\n",
              "  0.4366863965988159,\n",
              "  0.4235645830631256,\n",
              "  0.3951956629753113,\n",
              "  0.3541701138019562,\n",
              "  0.36931347846984863,\n",
              "  0.3408164083957672,\n",
              "  0.3559415340423584,\n",
              "  0.3650793135166168,\n",
              "  0.38867422938346863,\n",
              "  0.35694819688796997,\n",
              "  0.34883424639701843,\n",
              "  0.37596237659454346,\n",
              "  0.3676657974720001,\n",
              "  0.31141233444213867,\n",
              "  0.3179532289505005,\n",
              "  0.3143235146999359,\n",
              "  0.333757221698761,\n",
              "  0.3800247609615326,\n",
              "  0.3671131432056427,\n",
              "  0.39575108885765076,\n",
              "  0.37951117753982544,\n",
              "  0.4632939100265503,\n",
              "  0.41544225811958313,\n",
              "  0.38009199500083923,\n",
              "  0.38796141743659973,\n",
              "  0.3246040344238281,\n",
              "  0.2975005805492401,\n",
              "  0.3418974280357361,\n",
              "  0.39262545108795166,\n",
              "  0.3697303235530853,\n",
              "  0.36921656131744385,\n",
              "  0.31531408429145813,\n",
              "  0.3219911754131317,\n",
              "  0.3157414197921753,\n",
              "  0.497839093208313,\n",
              "  0.36641255021095276,\n",
              "  0.3734375834465027,\n",
              "  0.32124772667884827,\n",
              "  0.30015310645103455,\n",
              "  0.2943854331970215,\n",
              "  0.3341263234615326,\n",
              "  0.35308969020843506,\n",
              "  0.33841079473495483,\n",
              "  0.32105323672294617,\n",
              "  0.3306252062320709,\n",
              "  0.3158840835094452,\n",
              "  0.2900719940662384,\n",
              "  0.3230820298194885,\n",
              "  0.32473644614219666,\n",
              "  0.30570629239082336,\n",
              "  0.2789241671562195,\n",
              "  0.2814931571483612,\n",
              "  0.279990553855896,\n",
              "  0.3086133301258087,\n",
              "  0.31101348996162415,\n",
              "  0.3485701382160187,\n",
              "  0.33687058091163635,\n",
              "  0.2746070325374603,\n",
              "  0.3129516541957855,\n",
              "  0.3704759478569031,\n",
              "  0.3859131336212158,\n",
              "  0.313831627368927,\n",
              "  0.24752481281757355,\n",
              "  0.28102341294288635,\n",
              "  0.3521231412887573,\n",
              "  0.33350759744644165,\n",
              "  0.2992711067199707,\n",
              "  0.36678066849708557,\n",
              "  0.29886335134506226,\n",
              "  0.3128455579280853,\n",
              "  0.28800687193870544,\n",
              "  0.27105480432510376,\n",
              "  0.2740461826324463,\n",
              "  0.24452202022075653,\n",
              "  0.2822164297103882,\n",
              "  0.30168619751930237,\n",
              "  0.3042798638343811,\n",
              "  0.37032124400138855,\n",
              "  0.3409992754459381,\n",
              "  0.3076745271682739,\n",
              "  0.31351912021636963,\n",
              "  0.31776055693626404,\n",
              "  0.2966374158859253,\n",
              "  0.25614261627197266,\n",
              "  0.3186086118221283,\n",
              "  0.2686810791492462,\n",
              "  0.3069743812084198,\n",
              "  0.3051674962043762,\n",
              "  0.2987176179885864,\n",
              "  0.2727389931678772,\n",
              "  0.2851431369781494,\n",
              "  0.32243821024894714,\n",
              "  0.33316028118133545,\n",
              "  0.26865798234939575,\n",
              "  0.2868385910987854,\n",
              "  0.24861952662467957,\n",
              "  0.2597162425518036,\n",
              "  0.2668701708316803,\n",
              "  0.31868553161621094,\n",
              "  0.30411067605018616,\n",
              "  0.2699728310108185,\n",
              "  0.2670227289199829,\n",
              "  0.29318147897720337,\n",
              "  0.24878910183906555,\n",
              "  0.23319053649902344,\n",
              "  0.2566412687301636,\n",
              "  0.26854220032691956,\n",
              "  0.34949925541877747,\n",
              "  0.3412296772003174,\n",
              "  0.2746143937110901,\n",
              "  0.21197457611560822,\n",
              "  0.20843619108200073,\n",
              "  0.24054725468158722,\n",
              "  0.24003878235816956,\n",
              "  0.23068860173225403,\n",
              "  0.29529881477355957],\n",
              " 'accuracy': [0.09061488509178162,\n",
              "  0.13106796145439148,\n",
              "  0.16343042254447937,\n",
              "  0.28317153453826904,\n",
              "  0.3090614974498749,\n",
              "  0.33656957745552063,\n",
              "  0.3980582654476166,\n",
              "  0.3462783098220825,\n",
              "  0.40614888072013855,\n",
              "  0.40776699781417847,\n",
              "  0.40776699781417847,\n",
              "  0.40291261672973633,\n",
              "  0.4789643883705139,\n",
              "  0.48543688654899597,\n",
              "  0.46278315782546997,\n",
              "  0.4789643883705139,\n",
              "  0.49352750182151794,\n",
              "  0.5113268494606018,\n",
              "  0.49352750182151794,\n",
              "  0.49676376581192017,\n",
              "  0.5355986952781677,\n",
              "  0.5339806079864502,\n",
              "  0.5355986952781677,\n",
              "  0.5339806079864502,\n",
              "  0.53721684217453,\n",
              "  0.5889967679977417,\n",
              "  0.5323624610900879,\n",
              "  0.5355986952781677,\n",
              "  0.5776699185371399,\n",
              "  0.5744336843490601,\n",
              "  0.5792880058288574,\n",
              "  0.5695793032646179,\n",
              "  0.6051779985427856,\n",
              "  0.590614914894104,\n",
              "  0.6084142327308655,\n",
              "  0.5954692363739014,\n",
              "  0.6100323796272278,\n",
              "  0.6003236174583435,\n",
              "  0.6213592290878296,\n",
              "  0.6488673090934753,\n",
              "  0.6537216901779175,\n",
              "  0.6326860785484314,\n",
              "  0.6990291476249695,\n",
              "  0.6423948407173157,\n",
              "  0.700647234916687,\n",
              "  0.6957928538322449,\n",
              "  0.6909385323524475,\n",
              "  0.6747573018074036,\n",
              "  0.6666666865348816,\n",
              "  0.6456310749053955,\n",
              "  0.6860841512680054,\n",
              "  0.6877022385597229,\n",
              "  0.692556619644165,\n",
              "  0.6893203854560852,\n",
              "  0.7135922312736511,\n",
              "  0.7055016160011292,\n",
              "  0.7233009934425354,\n",
              "  0.708737850189209,\n",
              "  0.6941747665405273,\n",
              "  0.708737850189209,\n",
              "  0.737864077091217,\n",
              "  0.737864077091217,\n",
              "  0.7362459301948547,\n",
              "  0.7556634545326233,\n",
              "  0.737864077091217,\n",
              "  0.737864077091217,\n",
              "  0.7411003112792969,\n",
              "  0.7686083912849426,\n",
              "  0.7524271607398987,\n",
              "  0.7491909265518188,\n",
              "  0.7734627723693848,\n",
              "  0.762135922908783,\n",
              "  0.8090614676475525,\n",
              "  0.7669903039932251,\n",
              "  0.7766990065574646,\n",
              "  0.762135922908783,\n",
              "  0.7815533876419067,\n",
              "  0.7766990065574646,\n",
              "  0.7847896218299866,\n",
              "  0.7734627723693848,\n",
              "  0.783171534538269,\n",
              "  0.8139158487319946,\n",
              "  0.7847896218299866,\n",
              "  0.7702265381813049,\n",
              "  0.7605177760124207,\n",
              "  0.807443380355835,\n",
              "  0.8090614676475525,\n",
              "  0.8203883767127991,\n",
              "  0.7961165308952332,\n",
              "  0.7880259156227112,\n",
              "  0.7928802371025085,\n",
              "  0.7313916087150574,\n",
              "  0.7928802371025085,\n",
              "  0.7864077687263489,\n",
              "  0.8236246109008789,\n",
              "  0.8414239287376404,\n",
              "  0.8106796145439148,\n",
              "  0.8025889992713928,\n",
              "  0.7928802371025085,\n",
              "  0.8106796145439148,\n",
              "  0.7961165308952332,\n",
              "  0.807443380355835,\n",
              "  0.8155339956283569,\n",
              "  0.8236246109008789,\n",
              "  0.8106796145439148,\n",
              "  0.8268608450889587,\n",
              "  0.8268608450889587,\n",
              "  0.8171520829200745,\n",
              "  0.8478964567184448,\n",
              "  0.8462783098220825,\n",
              "  0.8349514603614807,\n",
              "  0.8220064640045166,\n",
              "  0.8333333134651184,\n",
              "  0.8300970792770386,\n",
              "  0.8495145440101624,\n",
              "  0.836569607257843,\n",
              "  0.8090614676475525,\n",
              "  0.852750837802887,\n",
              "  0.8462783098220825,\n",
              "  0.8478964567184448,\n",
              "  0.8543689250946045,\n",
              "  0.8478964567184448,\n",
              "  0.8640776872634888,\n",
              "  0.8656957745552063,\n",
              "  0.8608414530754089,\n",
              "  0.8430420756340027,\n",
              "  0.8300970792770386,\n",
              "  0.7928802371025085,\n",
              "  0.8673139214515686,\n",
              "  0.8608414530754089,\n",
              "  0.8592233061790466,\n",
              "  0.8689320683479309,\n",
              "  0.8770226240158081,\n",
              "  0.8511326909065247,\n",
              "  0.8495145440101624,\n",
              "  0.8705501556396484,\n",
              "  0.8721683025360107,\n",
              "  0.8770226240158081,\n",
              "  0.8721683025360107,\n",
              "  0.8705501556396484,\n",
              "  0.852750837802887,\n",
              "  0.8673139214515686,\n",
              "  0.8754045367240906,\n",
              "  0.8640776872634888,\n",
              "  0.8754045367240906,\n",
              "  0.8673139214515686,\n",
              "  0.8608414530754089,\n",
              "  0.8576051592826843,\n",
              "  0.8786407709121704,\n",
              "  0.8851132392883301,\n",
              "  0.8689320683479309,\n",
              "  0.8721683025360107,\n",
              "  0.8948220014572144,\n",
              "  0.8559870719909668,\n",
              "  0.8592233061790466,\n",
              "  0.8754045367240906,\n",
              "  0.8592233061790466,\n",
              "  0.8818770051002502,\n",
              "  0.8705501556396484,\n",
              "  0.8818770051002502,\n",
              "  0.8964401483535767,\n",
              "  0.8737863898277283,\n",
              "  0.8656957745552063,\n",
              "  0.8867313861846924,\n",
              "  0.8948220014572144,\n",
              "  0.8786407709121704,\n",
              "  0.8608414530754089,\n",
              "  0.8980582356452942,\n",
              "  0.8705501556396484,\n",
              "  0.9110032320022583,\n",
              "  0.8851132392883301,\n",
              "  0.8915857672691345,\n",
              "  0.8996763825416565,\n",
              "  0.8899676203727722,\n",
              "  0.8786407709121704,\n",
              "  0.8802589178085327,\n",
              "  0.9029126167297363,\n",
              "  0.8980582356452942,\n",
              "  0.893203854560852,\n",
              "  0.893203854560852,\n",
              "  0.901294469833374,\n",
              "  0.9190938472747803,\n",
              "  0.8996763825416565,\n",
              "  0.8737863898277283,\n",
              "  0.8786407709121704,\n",
              "  0.8851132392883301,\n",
              "  0.8915857672691345,\n",
              "  0.9045307636260986,\n",
              "  0.8964401483535767,\n",
              "  0.9029126167297363,\n",
              "  0.8948220014572144,\n",
              "  0.8964401483535767,\n",
              "  0.8883495330810547,\n",
              "  0.909385085105896,\n",
              "  0.909385085105896,\n",
              "  0.893203854560852,\n",
              "  0.9029126167297363,\n",
              "  0.9239482283592224,\n",
              "  0.9061488509178162,\n",
              "  0.9207119941711426,\n",
              "  0.9110032320022583,\n",
              "  0.893203854560852,\n",
              "  0.8980582356452942,\n",
              "  0.8996763825416565,\n",
              "  0.9077669978141785,\n",
              "  0.8673139214515686,\n",
              "  0.8883495330810547,\n",
              "  0.8996763825416565,\n",
              "  0.8915857672691345,\n",
              "  0.9158576130867004,\n",
              "  0.9239482283592224,\n",
              "  0.9126213788986206,\n",
              "  0.8980582356452942,\n",
              "  0.9126213788986206,\n",
              "  0.8899676203727722,\n",
              "  0.9190938472747803,\n",
              "  0.9239482283592224,\n",
              "  0.9158576130867004,\n",
              "  0.8559870719909668,\n",
              "  0.901294469833374,\n",
              "  0.8964401483535767,\n",
              "  0.9223300814628601,\n",
              "  0.9288026094436646,\n",
              "  0.9320388436317444,\n",
              "  0.9029126167297363,\n",
              "  0.9029126167297363,\n",
              "  0.9126213788986206,\n",
              "  0.9158576130867004,\n",
              "  0.909385085105896,\n",
              "  0.9288026094436646,\n",
              "  0.9336569309234619,\n",
              "  0.9336569309234619,\n",
              "  0.9126213788986206,\n",
              "  0.9288026094436646,\n",
              "  0.9304206967353821,\n",
              "  0.9288026094436646,\n",
              "  0.9255663156509399,\n",
              "  0.917475700378418,\n",
              "  0.9190938472747803,\n",
              "  0.9061488509178162,\n",
              "  0.9190938472747803,\n",
              "  0.9320388436317444,\n",
              "  0.9126213788986206,\n",
              "  0.8948220014572144,\n",
              "  0.8964401483535767,\n",
              "  0.9239482283592224,\n",
              "  0.9401294589042664,\n",
              "  0.9223300814628601,\n",
              "  0.901294469833374,\n",
              "  0.9320388436317444,\n",
              "  0.917475700378418,\n",
              "  0.8948220014572144,\n",
              "  0.9239482283592224,\n",
              "  0.9271844625473022,\n",
              "  0.9207119941711426,\n",
              "  0.9336569309234619,\n",
              "  0.9207119941711426,\n",
              "  0.9401294589042664,\n",
              "  0.917475700378418,\n",
              "  0.9190938472747803,\n",
              "  0.9304206967353821,\n",
              "  0.9029126167297363,\n",
              "  0.9126213788986206,\n",
              "  0.9255663156509399,\n",
              "  0.917475700378418,\n",
              "  0.9110032320022583,\n",
              "  0.9304206967353821,\n",
              "  0.9368932247161865,\n",
              "  0.9255663156509399,\n",
              "  0.9401294589042664,\n",
              "  0.9255663156509399,\n",
              "  0.9336569309234619,\n",
              "  0.9368932247161865,\n",
              "  0.9368932247161865,\n",
              "  0.9352750778198242,\n",
              "  0.9158576130867004,\n",
              "  0.9207119941711426,\n",
              "  0.9449838399887085,\n",
              "  0.9352750778198242,\n",
              "  0.9368932247161865,\n",
              "  0.9288026094436646,\n",
              "  0.9401294589042664,\n",
              "  0.9239482283592224,\n",
              "  0.9255663156509399,\n",
              "  0.9320388436317444,\n",
              "  0.938511312007904,\n",
              "  0.9304206967353821,\n",
              "  0.938511312007904,\n",
              "  0.9530744552612305,\n",
              "  0.9352750778198242,\n",
              "  0.9336569309234619,\n",
              "  0.9255663156509399,\n",
              "  0.917475700378418,\n",
              "  0.9401294589042664,\n",
              "  0.9530744552612305,\n",
              "  0.954692542552948,\n",
              "  0.9368932247161865,\n",
              "  0.9498381614685059,\n",
              "  0.9482200741767883,\n",
              "  0.917475700378418],\n",
              " 'val_loss': [17.998321533203125,\n",
              "  12.424026489257812,\n",
              "  8.305891990661621,\n",
              "  5.582507133483887,\n",
              "  3.9785778522491455,\n",
              "  3.136904239654541,\n",
              "  2.9035537242889404,\n",
              "  2.6752312183380127,\n",
              "  2.5326852798461914,\n",
              "  2.4749269485473633,\n",
              "  2.4412643909454346,\n",
              "  2.31746768951416,\n",
              "  2.2297215461730957,\n",
              "  2.1780879497528076,\n",
              "  2.191498041152954,\n",
              "  2.1180379390716553,\n",
              "  2.055387020111084,\n",
              "  2.091679811477661,\n",
              "  1.9994481801986694,\n",
              "  1.9408860206604004,\n",
              "  1.9292417764663696,\n",
              "  1.8820312023162842,\n",
              "  1.8174797296524048,\n",
              "  1.739161729812622,\n",
              "  1.719650149345398,\n",
              "  1.7259950637817383,\n",
              "  1.750370979309082,\n",
              "  1.7299085855484009,\n",
              "  1.6843860149383545,\n",
              "  1.6211341619491577,\n",
              "  1.5912333726882935,\n",
              "  1.531211495399475,\n",
              "  1.528672695159912,\n",
              "  1.52036452293396,\n",
              "  1.5114893913269043,\n",
              "  1.508468508720398,\n",
              "  1.4963856935501099,\n",
              "  1.4776512384414673,\n",
              "  1.4076849222183228,\n",
              "  1.3619755506515503,\n",
              "  1.3069664239883423,\n",
              "  1.2714581489562988,\n",
              "  1.2473247051239014,\n",
              "  1.2001441717147827,\n",
              "  1.1491636037826538,\n",
              "  1.2071142196655273,\n",
              "  1.1440130472183228,\n",
              "  1.1057538986206055,\n",
              "  1.2142454385757446,\n",
              "  1.1549265384674072,\n",
              "  1.0758252143859863,\n",
              "  1.1145979166030884,\n",
              "  1.0507678985595703,\n",
              "  0.9437766671180725,\n",
              "  0.9745783805847168,\n",
              "  1.0375841856002808,\n",
              "  0.9430070519447327,\n",
              "  1.0484251976013184,\n",
              "  1.0383398532867432,\n",
              "  1.0021268129348755,\n",
              "  0.9528464078903198,\n",
              "  0.8970786929130554,\n",
              "  0.9014519453048706,\n",
              "  0.8943026661872864,\n",
              "  1.0097190141677856,\n",
              "  0.9106288552284241,\n",
              "  1.0220496654510498,\n",
              "  0.8765591382980347,\n",
              "  0.8957849144935608,\n",
              "  0.9053433537483215,\n",
              "  0.8220740556716919,\n",
              "  0.778965175151825,\n",
              "  0.859756588935852,\n",
              "  0.8098294138908386,\n",
              "  0.8998949527740479,\n",
              "  0.8600480556488037,\n",
              "  0.8266119956970215,\n",
              "  0.8623102903366089,\n",
              "  0.8370819091796875,\n",
              "  0.8534813523292542,\n",
              "  0.798698902130127,\n",
              "  0.7712088227272034,\n",
              "  0.9771300554275513,\n",
              "  1.002884030342102,\n",
              "  0.8367719054222107,\n",
              "  0.7375458478927612,\n",
              "  0.7514773011207581,\n",
              "  0.82657390832901,\n",
              "  0.774516761302948,\n",
              "  0.8741758465766907,\n",
              "  0.79520183801651,\n",
              "  0.959204375743866,\n",
              "  0.795477032661438,\n",
              "  0.7304128408432007,\n",
              "  0.7136433720588684,\n",
              "  0.7175655961036682,\n",
              "  0.7071459889411926,\n",
              "  0.8453278541564941,\n",
              "  0.760954737663269,\n",
              "  0.7532710433006287,\n",
              "  0.817430853843689,\n",
              "  0.7302460074424744,\n",
              "  0.783947229385376,\n",
              "  0.6978099942207336,\n",
              "  0.7801069021224976,\n",
              "  0.6806817054748535,\n",
              "  0.7064074873924255,\n",
              "  0.7403080463409424,\n",
              "  0.6633930802345276,\n",
              "  0.6816223859786987,\n",
              "  0.7437138557434082,\n",
              "  0.7129971385002136,\n",
              "  0.6835818886756897,\n",
              "  0.7709158658981323,\n",
              "  0.6644707322120667,\n",
              "  0.6804408431053162,\n",
              "  0.7118826508522034,\n",
              "  0.7011703252792358,\n",
              "  0.6604905128479004,\n",
              "  0.6774033308029175,\n",
              "  0.6752826571464539,\n",
              "  0.6832718253135681,\n",
              "  0.6646685004234314,\n",
              "  0.6452718377113342,\n",
              "  0.789046049118042,\n",
              "  0.6834672689437866,\n",
              "  1.0044840574264526,\n",
              "  0.7356300354003906,\n",
              "  0.6394408941268921,\n",
              "  0.6334407329559326,\n",
              "  0.6391024589538574,\n",
              "  0.6697598099708557,\n",
              "  0.6152838468551636,\n",
              "  0.6434270143508911,\n",
              "  0.6437668800354004,\n",
              "  0.6029915809631348,\n",
              "  0.656431257724762,\n",
              "  0.6272909641265869,\n",
              "  0.5928767323493958,\n",
              "  0.7323892116546631,\n",
              "  0.6838995814323425,\n",
              "  0.641147255897522,\n",
              "  0.6519666910171509,\n",
              "  0.6533486247062683,\n",
              "  0.6439328789710999,\n",
              "  0.6508417129516602,\n",
              "  0.7551109194755554,\n",
              "  0.6999525427818298,\n",
              "  0.6585054397583008,\n",
              "  0.7489272356033325,\n",
              "  0.6952291131019592,\n",
              "  0.5808695554733276,\n",
              "  0.6063035726547241,\n",
              "  0.7010601162910461,\n",
              "  0.6298574209213257,\n",
              "  0.7608925104141235,\n",
              "  0.6177136301994324,\n",
              "  0.6783128380775452,\n",
              "  0.6477848887443542,\n",
              "  0.5952584743499756,\n",
              "  0.6715558171272278,\n",
              "  0.7051323652267456,\n",
              "  0.5957780480384827,\n",
              "  0.6389943957328796,\n",
              "  0.6299881339073181,\n",
              "  0.6185500621795654,\n",
              "  0.6367282271385193,\n",
              "  0.6929284334182739,\n",
              "  0.5795654058456421,\n",
              "  0.6169995665550232,\n",
              "  0.6382231116294861,\n",
              "  0.63408362865448,\n",
              "  0.5990537405014038,\n",
              "  0.5864154696464539,\n",
              "  0.6342282891273499,\n",
              "  0.6668403744697571,\n",
              "  0.5688806176185608,\n",
              "  0.6237854957580566,\n",
              "  0.5842422246932983,\n",
              "  0.658171534538269,\n",
              "  0.6061977744102478,\n",
              "  0.6086839437484741,\n",
              "  0.6320164799690247,\n",
              "  0.7640345096588135,\n",
              "  0.6888928413391113,\n",
              "  0.6480635404586792,\n",
              "  0.6633073091506958,\n",
              "  0.5627357959747314,\n",
              "  0.5690434575080872,\n",
              "  0.5751037001609802,\n",
              "  0.6138123273849487,\n",
              "  0.6938963532447815,\n",
              "  0.6099385619163513,\n",
              "  0.6281759738922119,\n",
              "  0.592100203037262,\n",
              "  0.6002840399742126,\n",
              "  0.6103505492210388,\n",
              "  0.5931110978126526,\n",
              "  0.5866572260856628,\n",
              "  0.6153321862220764,\n",
              "  0.647006630897522,\n",
              "  0.6588504314422607,\n",
              "  0.6567590236663818,\n",
              "  0.6793664693832397,\n",
              "  0.6294075846672058,\n",
              "  0.7174240350723267,\n",
              "  0.6701876521110535,\n",
              "  0.7042814493179321,\n",
              "  0.6114543080329895,\n",
              "  0.6168627738952637,\n",
              "  0.5915297865867615,\n",
              "  0.6845260262489319,\n",
              "  0.7179173231124878,\n",
              "  0.6305235624313354,\n",
              "  0.5791522264480591,\n",
              "  0.6312201023101807,\n",
              "  0.6193972229957581,\n",
              "  0.6765289306640625,\n",
              "  0.6868536472320557,\n",
              "  0.6410704255104065,\n",
              "  0.629056990146637,\n",
              "  0.6341404318809509,\n",
              "  0.6194007992744446,\n",
              "  0.6364196538925171,\n",
              "  0.6407193541526794,\n",
              "  0.6539254188537598,\n",
              "  0.6678779125213623,\n",
              "  0.6262689232826233,\n",
              "  0.6379421353340149,\n",
              "  0.6365808844566345,\n",
              "  0.6710092425346375,\n",
              "  0.6292964816093445,\n",
              "  0.6315876245498657,\n",
              "  0.5938788652420044,\n",
              "  0.6279094815254211,\n",
              "  0.6348011493682861,\n",
              "  0.6313146352767944,\n",
              "  0.6479960083961487,\n",
              "  0.6763817667961121,\n",
              "  0.6923465728759766,\n",
              "  0.6010469794273376,\n",
              "  0.7312109470367432,\n",
              "  0.6901557445526123,\n",
              "  0.6993394494056702,\n",
              "  0.6839427947998047,\n",
              "  0.6758109927177429,\n",
              "  0.6032347679138184,\n",
              "  0.7433755397796631,\n",
              "  0.6570878624916077,\n",
              "  0.6556431651115417,\n",
              "  0.7285321950912476,\n",
              "  0.6308016180992126,\n",
              "  0.7052636742591858,\n",
              "  0.6302411556243896,\n",
              "  0.648857593536377,\n",
              "  0.6421841382980347,\n",
              "  0.6381710767745972,\n",
              "  0.7162539958953857,\n",
              "  0.6462276577949524,\n",
              "  0.7139781713485718,\n",
              "  0.8369537591934204,\n",
              "  0.6626155972480774,\n",
              "  0.7043709754943848,\n",
              "  0.709869921207428,\n",
              "  0.7353901267051697,\n",
              "  0.739446759223938,\n",
              "  0.6621044278144836,\n",
              "  0.6743288040161133,\n",
              "  0.6529523730278015,\n",
              "  0.6744739413261414,\n",
              "  0.697027862071991,\n",
              "  0.6771283745765686,\n",
              "  0.6432098150253296,\n",
              "  0.6337599158287048,\n",
              "  0.7234319448471069,\n",
              "  0.7111330032348633,\n",
              "  0.6719806790351868,\n",
              "  0.6643528342247009,\n",
              "  0.6106663346290588,\n",
              "  0.6732386350631714,\n",
              "  0.6600216031074524,\n",
              "  0.724157452583313,\n",
              "  0.6627922654151917,\n",
              "  0.6363392472267151,\n",
              "  0.6575019359588623,\n",
              "  0.6410409212112427,\n",
              "  0.6680680513381958,\n",
              "  0.6632740497589111,\n",
              "  0.6459423303604126,\n",
              "  0.6894166469573975,\n",
              "  0.7089663743972778,\n",
              "  0.8326711058616638,\n",
              "  0.6608376502990723,\n",
              "  0.6564053297042847,\n",
              "  0.6877313852310181,\n",
              "  0.63215571641922,\n",
              "  0.7259758114814758,\n",
              "  0.6463744044303894,\n",
              "  0.6893678903579712,\n",
              "  0.8474025726318359],\n",
              " 'val_accuracy': [0.14193548262119293,\n",
              "  0.16774193942546844,\n",
              "  0.22580644488334656,\n",
              "  0.28387096524238586,\n",
              "  0.3677419424057007,\n",
              "  0.47096773982048035,\n",
              "  0.38064515590667725,\n",
              "  0.4838709533214569,\n",
              "  0.42580646276474,\n",
              "  0.46451613306999207,\n",
              "  0.4580645263195038,\n",
              "  0.5870967507362366,\n",
              "  0.5935483574867249,\n",
              "  0.5870967507362366,\n",
              "  0.5096774101257324,\n",
              "  0.57419353723526,\n",
              "  0.5935483574867249,\n",
              "  0.5548387169837952,\n",
              "  0.5677419304847717,\n",
              "  0.5935483574867249,\n",
              "  0.6000000238418579,\n",
              "  0.6064516305923462,\n",
              "  0.6193548440933228,\n",
              "  0.625806450843811,\n",
              "  0.6451612710952759,\n",
              "  0.6064516305923462,\n",
              "  0.6129032373428345,\n",
              "  0.6129032373428345,\n",
              "  0.6129032373428345,\n",
              "  0.6000000238418579,\n",
              "  0.6387096643447876,\n",
              "  0.6967741847038269,\n",
              "  0.6709677577018738,\n",
              "  0.5935483574867249,\n",
              "  0.6709677577018738,\n",
              "  0.6193548440933228,\n",
              "  0.5806451439857483,\n",
              "  0.6322580575942993,\n",
              "  0.6516128778457642,\n",
              "  0.6193548440933228,\n",
              "  0.6774193644523621,\n",
              "  0.6580645442008972,\n",
              "  0.6774193644523621,\n",
              "  0.6967741847038269,\n",
              "  0.6967741847038269,\n",
              "  0.7419354915618896,\n",
              "  0.6709677577018738,\n",
              "  0.6903225779533386,\n",
              "  0.6451612710952759,\n",
              "  0.6645161509513855,\n",
              "  0.7032257914543152,\n",
              "  0.6580645442008972,\n",
              "  0.6774193644523621,\n",
              "  0.7354838848114014,\n",
              "  0.7161290049552917,\n",
              "  0.7290322780609131,\n",
              "  0.7290322780609131,\n",
              "  0.6322580575942993,\n",
              "  0.6709677577018738,\n",
              "  0.7032257914543152,\n",
              "  0.7290322780609131,\n",
              "  0.7290322780609131,\n",
              "  0.7677419185638428,\n",
              "  0.7612903118133545,\n",
              "  0.6580645442008972,\n",
              "  0.7161290049552917,\n",
              "  0.7354838848114014,\n",
              "  0.7483870983123779,\n",
              "  0.7483870983123779,\n",
              "  0.7419354915618896,\n",
              "  0.7806451320648193,\n",
              "  0.7806451320648193,\n",
              "  0.7677419185638428,\n",
              "  0.7806451320648193,\n",
              "  0.7354838848114014,\n",
              "  0.7677419185638428,\n",
              "  0.7612903118133545,\n",
              "  0.7548387050628662,\n",
              "  0.7483870983123779,\n",
              "  0.7419354915618896,\n",
              "  0.7806451320648193,\n",
              "  0.7806451320648193,\n",
              "  0.7032257914543152,\n",
              "  0.7032257914543152,\n",
              "  0.7612903118133545,\n",
              "  0.8064516186714172,\n",
              "  0.774193525314331,\n",
              "  0.7548387050628662,\n",
              "  0.774193525314331,\n",
              "  0.7225806713104248,\n",
              "  0.7677419185638428,\n",
              "  0.7870967984199524,\n",
              "  0.7612903118133545,\n",
              "  0.8193548321723938,\n",
              "  0.7935484051704407,\n",
              "  0.800000011920929,\n",
              "  0.7935484051704407,\n",
              "  0.7548387050628662,\n",
              "  0.7935484051704407,\n",
              "  0.8064516186714172,\n",
              "  0.7548387050628662,\n",
              "  0.8064516186714172,\n",
              "  0.8064516186714172,\n",
              "  0.7935484051704407,\n",
              "  0.7870967984199524,\n",
              "  0.7935484051704407,\n",
              "  0.8064516186714172,\n",
              "  0.7806451320648193,\n",
              "  0.8129032254219055,\n",
              "  0.8258064389228821,\n",
              "  0.8064516186714172,\n",
              "  0.8129032254219055,\n",
              "  0.8322580456733704,\n",
              "  0.774193525314331,\n",
              "  0.8193548321723938,\n",
              "  0.8387096524238586,\n",
              "  0.8387096524238586,\n",
              "  0.8258064389228821,\n",
              "  0.8322580456733704,\n",
              "  0.8645161390304565,\n",
              "  0.8064516186714172,\n",
              "  0.8258064389228821,\n",
              "  0.8451613187789917,\n",
              "  0.8580645322799683,\n",
              "  0.800000011920929,\n",
              "  0.8193548321723938,\n",
              "  0.7419354915618896,\n",
              "  0.8387096524238586,\n",
              "  0.8580645322799683,\n",
              "  0.85161292552948,\n",
              "  0.85161292552948,\n",
              "  0.8451613187789917,\n",
              "  0.8580645322799683,\n",
              "  0.8580645322799683,\n",
              "  0.8451613187789917,\n",
              "  0.8774193525314331,\n",
              "  0.8387096524238586,\n",
              "  0.8580645322799683,\n",
              "  0.8580645322799683,\n",
              "  0.8193548321723938,\n",
              "  0.8322580456733704,\n",
              "  0.8387096524238586,\n",
              "  0.8580645322799683,\n",
              "  0.8645161390304565,\n",
              "  0.8258064389228821,\n",
              "  0.8580645322799683,\n",
              "  0.8322580456733704,\n",
              "  0.8193548321723938,\n",
              "  0.8258064389228821,\n",
              "  0.7935484051704407,\n",
              "  0.8322580456733704,\n",
              "  0.8838709592819214,\n",
              "  0.8645161390304565,\n",
              "  0.8451613187789917,\n",
              "  0.8387096524238586,\n",
              "  0.8193548321723938,\n",
              "  0.85161292552948,\n",
              "  0.85161292552948,\n",
              "  0.8838709592819214,\n",
              "  0.8774193525314331,\n",
              "  0.8387096524238586,\n",
              "  0.8387096524238586,\n",
              "  0.8645161390304565,\n",
              "  0.8580645322799683,\n",
              "  0.8387096524238586,\n",
              "  0.85161292552948,\n",
              "  0.85161292552948,\n",
              "  0.8193548321723938,\n",
              "  0.8709677457809448,\n",
              "  0.8709677457809448,\n",
              "  0.8774193525314331,\n",
              "  0.8580645322799683,\n",
              "  0.8838709592819214,\n",
              "  0.8709677457809448,\n",
              "  0.8645161390304565,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.85161292552948,\n",
              "  0.8709677457809448,\n",
              "  0.8709677457809448,\n",
              "  0.8774193525314331,\n",
              "  0.8322580456733704,\n",
              "  0.8451613187789917,\n",
              "  0.85161292552948,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.896774172782898,\n",
              "  0.8903225660324097,\n",
              "  0.8774193525314331,\n",
              "  0.8451613187789917,\n",
              "  0.8903225660324097,\n",
              "  0.8774193525314331,\n",
              "  0.8774193525314331,\n",
              "  0.8838709592819214,\n",
              "  0.8580645322799683,\n",
              "  0.8774193525314331,\n",
              "  0.8774193525314331,\n",
              "  0.8774193525314331,\n",
              "  0.8645161390304565,\n",
              "  0.8774193525314331,\n",
              "  0.8709677457809448,\n",
              "  0.85161292552948,\n",
              "  0.8709677457809448,\n",
              "  0.85161292552948,\n",
              "  0.8451613187789917,\n",
              "  0.8129032254219055,\n",
              "  0.8774193525314331,\n",
              "  0.8903225660324097,\n",
              "  0.896774172782898,\n",
              "  0.8645161390304565,\n",
              "  0.85161292552948,\n",
              "  0.8387096524238586,\n",
              "  0.8774193525314331,\n",
              "  0.8645161390304565,\n",
              "  0.8838709592819214,\n",
              "  0.8580645322799683,\n",
              "  0.8774193525314331,\n",
              "  0.85161292552948,\n",
              "  0.8774193525314331,\n",
              "  0.8709677457809448,\n",
              "  0.8774193525314331,\n",
              "  0.896774172782898,\n",
              "  0.8903225660324097,\n",
              "  0.8645161390304565,\n",
              "  0.8774193525314331,\n",
              "  0.9032257795333862,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.8838709592819214,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.9032257795333862,\n",
              "  0.8709677457809448,\n",
              "  0.8774193525314331,\n",
              "  0.8709677457809448,\n",
              "  0.8774193525314331,\n",
              "  0.896774172782898,\n",
              "  0.85161292552948,\n",
              "  0.8709677457809448,\n",
              "  0.8645161390304565,\n",
              "  0.8774193525314331,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.8580645322799683,\n",
              "  0.8709677457809448,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.8903225660324097,\n",
              "  0.8645161390304565,\n",
              "  0.8709677457809448,\n",
              "  0.8580645322799683,\n",
              "  0.8903225660324097,\n",
              "  0.8838709592819214,\n",
              "  0.8645161390304565,\n",
              "  0.8903225660324097,\n",
              "  0.8580645322799683,\n",
              "  0.8387096524238586,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.8903225660324097,\n",
              "  0.8645161390304565,\n",
              "  0.8709677457809448,\n",
              "  0.9032257795333862,\n",
              "  0.9032257795333862,\n",
              "  0.9096774458885193,\n",
              "  0.896774172782898,\n",
              "  0.8774193525314331,\n",
              "  0.896774172782898,\n",
              "  0.896774172782898,\n",
              "  0.9225806593894958,\n",
              "  0.8580645322799683,\n",
              "  0.8322580456733704,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.896774172782898,\n",
              "  0.8774193525314331,\n",
              "  0.8838709592819214,\n",
              "  0.8645161390304565,\n",
              "  0.8774193525314331,\n",
              "  0.9032257795333862,\n",
              "  0.8774193525314331,\n",
              "  0.896774172782898,\n",
              "  0.8709677457809448,\n",
              "  0.8903225660324097,\n",
              "  0.896774172782898,\n",
              "  0.85161292552948,\n",
              "  0.8709677457809448,\n",
              "  0.8580645322799683,\n",
              "  0.8903225660324097,\n",
              "  0.8903225660324097,\n",
              "  0.896774172782898,\n",
              "  0.8903225660324097,\n",
              "  0.8774193525314331,\n",
              "  0.896774172782898,\n",
              "  0.8838709592819214,\n",
              "  0.8451613187789917]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(XKey_train,yKey_train,batch_size=32,epochs = 100,validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyjs4DBjh9li",
        "outputId": "e2cbb525-5607-43ef-965b-f494dcf06fa6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/20 [>.............................] - ETA: 2s - loss: 0.3322 - accuracy: 0.8750"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 3s 150ms/step - loss: 0.3438 - accuracy: 0.9094 - val_loss: 0.7776 - val_accuracy: 0.8645\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.3549 - accuracy: 0.9175 - val_loss: 0.7804 - val_accuracy: 0.8645\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.2679 - accuracy: 0.9466 - val_loss: 0.6993 - val_accuracy: 0.8774\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2401 - accuracy: 0.9401 - val_loss: 0.6524 - val_accuracy: 0.8903\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.2298 - accuracy: 0.9434 - val_loss: 0.6560 - val_accuracy: 0.9097\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.2305 - accuracy: 0.9450 - val_loss: 0.6870 - val_accuracy: 0.8774\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.2248 - accuracy: 0.9337 - val_loss: 0.6724 - val_accuracy: 0.8968\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.2024 - accuracy: 0.9579 - val_loss: 0.6560 - val_accuracy: 0.9097\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.2167 - accuracy: 0.9450 - val_loss: 0.6534 - val_accuracy: 0.9032\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1971 - accuracy: 0.9531 - val_loss: 0.6567 - val_accuracy: 0.9032\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1865 - accuracy: 0.9595 - val_loss: 0.6609 - val_accuracy: 0.8968\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 3s 163ms/step - loss: 0.2238 - accuracy: 0.9450 - val_loss: 0.7130 - val_accuracy: 0.8774\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.2192 - accuracy: 0.9482 - val_loss: 0.6542 - val_accuracy: 0.9032\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.2584 - accuracy: 0.9337 - val_loss: 0.8274 - val_accuracy: 0.8516\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.3544 - accuracy: 0.9175 - val_loss: 0.7137 - val_accuracy: 0.8903\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.2746 - accuracy: 0.9256 - val_loss: 0.7271 - val_accuracy: 0.8839\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.3239 - accuracy: 0.9239 - val_loss: 0.7099 - val_accuracy: 0.8839\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.2821 - accuracy: 0.9353 - val_loss: 0.6750 - val_accuracy: 0.9032\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.2567 - accuracy: 0.9401 - val_loss: 0.6754 - val_accuracy: 0.8968\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.2361 - accuracy: 0.9417 - val_loss: 0.7028 - val_accuracy: 0.8968\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 3s 165ms/step - loss: 0.1897 - accuracy: 0.9660 - val_loss: 0.6736 - val_accuracy: 0.8903\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2158 - accuracy: 0.9450 - val_loss: 0.7027 - val_accuracy: 0.8774\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.2106 - accuracy: 0.9531 - val_loss: 0.7127 - val_accuracy: 0.9032\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.2252 - accuracy: 0.9466 - val_loss: 0.7275 - val_accuracy: 0.8710\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 3s 141ms/step - loss: 0.1979 - accuracy: 0.9612 - val_loss: 0.7196 - val_accuracy: 0.8968\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 3s 159ms/step - loss: 0.2166 - accuracy: 0.9498 - val_loss: 0.6785 - val_accuracy: 0.8968\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.2326 - accuracy: 0.9482 - val_loss: 0.6893 - val_accuracy: 0.8968\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.2267 - accuracy: 0.9482 - val_loss: 0.7554 - val_accuracy: 0.8774\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.2384 - accuracy: 0.9369 - val_loss: 0.6811 - val_accuracy: 0.9032\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.2847 - accuracy: 0.9353 - val_loss: 0.7566 - val_accuracy: 0.8581\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 3s 141ms/step - loss: 0.2941 - accuracy: 0.9207 - val_loss: 0.8177 - val_accuracy: 0.8516\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.3270 - accuracy: 0.9094 - val_loss: 0.7704 - val_accuracy: 0.8516\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.3044 - accuracy: 0.9417 - val_loss: 0.6686 - val_accuracy: 0.9097\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.2493 - accuracy: 0.9482 - val_loss: 0.6709 - val_accuracy: 0.9097\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.2073 - accuracy: 0.9579 - val_loss: 0.7258 - val_accuracy: 0.8839\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.2318 - accuracy: 0.9434 - val_loss: 0.6770 - val_accuracy: 0.8968\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2704 - accuracy: 0.9434 - val_loss: 0.7220 - val_accuracy: 0.8839\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.2537 - accuracy: 0.9353 - val_loss: 0.7186 - val_accuracy: 0.8710\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.2588 - accuracy: 0.9369 - val_loss: 0.6833 - val_accuracy: 0.9097\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.2563 - accuracy: 0.9466 - val_loss: 0.6600 - val_accuracy: 0.8903\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.2061 - accuracy: 0.9498 - val_loss: 0.6626 - val_accuracy: 0.9097\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.1902 - accuracy: 0.9595 - val_loss: 0.7002 - val_accuracy: 0.8774\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2254 - accuracy: 0.9515 - val_loss: 0.6447 - val_accuracy: 0.9097\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.1776 - accuracy: 0.9612 - val_loss: 0.6920 - val_accuracy: 0.8710\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 3s 144ms/step - loss: 0.2373 - accuracy: 0.9385 - val_loss: 0.7568 - val_accuracy: 0.8839\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2813 - accuracy: 0.9417 - val_loss: 0.7227 - val_accuracy: 0.8968\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.2796 - accuracy: 0.9320 - val_loss: 0.7050 - val_accuracy: 0.8839\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.2974 - accuracy: 0.9369 - val_loss: 0.7185 - val_accuracy: 0.8903\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 3s 177ms/step - loss: 0.2472 - accuracy: 0.9515 - val_loss: 0.6490 - val_accuracy: 0.8839\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.2382 - accuracy: 0.9434 - val_loss: 0.7344 - val_accuracy: 0.8710\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.2484 - accuracy: 0.9466 - val_loss: 0.7600 - val_accuracy: 0.8645\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2525 - accuracy: 0.9482 - val_loss: 0.6692 - val_accuracy: 0.9161\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.2220 - accuracy: 0.9450 - val_loss: 0.7096 - val_accuracy: 0.8839\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.2401 - accuracy: 0.9498 - val_loss: 0.6624 - val_accuracy: 0.8903\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.1919 - accuracy: 0.9612 - val_loss: 0.7888 - val_accuracy: 0.8710\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.2594 - accuracy: 0.9304 - val_loss: 0.6742 - val_accuracy: 0.8839\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2036 - accuracy: 0.9612 - val_loss: 0.6930 - val_accuracy: 0.8774\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 3s 161ms/step - loss: 0.2114 - accuracy: 0.9547 - val_loss: 0.6564 - val_accuracy: 0.8903\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.1960 - accuracy: 0.9531 - val_loss: 0.6839 - val_accuracy: 0.8968\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.1658 - accuracy: 0.9693 - val_loss: 0.6576 - val_accuracy: 0.8903\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1781 - accuracy: 0.9595 - val_loss: 0.6391 - val_accuracy: 0.8968\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1695 - accuracy: 0.9693 - val_loss: 0.6257 - val_accuracy: 0.9097\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 4s 180ms/step - loss: 0.1526 - accuracy: 0.9741 - val_loss: 0.6867 - val_accuracy: 0.8839\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1791 - accuracy: 0.9644 - val_loss: 0.6581 - val_accuracy: 0.9032\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.1994 - accuracy: 0.9531 - val_loss: 0.6673 - val_accuracy: 0.9032\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.2172 - accuracy: 0.9547 - val_loss: 0.6499 - val_accuracy: 0.9032\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.1686 - accuracy: 0.9725 - val_loss: 0.6787 - val_accuracy: 0.8968\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.1879 - accuracy: 0.9612 - val_loss: 0.6820 - val_accuracy: 0.8839\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 0.2022 - accuracy: 0.9498 - val_loss: 0.6807 - val_accuracy: 0.9161\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.1773 - accuracy: 0.9595 - val_loss: 0.6478 - val_accuracy: 0.9161\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.1624 - accuracy: 0.9693 - val_loss: 0.6752 - val_accuracy: 0.9161\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 4s 186ms/step - loss: 0.1836 - accuracy: 0.9547 - val_loss: 0.7094 - val_accuracy: 0.8968\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.2023 - accuracy: 0.9498 - val_loss: 0.6923 - val_accuracy: 0.9097\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.1563 - accuracy: 0.9773 - val_loss: 0.6679 - val_accuracy: 0.9097\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.1761 - accuracy: 0.9547 - val_loss: 0.7579 - val_accuracy: 0.8903\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 3s 164ms/step - loss: 0.1692 - accuracy: 0.9693 - val_loss: 0.7525 - val_accuracy: 0.8968\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.1785 - accuracy: 0.9660 - val_loss: 0.6816 - val_accuracy: 0.9032\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1696 - accuracy: 0.9628 - val_loss: 0.6926 - val_accuracy: 0.8968\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1764 - accuracy: 0.9644 - val_loss: 0.7833 - val_accuracy: 0.8581\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1938 - accuracy: 0.9579 - val_loss: 0.6842 - val_accuracy: 0.9226\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 4s 186ms/step - loss: 0.1688 - accuracy: 0.9660 - val_loss: 0.7130 - val_accuracy: 0.9032\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.1711 - accuracy: 0.9579 - val_loss: 0.7022 - val_accuracy: 0.9032\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1518 - accuracy: 0.9644 - val_loss: 0.7711 - val_accuracy: 0.8968\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.2103 - accuracy: 0.9547 - val_loss: 0.7817 - val_accuracy: 0.8968\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 3s 164ms/step - loss: 0.2523 - accuracy: 0.9450 - val_loss: 0.7685 - val_accuracy: 0.8774\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.2000 - accuracy: 0.9644 - val_loss: 0.7665 - val_accuracy: 0.8903\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.1928 - accuracy: 0.9579 - val_loss: 0.7143 - val_accuracy: 0.8903\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.2377 - accuracy: 0.9466 - val_loss: 0.7867 - val_accuracy: 0.8968\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.1865 - accuracy: 0.9579 - val_loss: 0.7633 - val_accuracy: 0.8903\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 4s 186ms/step - loss: 0.1997 - accuracy: 0.9547 - val_loss: 0.7742 - val_accuracy: 0.8774\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.2201 - accuracy: 0.9515 - val_loss: 0.7489 - val_accuracy: 0.8839\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.2278 - accuracy: 0.9466 - val_loss: 0.8000 - val_accuracy: 0.8710\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.2360 - accuracy: 0.9450 - val_loss: 0.7317 - val_accuracy: 0.8903\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 3s 163ms/step - loss: 0.2437 - accuracy: 0.9434 - val_loss: 0.8648 - val_accuracy: 0.8645\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2368 - accuracy: 0.9515 - val_loss: 0.7766 - val_accuracy: 0.8903\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.2681 - accuracy: 0.9401 - val_loss: 0.6973 - val_accuracy: 0.8903\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1793 - accuracy: 0.9741 - val_loss: 0.7114 - val_accuracy: 0.8968\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.1872 - accuracy: 0.9579 - val_loss: 0.7076 - val_accuracy: 0.8903\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.1702 - accuracy: 0.9693 - val_loss: 0.6731 - val_accuracy: 0.8968\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.1755 - accuracy: 0.9644 - val_loss: 0.6889 - val_accuracy: 0.9032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAFodDTPiBbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}