{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOBDWBHIT8usPTGRqSY815e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install mediapipe\n","import cv2\n","import mediapipe as mp\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","\n","mp_pose = mp.solutions.pose\n","\n","# Setup the Pose function for images - independently for the images standalone processing.\n","pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.8,model_complexity=2)\n","\n","# Setup the Pose function for videos - for video processing.\n","pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.7,\n","                          min_tracking_confidence=0.7)\n","\n","# Initialize mediapipe drawing class - to draw the landmarks points.\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","\n","def mpPoseImg(image_pose, pose=pose_image, draw=False, display=False):\n","    \n","    annotated_image = image_pose.copy()\n","    \n","    image_in_RGB = cv2.cvtColor(image_pose, cv2.COLOR_BGR2RGB)\n"," \n","    \n","    results = pose.process(image_in_RGB)\n","\n","    if results.pose_landmarks and draw:    \n","      mp_drawing.draw_landmarks(image=annotated_image,landmark_list=results.pose_landmarks,connections=mp_pose.POSE_CONNECTIONS,\n","                                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n","\n","\n","        # mp_drawing.draw_landmarks(image=original_image, landmark_list=resultant.pose_landmarks,connections=mp_pose.POSE_CONNECTIONS,\n","        #                           landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),thickness=3, circle_radius=3),\n","        #                           connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),thickness=2, circle_radius=2))\n","\n","    if display:\n","            \n","            # plt.figure(figsize=[22,22])\n","            # plt.subplot(121);plt.imshow(image_pose[:,:,::-1]);plt.title(\"Input Image\");plt.axis('off');\n","            # plt.subplot(122);plt.imshow(annotated_image[:,:,::-1]);plt.title(\"Pose detected Image\");plt.axis('off');\n","            cv2.imshow(annotated_image)\n","            return results\n","\n","    else:\n","        return annotated_image,results\n","     \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ahy_CKjVkXO5","executionInfo":{"status":"ok","timestamp":1681560330227,"user_tz":-330,"elapsed":20576,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}},"outputId":"dbedbc92-6350-4d49-a90d-f18cf1251a34"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.9.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.9.2.1\n","Downloading model to /usr/local/lib/python3.9/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"]}]},{"cell_type":"code","source":["train_keypoints=[]\n","\n","numOfKeypoints = 34\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","# calculate the center of the object in each frame\n","\n","def CentralisedKeypoints1(keypoints):\n","  frame_centers = []\n","  for frame_keypoints in keypoints:\n","    # assume the center is the mean of all keypoints\n","    center_x = np.nanmean([kp[0] for kp in frame_keypoints if kp[0] is not None])\n","    center_y = np.nanmean([kp[1] for kp in frame_keypoints if kp[1] is not None])\n","    frame_centers.append((center_x, center_y))\n","\n","  relative_keypoints = keypoints\n","\n","  for j in range(len(relative_keypoints)):               #frame no\n","    for k in range(len(relative_keypoints[j])):          # each keypoint of frame\n","      if relative_keypoints[j][k][0] is not None:\n","          relative_keypoints[j][k][0] -= frame_centers[j][0]\n","      if relative_keypoints[j][k][1] is not None:\n","        relative_keypoints[j][k][1] -= frame_centers[j][1]\n","\n","  \n","  for i in range(len(relative_keypoints)):   #each frame\n","    for j in range(len(relative_keypoints[i])):   #34 keypoints\n","      if relative_keypoints[i][j][0] is None:\n","        relative_keypoints[i][j][0] = 0.0\n","      if relative_keypoints[i][j][1] is None:\n","        relative_keypoints[i][j][1] = 0.0\n","\n","  return relative_keypoints\n","\n"],"metadata":{"id":"9UIv3_tewvLG","executionInfo":{"status":"ok","timestamp":1681560330228,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["numOfKeypoints = 34\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","# calculate the center of the object in each frame\n","\n","def CentralisedKeypoints2(keypoints):\n","  frame_centers = []\n","  for frame_keypoints in keypoints:\n","    # assume the center is the mean of all keypoints\n","    center_x = np.nanmean([kp[0] for kp in frame_keypoints if kp[0] is not None])\n","    center_y = np.nanmean([kp[1] for kp in frame_keypoints if kp[1] is not None])\n","    frame_centers.append((center_x, center_y))\n","\n","  relative_keypoints = keypoints\n","\n","  for j in range(len(relative_keypoints)):               #frame no\n","    for k in range(len(relative_keypoints[j])):          # each keypoint of frame\n","      if relative_keypoints[j][k][0] is not None:\n","          relative_keypoints[j][k][0] -= frame_centers[j][0]\n","      if relative_keypoints[j][k][1] is not None:\n","        relative_keypoints[j][k][1] -= frame_centers[j][1]\n","\n","  \n","  frames_keypoints=[]\n","  \n","  for i in range(len(relative_keypoints)):   #each frame\n","    flag=1\n","    for j in range(len(relative_keypoints[i])):   #34 keypoints\n","      if relative_keypoints[i][j][0] is None or relative_keypoints[i][j][1] is None:\n","        flag = 0\n","        break\n","    \n","    if(flag == 1) :\n","      frames_keypoints.append(relative_keypoints[i])\n","  \n","  kk = [[0.0,0.0] for _ in range(numOfKeypoints)]\n","  if(len(frames_keypoints)>0):\n","    for i in range (16-len(frames_keypoints)):\n","        frames_keypoints.append(kk)\n","\n","  return frames_keypoints"],"metadata":{"id":"9xjtGTMxp0Py","executionInfo":{"status":"ok","timestamp":1681560330229,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["numOfKeypoints = 34\n","def getKeyPointsforFrame(frame):\n","  img,results = mpPoseImg(frame)\n","  keypoints = [[None,None] for _ in range(numOfKeypoints)]\n","  if results.pose_landmarks == None:\n","    return keypoints\n","    \n","  landmarks = results.pose_landmarks\n","  for i in range(numOfKeypoints-2):\n","     mark = landmarks.landmark[i]\n","     keypoints[i][0]=mark.x\n","     keypoints[i][1]=mark.y\n","  #------------------------------------------------------------------------------------------------\n","  try:\n","    # MID HIP COORDINATES\n","      left_hip_index = 23\n","      right_hip_index = 24\n","      Mid_hip_x =  (keypoints[left_hip_index][0]+keypoints[right_hip_index][0])/2\n","      Mid_hip_y =(keypoints[left_hip_index][1]+keypoints[right_hip_index][1])/2\n","      # keypoints.append([Mid_hip_x,Mid_hip_y])\n","      keypoints[32][0]=Mid_hip_x\n","      keypoints[32][1]=Mid_hip_y\n","                          \n","  except:\n","      pass\n","      \n","  #------------------------------------------------------------------------------------------------\n","  try:\n","    # NECK COORDINATES\n","      left_shoulder_index = 11\n","      right_shoulder_index = 12\n","      Neck_x =  (keypoints[left_shoulder_index][0]+keypoints[right_shoulder_index][0])/2\n","      Neck_y =(keypoints[left_shoulder_index][1]+keypoints[right_shoulder_index][1])/2\n","      # keypoints.append([Neck_x,Neck_y])\n","      keypoints[33][0]=Neck_x\n","      keypoints[33][1]=Neck_y\n","                          \n","  except:\n","      pass\n","\n","  #------------------------------------------------------------------------------------------------\n","     \n","  return keypoints"],"metadata":{"id":"BE_aOtojxwjs","executionInfo":{"status":"ok","timestamp":1681560330230,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras import initializers\n","from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n","from keras.models import load_model\n","\n","# This will prompt for authorization.\n","drive.mount('/MyDrive')\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import os,cv2\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","data_path = '../MyDrive/MyDrive/Dataset'\n","\n","\n","labels=[]\n","for folder in os.listdir(data_path):\n","    labels.append(folder)\n","labels.sort() #len = 107\n","\n","train_videos=[]\n","train_labels=[]\n","train_keypoints = []\n","import traceback\n","import random\n","\n","num = 1\n","\n","for i,folder in enumerate(labels):\n","  try:\n","    for video in os.listdir(data_path+'/'+folder):\n","      print(num)\n","      num+=1\n","      video = os.path.join(data_path+'/'+folder+'/'+video)\n","      # Open the video file\n","      cap = cv2.VideoCapture(video)\n","\n","      # Get the total number of frames in the video\n","      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","      # Select a random starting frame number\n","      start_frame = random.randint(0, total_frames - 16)\n","\n","      # Set the number of frames to be selected\n","      num_frames = 16\n","\n","      # Set the frame number to start with\n","      cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","      # Loop through the frames and save them\n","      frames = []\n","      keypoints = []\n","      for j in range(num_frames):\n","          ret, image = cap.read()\n","          if ret:\n","            frames.append(cv2.resize(image,(112,112)))\n","            kk = getKeyPointsforFrame(image)\n","            keypoints.append(kk)\n","\n","      # Release the video file\n","      cap.release()\n","\n","\n","      train_videos.append(frames)\n","\n","      train_keypoints.append(CentralisedKeypoints1(keypoints))\n","      train_labels.append(i)\n","      # if(num>2):\n","      #    break\n","\n","  except Exception:\n","    traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"g8A6vXa45cwg","executionInfo":{"status":"error","timestamp":1681560641481,"user_tz":-330,"elapsed":224755,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}},"outputId":"e2509b6d-ebb4-4d95-a838-8b95066c25ea"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /MyDrive\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b1b98680299e>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;31m# Open the video file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;31m# Get the total number of frames in the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import cv2\n","cap = cv2.VideoCapture(0)\n","\n","\n","RGBmodel = load_model('RGBStream.h5')\n","Posemodel = load_model('PoseStream.h5')\n","\n","\n","video = []\n","\n","while True:\n","  ret, frame = cap.read()\n","\n","  if not ret:\n","    break\n","  \n","  video.append(frame)\n","\n","  if(len(video)==16):\n","    keypoints = []\n","    for image in video:\n","      keypoints.append(getKeyPointsforFrame(image))\n","\n","    keypoints = CentralisedKeypoints1(keypoints)\n","\n","    yogaPose = predictYogaPose(video,keypoints)\n","\n"],"metadata":{"id":"aAko03GO1i9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_videos = np.asarray(train_videos)\n","train_keypoints = np.asarray(train_keypoints)\n","train_labels = np.asarray(train_labels).astype('int32')\n","\n","# train_keypoints = tf.convert_to_tensor(train_keypoints, dtype=tf.float32)\n","# # yKey_train = tf.cast(yKey_train , dtype=tf.float32)\n","# train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n","\n","# train_keypoints = np.array([np.array(val) for val in train_keypoints])\n","\n","from sklearn.model_selection import train_test_split\n","\n","XKey_train,XKey_test,yKey_train,yKey_test = train_test_split(train_keypoints,train_labels,test_size=0.25,shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"n9NQTKVJ6pd3","executionInfo":{"status":"error","timestamp":1681560336339,"user_tz":-330,"elapsed":10,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}},"outputId":"54249dee-47da-43a6-d09e-175d559f0e2c"},"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ae7b9f9c5669>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_videos = np.asarray(train_videos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_keypoints = tf.convert_to_tensor(train_keypoints, dtype=tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"]}]},{"cell_type":"code","source":["from keras import models, layers\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, TimeDistributed, GlobalAveragePooling1D\n","from keras.optimizers import Adam\n","from keras.initializers import GlorotNormal\n","from keras.regularizers import L1L2\n","\n","yKey_train = np.squeeze(yKey_train)\n","\n","tf.config.run_functions_eagerly(True)\n","\n","# Define model input shape\n","input_shape = (16, 34, 2)\n","\n","# Build model architecture\n","model = models.Sequential()\n","\n","model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, activation='relu'),input_shape = input_shape))\n","# model.add(TimeDistributed(BatchNormalization()))\n","model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n","model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n","model.add(TimeDistributed(Conv1D(filters=50, kernel_size=3, activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n","\n","# flatten output from CNN layers\n","model.add(TimeDistributed(GlobalAveragePooling1D()))\n","\n","# add LSTM layer\n","# model.add(LSTM(100, dropout=0.0, recurrent_dropout=0.0, kernel_initializer=GlorotNormal(seed=42), recurrent_regularizer=L1L2(l1=0.0, l2=0.001), bias_regularizer=L1L2(l1=0.0, l2=0.001), recurrent_initializer=GlorotNormal(seed=42), return_sequences=False))\n","model.add(LSTM(100, recurrent_regularizer=L1L2(l1=0.01, l2=0.001), bias_regularizer=L1L2(l1=0.01, l2=0.001), recurrent_initializer=GlorotNormal(seed=42), return_sequences=False))\n","\n","# TODO: forget bias \n","# TODO: L1 = 0.01\n","\n","\n","# add fully connected layers\n","\n","# model.add(Dropout(0.5))\n","# model.add(Dense(100, activation='relu'))\n","\n","model.add(Dropout(0.5))\n","model.add(Dense(80, activation='relu'))\n","\n","# add Softmax layer\n","model.add(Dense(20, activation='softmax'))\n","\n","# compile model\n","adam = Adam(learning_rate=0.001) \n","model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","\n","# print model summary\n","# print(model.summary())\n","\n","history = model.fit(XKey_train,yKey_train,batch_size=32,epochs = 300,validation_split = 0.2)"],"metadata":{"id":"dW4Fb2a3zUB9","executionInfo":{"status":"aborted","timestamp":1681560336340,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aman Brar","userId":"16594611556853887311"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["softmaxRGB = RGBmodel.predict(X_trainRGB)\n","softmaxPose = Posemodel.predict(XKey_train)"],"metadata":{"id":"Yn8GnDFQ4sDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Combine the softmax outputs by taking the average of their values\n","combined_softmax = (softmaxRGB*0.7 + softmaxPose*0.3) \n","\n","# Generate some example ground truth labels\n","actualValues = Ytrain\n","\n","# Convert the combined softmax probabilities to predicted labels\n","predicted_labels = np.argmax(combined_softmax)\n","\n","# Compute the accuracy of the predictions\n","accuracy = np.mean(predicted_labels == actualValues)\n","\n","# Print the results\n","print(\"Combined softmax probabilities:\", combined_softmax)\n","print(\"Ground truth labels:\", actualValues)\n","print(\"Predicted labels:\", predicted_labels)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"id":"ShtiENuz4qhh"},"execution_count":null,"outputs":[]}]}