{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberof gpus:  1\n"
     ]
    }
   ],
   "source": [
    "print('numberof gpus: ',len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoCLpQgMlXBn",
    "outputId": "4706d1b3-a1d0-4f1d-ae21-191b4492f498"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m################### RUN THIS CELL\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m############ Manaaaaan\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Drive helper and mount\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "\n",
    "################### RUN THIS CELL\n",
    "\n",
    "############ Manaaaaan\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
    "from keras.models import load_model\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/MyDrive')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os,cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = '../MyDrive/MyDrive/Dataset'\n",
    "\n",
    "\n",
    "labels=[]\n",
    "for folder in os.listdir(data_path):\n",
    "    labels.append(folder)\n",
    "labels.sort() #len = 107\n",
    "\n",
    "\n",
    "\n",
    "train_videos=[]\n",
    "train_labels=[]\n",
    "import traceback\n",
    "import random\n",
    "\n",
    "num = 1\n",
    "\n",
    "for i,folder in enumerate(labels):\n",
    "    try:\n",
    "      for video in os.listdir(data_path+'/'+folder):\n",
    "        print(num)\n",
    "        num+=1\n",
    "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        # Get the total number of frames in the video\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Select a random starting frame number\n",
    "        start_frame = random.randint(0, total_frames - 16)\n",
    "\n",
    "        # Set the number of frames to be selected\n",
    "        num_frames = 16\n",
    "\n",
    "        # Set the frame number to start with\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Loop through the frames and save them\n",
    "        frames = []\n",
    "        for j in range(num_frames):\n",
    "            ret, image = cap.read()\n",
    "            if ret:\n",
    "              frames.append(cv2.resize(image,(112,112)))\n",
    "\n",
    "        # Release the video file\n",
    "        cap.release()\n",
    "\n",
    "        train_videos.append(frames)\n",
    "        train_labels.append(i)\n",
    "\n",
    "    except Exception:\n",
    "      traceback.print_exc()\n",
    "train_videos = np.asarray(train_videos)\n",
    "train_labels = np.asarray(train_labels).astype('int64')\n",
    "\n",
    "# import numpy as np\n",
    "# arr_reshaped = train_videos.reshape(train_videos.shape[0], -1) #1151\n",
    "# np.savetxt(\"dataset.txt\", arr_reshaped)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
    "\n",
    "input_shape =(32,16,112,112,3)\n",
    "output_shape = 20\n",
    "\n",
    "tf.keras.regularizers.L1L2(\n",
    "    l1=0.01, l2=0.1\n",
    ")\n",
    "\n",
    "RGBmodel = Sequential()\n",
    "\n",
    "# model.add(RandomFlip(\"horizontal\"))\n",
    "# model.add(RandomRotation(0.1))\n",
    "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
    "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
    "\n",
    "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
    "\n",
    "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "RGBmodel.add(Flatten())\n",
    "\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
    "\n",
    "\n",
    "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define your model here\n",
    "\n",
    "\n",
    "# set up the checkpointing callback\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", save_weights_only=True)\n",
    "\n",
    "# train the model for 100 epochs and save the weights\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "# load the saved weights and continue training for another 100 epochs\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "# repeat the previous step until you have trained for 500 epochs\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "opt = SGD(learning_rate=0.0001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "# Print the summary of the model\n",
    "RGBmodel.summary()\n",
    "RGBmodel.save('RGBmodel.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3phz1CUCZfY8"
   },
   "outputs": [],
   "source": [
    "RGBmodel = Sequential()\n",
    "\n",
    "# model.add(RandomFlip(\"horizontal\"))\n",
    "# model.add(RandomRotation(0.1))\n",
    "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
    "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
    "\n",
    "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
    "\n",
    "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "RGBmodel.add(Flatten())\n",
    "\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
    "\n",
    "\n",
    "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.0001)s\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
    "RGBmodel.save('RGBmodel.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEAbUHYqZOTO"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define your model here\n",
    "\n",
    "\n",
    "# set up the checkpointing callback\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", save_weights_only=True)\n",
    "\n",
    "# train the model for 100 epochs and save the weights\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "# load the saved weights and continue training for another 100 epochs\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "# repeat the previous step until you have trained for 500 epochs\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n",
    "\n",
    "opt = SGD(learning_rate=0.0001)s\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.load_weights(\"model_weights.h5\")\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PCoZzxq8HB6"
   },
   "outputs": [],
   "source": [
    "RGBmodel.save('RGBmodel.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLWUM-aXeYNZ",
    "outputId": "28ccfd2d-c443-45d3-e0c1-ad0a7cb39778",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################### RUN THIS CELL\n",
    "\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "# from google.colab import drive\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras.layers import Conv3D, MaxPooling3D,Dense,ZeroPadding3D,Flatten,Activation,BatchNormalization,RandomFlip,RandomRotation,Rescaling\n",
    "from keras.models import load_model\n",
    "\n",
    "# This will prompt for authorization.\n",
    "# drive.mount('/MyDrive')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os,cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data_path = '../MyDrive/MyDrive/Dataset'\n",
    "\n",
    "data_path = './Dataset'\n",
    "labels=[]\n",
    "for folder in os.listdir(data_path):\n",
    "    labels.append(folder)\n",
    "labels.sort() #len = 107\n",
    "\n",
    "\n",
    "\n",
    "train_videos=[]\n",
    "train_labels=[]\n",
    "import traceback\n",
    "import random\n",
    "\n",
    "num = 1\n",
    "\n",
    "for i,folder in enumerate(labels):\n",
    "    try:\n",
    "      for video in os.listdir(data_path+'/'+folder):\n",
    "        print(num)\n",
    "        num+=1\n",
    "        video = os.path.join(data_path+'/'+folder+'/'+video)\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        # Get the total number of frames in the video\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Select a random starting frame number\n",
    "        start_frame = random.randint(0, total_frames - 16)\n",
    "\n",
    "        # Set the number of frames to be selected\n",
    "        num_frames = 16\n",
    "\n",
    "        # Set the frame number to start with\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Loop through the frames and save them\n",
    "        frames = []\n",
    "        for j in range(num_frames):\n",
    "            ret, image = cap.read()\n",
    "            if ret:\n",
    "              frames.append(cv2.resize(image,(112,112)))\n",
    "\n",
    "        # Release the video file\n",
    "        cap.release()\n",
    "\n",
    "        train_videos.append(frames)\n",
    "        train_labels.append(i)\n",
    "\n",
    "    except Exception:\n",
    "      traceback.print_exc()\n",
    "train_videos = np.asarray(train_videos)\n",
    "train_labels = np.asarray(train_labels).astype('int64')\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
    "\n",
    "# input_shape =(32,16,112,112,3)\n",
    "# output_shape = 20\n",
    "\n",
    "# tf.keras.regularizers.L1L2(\n",
    "#     l1=0.01, l2=0.1\n",
    "# )\n",
    "\n",
    "# RGBmodel = Sequential()\n",
    "\n",
    "# # model.add(RandomFlip(\"horizontal\"))\n",
    "# # model.add(RandomRotation(0.1))\n",
    "# # RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
    "# RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
    "# RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
    "\n",
    "# RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
    "# RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "# RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
    "# RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
    "# RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "# RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
    "# RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
    "# RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "# RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
    "# RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
    "\n",
    "# RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
    "# RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "# RGBmodel.add(Flatten())\n",
    "\n",
    "# RGBmodel.add(Dropout(rate=0.5))\n",
    "# RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
    "# RGBmodel.add(Dropout(rate=0.5))\n",
    "# RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
    "\n",
    "\n",
    "# RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.001)\n",
    "# RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "# RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
    "\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.0001)\n",
    "# RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "# RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# # Print the summary of the model\n",
    "# RGBmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIN9pqD-gx8U",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "23/23 [==============================] - 66s 1s/step - loss: 2309.3901 - accuracy: 0.0371 - val_loss: 2288.7871 - val_accuracy: 0.0275\n",
      "Epoch 2/400\n",
      "23/23 [==============================] - 13s 589ms/step - loss: 2270.7314 - accuracy: 0.0371 - val_loss: 2250.6379 - val_accuracy: 0.0275\n",
      "Epoch 3/400\n",
      "23/23 [==============================] - 13s 588ms/step - loss: 2232.7400 - accuracy: 0.0523 - val_loss: 2212.8042 - val_accuracy: 0.0275\n",
      "Epoch 4/400\n",
      "23/23 [==============================] - 14s 589ms/step - loss: 2195.0586 - accuracy: 0.0523 - val_loss: 2175.3330 - val_accuracy: 0.0275\n",
      "Epoch 5/400\n",
      "23/23 [==============================] - 14s 589ms/step - loss: 2157.7351 - accuracy: 0.0591 - val_loss: 2138.1660 - val_accuracy: 0.0275\n",
      "Epoch 6/400\n",
      "23/23 [==============================] - 14s 590ms/step - loss: 2120.7339 - accuracy: 0.0385 - val_loss: 2101.3384 - val_accuracy: 0.0275\n",
      "Epoch 7/400\n",
      "23/23 [==============================] - 14s 591ms/step - loss: 2084.0608 - accuracy: 0.0619 - val_loss: 2064.8433 - val_accuracy: 0.0275\n",
      "Epoch 8/400\n",
      "23/23 [==============================] - 14s 590ms/step - loss: 2047.7042 - accuracy: 0.0729 - val_loss: 2028.6749 - val_accuracy: 0.0275\n",
      "Epoch 9/400\n",
      "23/23 [==============================] - 14s 592ms/step - loss: 2011.6949 - accuracy: 0.0660 - val_loss: 1992.8298 - val_accuracy: 0.0275\n",
      "Epoch 10/400\n",
      "23/23 [==============================] - 14s 593ms/step - loss: 1976.0132 - accuracy: 0.0578 - val_loss: 1957.3151 - val_accuracy: 0.0275\n",
      "Epoch 11/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 1940.6296 - accuracy: 0.0743 - val_loss: 1922.1331 - val_accuracy: 0.0275\n",
      "Epoch 12/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1905.6097 - accuracy: 0.0812 - val_loss: 1887.2767 - val_accuracy: 0.0275\n",
      "Epoch 13/400\n",
      "23/23 [==============================] - 14s 592ms/step - loss: 1870.8943 - accuracy: 0.0619 - val_loss: 1852.7476 - val_accuracy: 0.0275\n",
      "Epoch 14/400\n",
      "23/23 [==============================] - 14s 592ms/step - loss: 1836.5344 - accuracy: 0.0523 - val_loss: 1818.5470 - val_accuracy: 0.0275\n",
      "Epoch 15/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1802.4850 - accuracy: 0.0715 - val_loss: 1784.6747 - val_accuracy: 0.0275\n",
      "Epoch 16/400\n",
      "23/23 [==============================] - 14s 593ms/step - loss: 1768.7681 - accuracy: 0.0798 - val_loss: 1751.1246 - val_accuracy: 0.0275\n",
      "Epoch 17/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1735.3629 - accuracy: 0.0702 - val_loss: 1717.9058 - val_accuracy: 0.0275\n",
      "Epoch 18/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1702.3140 - accuracy: 0.0660 - val_loss: 1685.0140 - val_accuracy: 0.0275\n",
      "Epoch 19/400\n",
      "23/23 [==============================] - 14s 593ms/step - loss: 1669.5720 - accuracy: 0.0578 - val_loss: 1652.4602 - val_accuracy: 0.0275\n",
      "Epoch 20/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1637.1708 - accuracy: 0.0674 - val_loss: 1620.2239 - val_accuracy: 0.0275\n",
      "Epoch 21/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1605.0867 - accuracy: 0.0729 - val_loss: 1588.3197 - val_accuracy: 0.0275\n",
      "Epoch 22/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1573.3489 - accuracy: 0.0605 - val_loss: 1556.7395 - val_accuracy: 0.0275\n",
      "Epoch 23/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1541.9180 - accuracy: 0.0702 - val_loss: 1525.4921 - val_accuracy: 0.0275\n",
      "Epoch 24/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1510.8234 - accuracy: 0.0674 - val_loss: 1494.5714 - val_accuracy: 0.0275\n",
      "Epoch 25/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1480.0635 - accuracy: 0.0729 - val_loss: 1463.9792 - val_accuracy: 0.0275\n",
      "Epoch 26/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1449.6199 - accuracy: 0.0757 - val_loss: 1433.7142 - val_accuracy: 0.0275\n",
      "Epoch 27/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1419.5176 - accuracy: 0.0784 - val_loss: 1403.7788 - val_accuracy: 0.0275\n",
      "Epoch 28/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1389.7301 - accuracy: 0.0674 - val_loss: 1374.1705 - val_accuracy: 0.0275\n",
      "Epoch 29/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 1360.2749 - accuracy: 0.0825 - val_loss: 1344.8906 - val_accuracy: 0.0275\n",
      "Epoch 30/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1331.1504 - accuracy: 0.0729 - val_loss: 1315.9425 - val_accuracy: 0.0275\n",
      "Epoch 31/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 1302.3534 - accuracy: 0.0702 - val_loss: 1287.3199 - val_accuracy: 0.0275\n",
      "Epoch 32/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 1273.8907 - accuracy: 0.0757 - val_loss: 1259.0239 - val_accuracy: 0.0275\n",
      "Epoch 33/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1245.7501 - accuracy: 0.0674 - val_loss: 1231.0592 - val_accuracy: 0.0275\n",
      "Epoch 34/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1217.9403 - accuracy: 0.0702 - val_loss: 1203.4209 - val_accuracy: 0.0275\n",
      "Epoch 35/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1190.4503 - accuracy: 0.0908 - val_loss: 1176.1105 - val_accuracy: 0.0330\n",
      "Epoch 36/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1163.3068 - accuracy: 0.0770 - val_loss: 1149.1302 - val_accuracy: 0.0275\n",
      "Epoch 37/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1136.4735 - accuracy: 0.0729 - val_loss: 1122.4786 - val_accuracy: 0.0275\n",
      "Epoch 38/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 1109.9818 - accuracy: 0.0743 - val_loss: 1096.1532 - val_accuracy: 0.0275\n",
      "Epoch 39/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 1083.8132 - accuracy: 0.0880 - val_loss: 1070.1538 - val_accuracy: 0.0275\n",
      "Epoch 40/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 1057.9587 - accuracy: 0.0853 - val_loss: 1044.4861 - val_accuracy: 0.0330\n",
      "Epoch 41/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 1032.4552 - accuracy: 0.0894 - val_loss: 1019.1436 - val_accuracy: 0.0330\n",
      "Epoch 42/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 1007.2670 - accuracy: 0.0963 - val_loss: 994.1300 - val_accuracy: 0.0275\n",
      "Epoch 43/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 982.4045 - accuracy: 0.0812 - val_loss: 969.4468 - val_accuracy: 0.0275\n",
      "Epoch 44/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 957.8683 - accuracy: 0.0922 - val_loss: 945.0872 - val_accuracy: 0.0275\n",
      "Epoch 45/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 933.6751 - accuracy: 0.0853 - val_loss: 921.0596 - val_accuracy: 0.0330\n",
      "Epoch 46/400\n",
      "23/23 [==============================] - 14s 594ms/step - loss: 909.8053 - accuracy: 0.0660 - val_loss: 897.3554 - val_accuracy: 0.0385\n",
      "Epoch 47/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 886.2575 - accuracy: 0.0935 - val_loss: 873.9858 - val_accuracy: 0.0275\n",
      "Epoch 48/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 863.0414 - accuracy: 0.0770 - val_loss: 850.9415 - val_accuracy: 0.0330\n",
      "Epoch 49/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 840.1479 - accuracy: 0.0908 - val_loss: 828.2251 - val_accuracy: 0.0385\n",
      "Epoch 50/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 817.5789 - accuracy: 0.1210 - val_loss: 805.8392 - val_accuracy: 0.0330\n",
      "Epoch 51/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 795.3483 - accuracy: 0.1224 - val_loss: 783.7792 - val_accuracy: 0.0330\n",
      "Epoch 52/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 773.4514 - accuracy: 0.1018 - val_loss: 762.0491 - val_accuracy: 0.0330\n",
      "Epoch 53/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 751.8697 - accuracy: 0.1045 - val_loss: 740.6436 - val_accuracy: 0.0385\n",
      "Epoch 54/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 730.6172 - accuracy: 0.0977 - val_loss: 719.5633 - val_accuracy: 0.0385\n",
      "Epoch 55/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 709.7061 - accuracy: 0.1155 - val_loss: 698.8152 - val_accuracy: 0.0330\n",
      "Epoch 56/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 689.1139 - accuracy: 0.1004 - val_loss: 678.3940 - val_accuracy: 0.0330\n",
      "Epoch 57/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 668.8430 - accuracy: 0.1307 - val_loss: 658.3011 - val_accuracy: 0.0385\n",
      "Epoch 58/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 648.9118 - accuracy: 0.0977 - val_loss: 638.5219 - val_accuracy: 0.0495\n",
      "Epoch 59/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 629.3026 - accuracy: 0.1224 - val_loss: 619.0787 - val_accuracy: 0.0659\n",
      "Epoch 60/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 610.0037 - accuracy: 0.1348 - val_loss: 599.9730 - val_accuracy: 0.0659\n",
      "Epoch 61/400\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 591.0611 - accuracy: 0.1114 - val_loss: 581.1698 - val_accuracy: 0.1154\n",
      "Epoch 62/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 572.4162 - accuracy: 0.1197 - val_loss: 562.7233 - val_accuracy: 0.1154\n",
      "Epoch 63/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 554.1197 - accuracy: 0.1238 - val_loss: 544.5655 - val_accuracy: 0.1044\n",
      "Epoch 64/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 536.1414 - accuracy: 0.1431 - val_loss: 526.7524 - val_accuracy: 0.1154\n",
      "Epoch 65/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 518.4912 - accuracy: 0.1486 - val_loss: 509.2848 - val_accuracy: 0.1209\n",
      "Epoch 66/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 501.1528 - accuracy: 0.1376 - val_loss: 492.1219 - val_accuracy: 0.0824\n",
      "Epoch 67/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 484.1774 - accuracy: 0.1582 - val_loss: 475.3267 - val_accuracy: 0.1538\n",
      "Epoch 68/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 467.5310 - accuracy: 0.1348 - val_loss: 458.7611 - val_accuracy: 0.0989\n",
      "Epoch 69/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 451.1734 - accuracy: 0.1596 - val_loss: 442.6183 - val_accuracy: 0.1209\n",
      "Epoch 70/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 435.1225 - accuracy: 0.2022 - val_loss: 426.8118 - val_accuracy: 0.1758\n",
      "Epoch 71/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 419.4786 - accuracy: 0.1747 - val_loss: 411.2616 - val_accuracy: 0.1429\n",
      "Epoch 72/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 404.1253 - accuracy: 0.1733 - val_loss: 396.0156 - val_accuracy: 0.1538\n",
      "Epoch 73/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 389.0748 - accuracy: 0.2050 - val_loss: 381.2590 - val_accuracy: 0.2363\n",
      "Epoch 74/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 374.4179 - accuracy: 0.2063 - val_loss: 366.7367 - val_accuracy: 0.1923\n",
      "Epoch 75/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 360.0761 - accuracy: 0.2077 - val_loss: 352.5210 - val_accuracy: 0.2088\n",
      "Epoch 76/400\n",
      "23/23 [==============================] - 14s 596ms/step - loss: 345.9802 - accuracy: 0.2132 - val_loss: 338.7294 - val_accuracy: 0.2198\n",
      "Epoch 77/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 332.3275 - accuracy: 0.2215 - val_loss: 325.2197 - val_accuracy: 0.1923\n",
      "Epoch 78/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 318.9685 - accuracy: 0.1884 - val_loss: 311.9042 - val_accuracy: 0.1868\n",
      "Epoch 79/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 305.8425 - accuracy: 0.2297 - val_loss: 299.0889 - val_accuracy: 0.2363\n",
      "Epoch 80/400\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 293.1519 - accuracy: 0.2476 - val_loss: 286.5222 - val_accuracy: 0.2582\n",
      "Epoch 81/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 280.7363 - accuracy: 0.2462 - val_loss: 274.3250 - val_accuracy: 0.2088\n",
      "Epoch 82/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 268.7452 - accuracy: 0.2146 - val_loss: 262.5037 - val_accuracy: 0.1978\n",
      "Epoch 83/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 256.9812 - accuracy: 0.2421 - val_loss: 251.0849 - val_accuracy: 0.2198\n",
      "Epoch 84/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 245.5829 - accuracy: 0.2462 - val_loss: 239.7016 - val_accuracy: 0.2253\n",
      "Epoch 85/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 234.5021 - accuracy: 0.2847 - val_loss: 228.7714 - val_accuracy: 0.2143\n",
      "Epoch 86/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 223.7654 - accuracy: 0.2751 - val_loss: 218.4108 - val_accuracy: 0.1923\n",
      "Epoch 87/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 213.3563 - accuracy: 0.2600 - val_loss: 208.0097 - val_accuracy: 0.2253\n",
      "Epoch 88/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 203.2767 - accuracy: 0.2710 - val_loss: 198.1237 - val_accuracy: 0.2637\n",
      "Epoch 89/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 193.5272 - accuracy: 0.2503 - val_loss: 188.5086 - val_accuracy: 0.3077\n",
      "Epoch 90/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 184.1360 - accuracy: 0.2655 - val_loss: 179.3551 - val_accuracy: 0.1923\n",
      "Epoch 91/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 175.0131 - accuracy: 0.2724 - val_loss: 170.3998 - val_accuracy: 0.2088\n",
      "Epoch 92/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 166.2601 - accuracy: 0.2531 - val_loss: 161.7484 - val_accuracy: 0.2582\n",
      "Epoch 93/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 157.7815 - accuracy: 0.2765 - val_loss: 153.5611 - val_accuracy: 0.2473\n",
      "Epoch 94/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 149.7070 - accuracy: 0.2806 - val_loss: 145.6242 - val_accuracy: 0.2033\n",
      "Epoch 95/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 141.9568 - accuracy: 0.2820 - val_loss: 138.1431 - val_accuracy: 0.2253\n",
      "Epoch 96/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 134.5032 - accuracy: 0.2792 - val_loss: 131.0220 - val_accuracy: 0.2473\n",
      "Epoch 97/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 127.3822 - accuracy: 0.2861 - val_loss: 124.0374 - val_accuracy: 0.2473\n",
      "Epoch 98/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 120.6010 - accuracy: 0.2971 - val_loss: 117.1461 - val_accuracy: 0.2967\n",
      "Epoch 99/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 114.1572 - accuracy: 0.2902 - val_loss: 110.9633 - val_accuracy: 0.2582\n",
      "Epoch 100/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 108.0536 - accuracy: 0.2834 - val_loss: 105.0091 - val_accuracy: 0.2308\n",
      "Epoch 101/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 102.2678 - accuracy: 0.2600 - val_loss: 99.4613 - val_accuracy: 0.2637\n",
      "Epoch 102/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 96.7993 - accuracy: 0.3122 - val_loss: 94.2261 - val_accuracy: 0.2802\n",
      "Epoch 103/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 91.6680 - accuracy: 0.2916 - val_loss: 89.1058 - val_accuracy: 0.2857\n",
      "Epoch 104/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 86.9299 - accuracy: 0.2930 - val_loss: 84.5721 - val_accuracy: 0.1923\n",
      "Epoch 105/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 82.4101 - accuracy: 0.2902 - val_loss: 80.3784 - val_accuracy: 0.3022\n",
      "Epoch 106/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 78.3488 - accuracy: 0.2682 - val_loss: 76.5911 - val_accuracy: 0.1978\n",
      "Epoch 107/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 74.4982 - accuracy: 0.2751 - val_loss: 72.6046 - val_accuracy: 0.2527\n",
      "Epoch 108/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 71.0252 - accuracy: 0.2834 - val_loss: 69.8301 - val_accuracy: 0.2143\n",
      "Epoch 109/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 67.9289 - accuracy: 0.2641 - val_loss: 66.7570 - val_accuracy: 0.2308\n",
      "Epoch 110/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 14s 598ms/step - loss: 65.1390 - accuracy: 0.2558 - val_loss: 63.6678 - val_accuracy: 0.2857\n",
      "Epoch 111/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 62.6546 - accuracy: 0.2696 - val_loss: 61.3995 - val_accuracy: 0.3077\n",
      "Epoch 112/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 60.5157 - accuracy: 0.2655 - val_loss: 59.7290 - val_accuracy: 0.2033\n",
      "Epoch 113/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 58.7249 - accuracy: 0.2586 - val_loss: 58.3048 - val_accuracy: 0.1813\n",
      "Epoch 114/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 57.2314 - accuracy: 0.2613 - val_loss: 56.4656 - val_accuracy: 0.2582\n",
      "Epoch 115/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 56.0669 - accuracy: 0.2462 - val_loss: 55.8428 - val_accuracy: 0.1923\n",
      "Epoch 116/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 55.2263 - accuracy: 0.2201 - val_loss: 55.0142 - val_accuracy: 0.2527\n",
      "Epoch 117/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 54.5784 - accuracy: 0.2256 - val_loss: 54.3336 - val_accuracy: 0.2088\n",
      "Epoch 118/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 54.2102 - accuracy: 0.2201 - val_loss: 54.1542 - val_accuracy: 0.2363\n",
      "Epoch 119/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 54.0190 - accuracy: 0.2228 - val_loss: 53.7805 - val_accuracy: 0.2747\n",
      "Epoch 120/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 53.7389 - accuracy: 0.2490 - val_loss: 54.4457 - val_accuracy: 0.1319\n",
      "Epoch 121/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 53.7107 - accuracy: 0.2242 - val_loss: 53.4699 - val_accuracy: 0.2363\n",
      "Epoch 122/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 53.4636 - accuracy: 0.2435 - val_loss: 53.5426 - val_accuracy: 0.1758\n",
      "Epoch 123/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 53.4390 - accuracy: 0.2352 - val_loss: 53.2780 - val_accuracy: 0.2637\n",
      "Epoch 124/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 53.3218 - accuracy: 0.2297 - val_loss: 53.2052 - val_accuracy: 0.1978\n",
      "Epoch 125/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 53.2502 - accuracy: 0.2407 - val_loss: 53.1838 - val_accuracy: 0.2363\n",
      "Epoch 126/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 53.1574 - accuracy: 0.2627 - val_loss: 53.3179 - val_accuracy: 0.2363\n",
      "Epoch 127/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 53.1401 - accuracy: 0.2256 - val_loss: 53.2932 - val_accuracy: 0.1978\n",
      "Epoch 128/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.9642 - accuracy: 0.2325 - val_loss: 52.9343 - val_accuracy: 0.2198\n",
      "Epoch 129/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.8996 - accuracy: 0.2393 - val_loss: 53.0372 - val_accuracy: 0.1868\n",
      "Epoch 130/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 52.8572 - accuracy: 0.2366 - val_loss: 53.1252 - val_accuracy: 0.2418\n",
      "Epoch 131/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.7644 - accuracy: 0.2407 - val_loss: 52.8765 - val_accuracy: 0.2198\n",
      "Epoch 132/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 52.6838 - accuracy: 0.2531 - val_loss: 52.6456 - val_accuracy: 0.2857\n",
      "Epoch 133/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.6373 - accuracy: 0.2779 - val_loss: 52.8365 - val_accuracy: 0.1923\n",
      "Epoch 134/400\n",
      "23/23 [==============================] - 14s 597ms/step - loss: 52.5680 - accuracy: 0.2517 - val_loss: 52.6301 - val_accuracy: 0.2582\n",
      "Epoch 135/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 52.5666 - accuracy: 0.2462 - val_loss: 52.6155 - val_accuracy: 0.2088\n",
      "Epoch 136/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 52.4335 - accuracy: 0.2847 - val_loss: 52.6734 - val_accuracy: 0.2143\n",
      "Epoch 137/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.4358 - accuracy: 0.2407 - val_loss: 52.3613 - val_accuracy: 0.2363\n",
      "Epoch 138/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.3217 - accuracy: 0.2517 - val_loss: 52.3123 - val_accuracy: 0.2802\n",
      "Epoch 139/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.2827 - accuracy: 0.2490 - val_loss: 52.2539 - val_accuracy: 0.3462\n",
      "Epoch 140/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.1560 - accuracy: 0.2448 - val_loss: 52.3316 - val_accuracy: 0.2527\n",
      "Epoch 141/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.1413 - accuracy: 0.2834 - val_loss: 52.2521 - val_accuracy: 0.2363\n",
      "Epoch 142/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.0845 - accuracy: 0.2724 - val_loss: 52.4330 - val_accuracy: 0.2308\n",
      "Epoch 143/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 52.0799 - accuracy: 0.2490 - val_loss: 52.1106 - val_accuracy: 0.2418\n",
      "Epoch 144/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.9530 - accuracy: 0.2930 - val_loss: 51.9208 - val_accuracy: 0.2637\n",
      "Epoch 145/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.9516 - accuracy: 0.2545 - val_loss: 51.8486 - val_accuracy: 0.2143\n",
      "Epoch 146/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.9073 - accuracy: 0.2916 - val_loss: 52.1920 - val_accuracy: 0.2473\n",
      "Epoch 147/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.8442 - accuracy: 0.2792 - val_loss: 51.8102 - val_accuracy: 0.2857\n",
      "Epoch 148/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.7795 - accuracy: 0.2944 - val_loss: 51.7814 - val_accuracy: 0.2692\n",
      "Epoch 149/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.6926 - accuracy: 0.3109 - val_loss: 51.7867 - val_accuracy: 0.1923\n",
      "Epoch 150/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.6493 - accuracy: 0.2944 - val_loss: 51.7751 - val_accuracy: 0.2692\n",
      "Epoch 151/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.5925 - accuracy: 0.2847 - val_loss: 51.8374 - val_accuracy: 0.2473\n",
      "Epoch 152/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.5523 - accuracy: 0.2889 - val_loss: 51.4743 - val_accuracy: 0.2692\n",
      "Epoch 153/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.5036 - accuracy: 0.2847 - val_loss: 51.5257 - val_accuracy: 0.2473\n",
      "Epoch 154/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.4132 - accuracy: 0.2971 - val_loss: 51.5522 - val_accuracy: 0.2363\n",
      "Epoch 155/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.3795 - accuracy: 0.3260 - val_loss: 51.4832 - val_accuracy: 0.2363\n",
      "Epoch 156/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.3794 - accuracy: 0.3012 - val_loss: 51.4085 - val_accuracy: 0.2637\n",
      "Epoch 157/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.2747 - accuracy: 0.3109 - val_loss: 51.4783 - val_accuracy: 0.1868\n",
      "Epoch 158/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 51.2183 - accuracy: 0.3054 - val_loss: 51.3375 - val_accuracy: 0.1978\n",
      "Epoch 159/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.1608 - accuracy: 0.3109 - val_loss: 51.3082 - val_accuracy: 0.2692\n",
      "Epoch 160/400\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 51.1369 - accuracy: 0.3563 - val_loss: 51.1732 - val_accuracy: 0.2637\n",
      "Epoch 161/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 51.0672 - accuracy: 0.3219 - val_loss: 51.0648 - val_accuracy: 0.3077\n",
      "Epoch 162/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.0080 - accuracy: 0.3067 - val_loss: 51.1455 - val_accuracy: 0.2308\n",
      "Epoch 163/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 51.0000 - accuracy: 0.3054 - val_loss: 51.4074 - val_accuracy: 0.2967\n",
      "Epoch 164/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.9830 - accuracy: 0.3164 - val_loss: 51.0350 - val_accuracy: 0.3077\n",
      "Epoch 165/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.8522 - accuracy: 0.3191 - val_loss: 51.2615 - val_accuracy: 0.1923\n",
      "Epoch 166/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.8484 - accuracy: 0.2930 - val_loss: 50.7650 - val_accuracy: 0.2857\n",
      "Epoch 167/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.7448 - accuracy: 0.3329 - val_loss: 50.8465 - val_accuracy: 0.2582\n",
      "Epoch 168/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.7070 - accuracy: 0.3287 - val_loss: 50.7086 - val_accuracy: 0.3022\n",
      "Epoch 169/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.6150 - accuracy: 0.3521 - val_loss: 50.7274 - val_accuracy: 0.3132\n",
      "Epoch 170/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.6487 - accuracy: 0.3411 - val_loss: 51.0497 - val_accuracy: 0.2473\n",
      "Epoch 171/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.5891 - accuracy: 0.3136 - val_loss: 50.7514 - val_accuracy: 0.2802\n",
      "Epoch 172/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.5179 - accuracy: 0.3164 - val_loss: 50.6932 - val_accuracy: 0.3626\n",
      "Epoch 173/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.4714 - accuracy: 0.3508 - val_loss: 50.4899 - val_accuracy: 0.3297\n",
      "Epoch 174/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.3745 - accuracy: 0.3439 - val_loss: 50.7974 - val_accuracy: 0.2363\n",
      "Epoch 175/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.4103 - accuracy: 0.3287 - val_loss: 50.4370 - val_accuracy: 0.2747\n",
      "Epoch 176/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.2655 - accuracy: 0.3466 - val_loss: 50.3989 - val_accuracy: 0.2912\n",
      "Epoch 177/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.2212 - accuracy: 0.3590 - val_loss: 50.2581 - val_accuracy: 0.3352\n",
      "Epoch 178/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 50.1923 - accuracy: 0.3659 - val_loss: 50.4724 - val_accuracy: 0.3242\n",
      "Epoch 179/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.1769 - accuracy: 0.3425 - val_loss: 50.3357 - val_accuracy: 0.2802\n",
      "Epoch 180/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.1595 - accuracy: 0.3576 - val_loss: 50.0828 - val_accuracy: 0.3132\n",
      "Epoch 181/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 50.0495 - accuracy: 0.3343 - val_loss: 50.1123 - val_accuracy: 0.3791\n",
      "Epoch 182/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 49.9919 - accuracy: 0.3535 - val_loss: 50.4601 - val_accuracy: 0.2857\n",
      "Epoch 183/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 50.0016 - accuracy: 0.3508 - val_loss: 50.0481 - val_accuracy: 0.3022\n",
      "Epoch 184/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.8714 - accuracy: 0.3728 - val_loss: 50.0560 - val_accuracy: 0.3297\n",
      "Epoch 185/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.8415 - accuracy: 0.3576 - val_loss: 50.1035 - val_accuracy: 0.2967\n",
      "Epoch 186/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.8285 - accuracy: 0.3549 - val_loss: 49.8734 - val_accuracy: 0.3407\n",
      "Epoch 187/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.7659 - accuracy: 0.3618 - val_loss: 49.7702 - val_accuracy: 0.3626\n",
      "Epoch 188/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.7571 - accuracy: 0.3728 - val_loss: 49.8334 - val_accuracy: 0.2967\n",
      "Epoch 189/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.6343 - accuracy: 0.3934 - val_loss: 49.8412 - val_accuracy: 0.2747\n",
      "Epoch 190/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 49.6676 - accuracy: 0.3700 - val_loss: 49.8582 - val_accuracy: 0.2857\n",
      "Epoch 191/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.5681 - accuracy: 0.3714 - val_loss: 49.8658 - val_accuracy: 0.3242\n",
      "Epoch 192/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.5830 - accuracy: 0.3838 - val_loss: 49.8781 - val_accuracy: 0.2308\n",
      "Epoch 193/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.5496 - accuracy: 0.3494 - val_loss: 49.6312 - val_accuracy: 0.3242\n",
      "Epoch 194/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.4537 - accuracy: 0.3645 - val_loss: 49.6485 - val_accuracy: 0.3407\n",
      "Epoch 195/400\n",
      "23/23 [==============================] - 14s 605ms/step - loss: 49.4382 - accuracy: 0.3604 - val_loss: 49.5394 - val_accuracy: 0.3077\n",
      "Epoch 196/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 49.2966 - accuracy: 0.4072 - val_loss: 49.4116 - val_accuracy: 0.3516\n",
      "Epoch 197/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.3176 - accuracy: 0.4003 - val_loss: 49.4922 - val_accuracy: 0.3352\n",
      "Epoch 198/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 49.2925 - accuracy: 0.4127 - val_loss: 49.3349 - val_accuracy: 0.2912\n",
      "Epoch 199/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.2085 - accuracy: 0.3590 - val_loss: 49.3201 - val_accuracy: 0.2912\n",
      "Epoch 200/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.1438 - accuracy: 0.3851 - val_loss: 49.3318 - val_accuracy: 0.3297\n",
      "Epoch 201/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 49.0659 - accuracy: 0.4154 - val_loss: 49.2013 - val_accuracy: 0.3407\n",
      "Epoch 202/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.0928 - accuracy: 0.3906 - val_loss: 49.2114 - val_accuracy: 0.3132\n",
      "Epoch 203/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 49.0494 - accuracy: 0.3893 - val_loss: 49.0410 - val_accuracy: 0.3462\n",
      "Epoch 204/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.9963 - accuracy: 0.3686 - val_loss: 49.0376 - val_accuracy: 0.3297\n",
      "Epoch 205/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.8844 - accuracy: 0.4044 - val_loss: 48.9604 - val_accuracy: 0.3077\n",
      "Epoch 206/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.8567 - accuracy: 0.3865 - val_loss: 49.0993 - val_accuracy: 0.3516\n",
      "Epoch 207/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 48.8415 - accuracy: 0.3865 - val_loss: 49.0784 - val_accuracy: 0.3571\n",
      "Epoch 208/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 48.8273 - accuracy: 0.3865 - val_loss: 48.7205 - val_accuracy: 0.3791\n",
      "Epoch 209/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.6846 - accuracy: 0.4085 - val_loss: 48.8505 - val_accuracy: 0.3242\n",
      "Epoch 210/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.7275 - accuracy: 0.4017 - val_loss: 48.9383 - val_accuracy: 0.3626\n",
      "Epoch 211/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 48.7038 - accuracy: 0.3865 - val_loss: 48.7657 - val_accuracy: 0.3516\n",
      "Epoch 212/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.5938 - accuracy: 0.4182 - val_loss: 49.1855 - val_accuracy: 0.2418\n",
      "Epoch 213/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.5917 - accuracy: 0.4127 - val_loss: 48.7085 - val_accuracy: 0.3297\n",
      "Epoch 214/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.4967 - accuracy: 0.4113 - val_loss: 48.9241 - val_accuracy: 0.3242\n",
      "Epoch 215/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 48.4739 - accuracy: 0.4195 - val_loss: 48.6397 - val_accuracy: 0.3846\n",
      "Epoch 216/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.4650 - accuracy: 0.3893 - val_loss: 48.5493 - val_accuracy: 0.3187\n",
      "Epoch 217/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 48.3370 - accuracy: 0.4278 - val_loss: 48.4527 - val_accuracy: 0.3846\n",
      "Epoch 218/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 48.3713 - accuracy: 0.4264 - val_loss: 48.5618 - val_accuracy: 0.3187\n",
      "Epoch 219/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 48.2673 - accuracy: 0.4209 - val_loss: 48.4902 - val_accuracy: 0.3297\n",
      "Epoch 220/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 14s 600ms/step - loss: 48.1505 - accuracy: 0.4250 - val_loss: 48.5695 - val_accuracy: 0.3187\n",
      "Epoch 221/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.1511 - accuracy: 0.4539 - val_loss: 48.5085 - val_accuracy: 0.3352\n",
      "Epoch 222/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.1712 - accuracy: 0.4470 - val_loss: 48.2348 - val_accuracy: 0.3681\n",
      "Epoch 223/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.1055 - accuracy: 0.4470 - val_loss: 48.2943 - val_accuracy: 0.3352\n",
      "Epoch 224/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 48.0226 - accuracy: 0.4182 - val_loss: 48.2940 - val_accuracy: 0.3626\n",
      "Epoch 225/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.9797 - accuracy: 0.4374 - val_loss: 48.3456 - val_accuracy: 0.2967\n",
      "Epoch 226/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.9615 - accuracy: 0.4360 - val_loss: 48.0973 - val_accuracy: 0.3187\n",
      "Epoch 227/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.8890 - accuracy: 0.4223 - val_loss: 48.2580 - val_accuracy: 0.3132\n",
      "Epoch 228/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.8822 - accuracy: 0.4388 - val_loss: 48.1401 - val_accuracy: 0.3242\n",
      "Epoch 229/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 47.8400 - accuracy: 0.4429 - val_loss: 47.8891 - val_accuracy: 0.3352\n",
      "Epoch 230/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 47.7594 - accuracy: 0.4457 - val_loss: 48.0398 - val_accuracy: 0.3187\n",
      "Epoch 231/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.7488 - accuracy: 0.4017 - val_loss: 47.8495 - val_accuracy: 0.3736\n",
      "Epoch 232/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.6874 - accuracy: 0.4539 - val_loss: 47.8957 - val_accuracy: 0.3516\n",
      "Epoch 233/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.6320 - accuracy: 0.4443 - val_loss: 47.8878 - val_accuracy: 0.3791\n",
      "Epoch 234/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.6242 - accuracy: 0.4223 - val_loss: 47.9431 - val_accuracy: 0.3516\n",
      "Epoch 235/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 47.5781 - accuracy: 0.4470 - val_loss: 47.6804 - val_accuracy: 0.3846\n",
      "Epoch 236/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 47.5138 - accuracy: 0.4580 - val_loss: 47.7048 - val_accuracy: 0.4176\n",
      "Epoch 237/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 47.5535 - accuracy: 0.4677 - val_loss: 47.6405 - val_accuracy: 0.3901\n",
      "Epoch 238/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.4201 - accuracy: 0.4704 - val_loss: 47.5196 - val_accuracy: 0.3462\n",
      "Epoch 239/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 47.3967 - accuracy: 0.4567 - val_loss: 47.7055 - val_accuracy: 0.3297\n",
      "Epoch 240/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.3026 - accuracy: 0.4718 - val_loss: 47.3996 - val_accuracy: 0.3681\n",
      "Epoch 241/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 47.2463 - accuracy: 0.4594 - val_loss: 47.3706 - val_accuracy: 0.4176\n",
      "Epoch 242/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.2449 - accuracy: 0.4801 - val_loss: 47.5152 - val_accuracy: 0.3681\n",
      "Epoch 243/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.1343 - accuracy: 0.4952 - val_loss: 47.2997 - val_accuracy: 0.3791\n",
      "Epoch 244/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.1212 - accuracy: 0.4704 - val_loss: 47.3212 - val_accuracy: 0.3626\n",
      "Epoch 245/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.1107 - accuracy: 0.4704 - val_loss: 47.4523 - val_accuracy: 0.3077\n",
      "Epoch 246/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 47.0801 - accuracy: 0.4402 - val_loss: 47.3393 - val_accuracy: 0.3516\n",
      "Epoch 247/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 47.0821 - accuracy: 0.4663 - val_loss: 47.2889 - val_accuracy: 0.3462\n",
      "Epoch 248/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.9680 - accuracy: 0.4608 - val_loss: 47.2139 - val_accuracy: 0.3571\n",
      "Epoch 249/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.8577 - accuracy: 0.4993 - val_loss: 47.2081 - val_accuracy: 0.4011\n",
      "Epoch 250/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.9041 - accuracy: 0.4814 - val_loss: 47.1249 - val_accuracy: 0.3846\n",
      "Epoch 251/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.8594 - accuracy: 0.4746 - val_loss: 47.1358 - val_accuracy: 0.3187\n",
      "Epoch 252/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.7709 - accuracy: 0.4869 - val_loss: 47.0858 - val_accuracy: 0.3846\n",
      "Epoch 253/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.7329 - accuracy: 0.4883 - val_loss: 46.8774 - val_accuracy: 0.4176\n",
      "Epoch 254/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.6785 - accuracy: 0.5048 - val_loss: 46.9510 - val_accuracy: 0.3187\n",
      "Epoch 255/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.7033 - accuracy: 0.4594 - val_loss: 46.7504 - val_accuracy: 0.4396\n",
      "Epoch 256/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.5862 - accuracy: 0.4869 - val_loss: 46.6998 - val_accuracy: 0.4286\n",
      "Epoch 257/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.5796 - accuracy: 0.4828 - val_loss: 46.8674 - val_accuracy: 0.3901\n",
      "Epoch 258/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 46.6232 - accuracy: 0.4787 - val_loss: 46.8805 - val_accuracy: 0.3956\n",
      "Epoch 259/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.5190 - accuracy: 0.4869 - val_loss: 46.5918 - val_accuracy: 0.4011\n",
      "Epoch 260/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 46.4287 - accuracy: 0.5034 - val_loss: 46.5277 - val_accuracy: 0.4560\n",
      "Epoch 261/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.3662 - accuracy: 0.5048 - val_loss: 46.8684 - val_accuracy: 0.3626\n",
      "Epoch 262/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.3922 - accuracy: 0.5103 - val_loss: 46.9906 - val_accuracy: 0.3407\n",
      "Epoch 263/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.3263 - accuracy: 0.4883 - val_loss: 46.4243 - val_accuracy: 0.3736\n",
      "Epoch 264/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.2823 - accuracy: 0.5144 - val_loss: 46.5935 - val_accuracy: 0.4066\n",
      "Epoch 265/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 46.2028 - accuracy: 0.5048 - val_loss: 46.4977 - val_accuracy: 0.4121\n",
      "Epoch 266/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 46.1611 - accuracy: 0.5021 - val_loss: 46.5654 - val_accuracy: 0.4066\n",
      "Epoch 267/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.1476 - accuracy: 0.5268 - val_loss: 46.4124 - val_accuracy: 0.3626\n",
      "Epoch 268/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 46.0297 - accuracy: 0.5447 - val_loss: 46.5596 - val_accuracy: 0.3901\n",
      "Epoch 269/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 46.1343 - accuracy: 0.5021 - val_loss: 46.2878 - val_accuracy: 0.3736\n",
      "Epoch 270/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.9650 - accuracy: 0.4952 - val_loss: 46.0082 - val_accuracy: 0.4286\n",
      "Epoch 271/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 45.9281 - accuracy: 0.5199 - val_loss: 46.1838 - val_accuracy: 0.4066\n",
      "Epoch 272/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 45.9421 - accuracy: 0.5034 - val_loss: 46.2654 - val_accuracy: 0.3846\n",
      "Epoch 273/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.8839 - accuracy: 0.5007 - val_loss: 46.1738 - val_accuracy: 0.4066\n",
      "Epoch 274/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 45.8600 - accuracy: 0.5378 - val_loss: 46.1645 - val_accuracy: 0.4011\n",
      "Epoch 275/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.8038 - accuracy: 0.5296 - val_loss: 46.2471 - val_accuracy: 0.4066\n",
      "Epoch 276/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 45.7809 - accuracy: 0.5186 - val_loss: 46.2460 - val_accuracy: 0.3791\n",
      "Epoch 277/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.6562 - accuracy: 0.5323 - val_loss: 46.0487 - val_accuracy: 0.3901\n",
      "Epoch 278/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.7276 - accuracy: 0.5021 - val_loss: 45.9963 - val_accuracy: 0.4176\n",
      "Epoch 279/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.6210 - accuracy: 0.5640 - val_loss: 45.8670 - val_accuracy: 0.3956\n",
      "Epoch 280/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.6136 - accuracy: 0.5213 - val_loss: 45.9594 - val_accuracy: 0.3791\n",
      "Epoch 281/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.4950 - accuracy: 0.5227 - val_loss: 45.7663 - val_accuracy: 0.4341\n",
      "Epoch 282/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.5155 - accuracy: 0.5131 - val_loss: 45.7771 - val_accuracy: 0.3352\n",
      "Epoch 283/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 45.4249 - accuracy: 0.5337 - val_loss: 45.7170 - val_accuracy: 0.4011\n",
      "Epoch 284/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.3413 - accuracy: 0.5378 - val_loss: 45.7411 - val_accuracy: 0.4396\n",
      "Epoch 285/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.3049 - accuracy: 0.5763 - val_loss: 45.6222 - val_accuracy: 0.4121\n",
      "Epoch 286/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.3328 - accuracy: 0.5475 - val_loss: 45.4785 - val_accuracy: 0.3846\n",
      "Epoch 287/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.2702 - accuracy: 0.5488 - val_loss: 45.4211 - val_accuracy: 0.4066\n",
      "Epoch 288/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.2163 - accuracy: 0.5296 - val_loss: 45.7750 - val_accuracy: 0.3791\n",
      "Epoch 289/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.1504 - accuracy: 0.5942 - val_loss: 45.6288 - val_accuracy: 0.4341\n",
      "Epoch 290/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.1199 - accuracy: 0.5585 - val_loss: 45.4385 - val_accuracy: 0.4066\n",
      "Epoch 291/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 45.0421 - accuracy: 0.5502 - val_loss: 45.5256 - val_accuracy: 0.4286\n",
      "Epoch 292/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.0042 - accuracy: 0.5612 - val_loss: 45.3678 - val_accuracy: 0.3681\n",
      "Epoch 293/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 45.0261 - accuracy: 0.5392 - val_loss: 45.2079 - val_accuracy: 0.4451\n",
      "Epoch 294/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.8725 - accuracy: 0.5695 - val_loss: 45.1953 - val_accuracy: 0.4780\n",
      "Epoch 295/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.8873 - accuracy: 0.5516 - val_loss: 45.1038 - val_accuracy: 0.4121\n",
      "Epoch 296/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.9066 - accuracy: 0.5502 - val_loss: 45.3472 - val_accuracy: 0.4176\n",
      "Epoch 297/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 44.8273 - accuracy: 0.5571 - val_loss: 45.1703 - val_accuracy: 0.4231\n",
      "Epoch 298/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.8809 - accuracy: 0.5530 - val_loss: 45.1897 - val_accuracy: 0.4505\n",
      "Epoch 299/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.6739 - accuracy: 0.5763 - val_loss: 45.0071 - val_accuracy: 0.4670\n",
      "Epoch 300/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.7222 - accuracy: 0.5667 - val_loss: 44.9920 - val_accuracy: 0.4725\n",
      "Epoch 301/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.6082 - accuracy: 0.5722 - val_loss: 45.0286 - val_accuracy: 0.4286\n",
      "Epoch 302/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.5921 - accuracy: 0.5626 - val_loss: 44.8077 - val_accuracy: 0.4615\n",
      "Epoch 303/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.5387 - accuracy: 0.5722 - val_loss: 45.0981 - val_accuracy: 0.4341\n",
      "Epoch 304/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 44.6179 - accuracy: 0.5832 - val_loss: 45.0022 - val_accuracy: 0.4231\n",
      "Epoch 305/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.5024 - accuracy: 0.5681 - val_loss: 44.7990 - val_accuracy: 0.3956\n",
      "Epoch 306/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.4783 - accuracy: 0.5695 - val_loss: 44.9559 - val_accuracy: 0.4505\n",
      "Epoch 307/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 44.4489 - accuracy: 0.5516 - val_loss: 44.6318 - val_accuracy: 0.4231\n",
      "Epoch 308/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.3985 - accuracy: 0.5530 - val_loss: 44.7078 - val_accuracy: 0.4286\n",
      "Epoch 309/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.3349 - accuracy: 0.5942 - val_loss: 44.9799 - val_accuracy: 0.3846\n",
      "Epoch 310/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.3685 - accuracy: 0.5571 - val_loss: 44.7969 - val_accuracy: 0.4286\n",
      "Epoch 311/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.2656 - accuracy: 0.5598 - val_loss: 44.5732 - val_accuracy: 0.4286\n",
      "Epoch 312/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 44.1755 - accuracy: 0.5915 - val_loss: 44.8124 - val_accuracy: 0.4341\n",
      "Epoch 313/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.1860 - accuracy: 0.5502 - val_loss: 44.5228 - val_accuracy: 0.4615\n",
      "Epoch 314/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 44.0319 - accuracy: 0.5956 - val_loss: 44.3665 - val_accuracy: 0.4670\n",
      "Epoch 315/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.9575 - accuracy: 0.5997 - val_loss: 44.3961 - val_accuracy: 0.4560\n",
      "Epoch 316/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 44.0063 - accuracy: 0.6107 - val_loss: 44.3414 - val_accuracy: 0.4451\n",
      "Epoch 317/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.9559 - accuracy: 0.6272 - val_loss: 44.4795 - val_accuracy: 0.4670\n",
      "Epoch 318/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 43.9995 - accuracy: 0.5777 - val_loss: 44.3531 - val_accuracy: 0.4615\n",
      "Epoch 319/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.7919 - accuracy: 0.6121 - val_loss: 44.5264 - val_accuracy: 0.3462\n",
      "Epoch 320/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 43.7854 - accuracy: 0.5873 - val_loss: 44.3038 - val_accuracy: 0.4176\n",
      "Epoch 321/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.7691 - accuracy: 0.5915 - val_loss: 44.3129 - val_accuracy: 0.4451\n",
      "Epoch 322/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 43.8190 - accuracy: 0.5887 - val_loss: 44.1331 - val_accuracy: 0.4176\n",
      "Epoch 323/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.7514 - accuracy: 0.6162 - val_loss: 44.3335 - val_accuracy: 0.4341\n",
      "Epoch 324/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.7487 - accuracy: 0.5818 - val_loss: 44.1535 - val_accuracy: 0.4560\n",
      "Epoch 325/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.5624 - accuracy: 0.6121 - val_loss: 44.2807 - val_accuracy: 0.4121\n",
      "Epoch 326/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.6358 - accuracy: 0.6107 - val_loss: 44.0003 - val_accuracy: 0.4725\n",
      "Epoch 327/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.5902 - accuracy: 0.5997 - val_loss: 44.0154 - val_accuracy: 0.4505\n",
      "Epoch 328/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.4386 - accuracy: 0.6272 - val_loss: 44.2216 - val_accuracy: 0.4231\n",
      "Epoch 329/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.4378 - accuracy: 0.6217 - val_loss: 43.8992 - val_accuracy: 0.4560\n",
      "Epoch 330/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 14s 599ms/step - loss: 43.4180 - accuracy: 0.6162 - val_loss: 43.9604 - val_accuracy: 0.4341\n",
      "Epoch 331/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 43.4650 - accuracy: 0.6011 - val_loss: 43.8554 - val_accuracy: 0.4396\n",
      "Epoch 332/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.2513 - accuracy: 0.6245 - val_loss: 43.7297 - val_accuracy: 0.4890\n",
      "Epoch 333/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.2660 - accuracy: 0.6217 - val_loss: 43.7063 - val_accuracy: 0.4505\n",
      "Epoch 334/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.2352 - accuracy: 0.6204 - val_loss: 43.7198 - val_accuracy: 0.4176\n",
      "Epoch 335/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 43.1541 - accuracy: 0.6162 - val_loss: 43.5540 - val_accuracy: 0.4341\n",
      "Epoch 336/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.1765 - accuracy: 0.6094 - val_loss: 43.7958 - val_accuracy: 0.3901\n",
      "Epoch 337/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.1125 - accuracy: 0.6162 - val_loss: 43.6569 - val_accuracy: 0.4121\n",
      "Epoch 338/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.9572 - accuracy: 0.6823 - val_loss: 43.6912 - val_accuracy: 0.4615\n",
      "Epoch 339/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.9958 - accuracy: 0.6410 - val_loss: 43.9117 - val_accuracy: 0.3846\n",
      "Epoch 340/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 43.0840 - accuracy: 0.6245 - val_loss: 43.6952 - val_accuracy: 0.5000\n",
      "Epoch 341/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 43.0589 - accuracy: 0.6382 - val_loss: 43.3179 - val_accuracy: 0.4396\n",
      "Epoch 342/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.9216 - accuracy: 0.6382 - val_loss: 43.4584 - val_accuracy: 0.4725\n",
      "Epoch 343/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.8870 - accuracy: 0.6451 - val_loss: 43.4515 - val_accuracy: 0.4505\n",
      "Epoch 344/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 42.8037 - accuracy: 0.6382 - val_loss: 43.5183 - val_accuracy: 0.4725\n",
      "Epoch 345/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.7092 - accuracy: 0.6520 - val_loss: 43.2306 - val_accuracy: 0.4945\n",
      "Epoch 346/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.8007 - accuracy: 0.6286 - val_loss: 43.2518 - val_accuracy: 0.4231\n",
      "Epoch 347/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.6867 - accuracy: 0.6286 - val_loss: 43.2601 - val_accuracy: 0.4286\n",
      "Epoch 348/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.6891 - accuracy: 0.6355 - val_loss: 43.4899 - val_accuracy: 0.3791\n",
      "Epoch 349/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.6586 - accuracy: 0.6231 - val_loss: 43.1665 - val_accuracy: 0.4560\n",
      "Epoch 350/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.5663 - accuracy: 0.6451 - val_loss: 43.1344 - val_accuracy: 0.4670\n",
      "Epoch 351/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.5482 - accuracy: 0.6341 - val_loss: 43.0271 - val_accuracy: 0.4670\n",
      "Epoch 352/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 42.5176 - accuracy: 0.6314 - val_loss: 43.0702 - val_accuracy: 0.4396\n",
      "Epoch 353/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 42.3917 - accuracy: 0.6410 - val_loss: 42.7525 - val_accuracy: 0.4176\n",
      "Epoch 354/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.3407 - accuracy: 0.6492 - val_loss: 43.0668 - val_accuracy: 0.3901\n",
      "Epoch 355/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.3331 - accuracy: 0.6410 - val_loss: 42.8408 - val_accuracy: 0.4780\n",
      "Epoch 356/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.3850 - accuracy: 0.6190 - val_loss: 43.0172 - val_accuracy: 0.4835\n",
      "Epoch 357/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.2654 - accuracy: 0.6341 - val_loss: 42.8143 - val_accuracy: 0.4615\n",
      "Epoch 358/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.2113 - accuracy: 0.6919 - val_loss: 42.8725 - val_accuracy: 0.4725\n",
      "Epoch 359/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 42.2718 - accuracy: 0.6561 - val_loss: 42.8706 - val_accuracy: 0.4780\n",
      "Epoch 360/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 42.1105 - accuracy: 0.6616 - val_loss: 42.7298 - val_accuracy: 0.4670\n",
      "Epoch 361/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.0708 - accuracy: 0.6754 - val_loss: 42.7712 - val_accuracy: 0.4890\n",
      "Epoch 362/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 42.0260 - accuracy: 0.6713 - val_loss: 42.6660 - val_accuracy: 0.4890\n",
      "Epoch 363/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.9709 - accuracy: 0.6671 - val_loss: 42.6209 - val_accuracy: 0.4780\n",
      "Epoch 364/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.9784 - accuracy: 0.6836 - val_loss: 42.4836 - val_accuracy: 0.4835\n",
      "Epoch 365/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.9951 - accuracy: 0.6479 - val_loss: 42.4390 - val_accuracy: 0.4945\n",
      "Epoch 366/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.9289 - accuracy: 0.6410 - val_loss: 42.4326 - val_accuracy: 0.4780\n",
      "Epoch 367/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.8908 - accuracy: 0.6602 - val_loss: 42.4945 - val_accuracy: 0.5055\n",
      "Epoch 368/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.8046 - accuracy: 0.6740 - val_loss: 42.3494 - val_accuracy: 0.4835\n",
      "Epoch 369/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 41.9023 - accuracy: 0.6561 - val_loss: 42.8090 - val_accuracy: 0.4121\n",
      "Epoch 370/400\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 41.8086 - accuracy: 0.6823 - val_loss: 42.2683 - val_accuracy: 0.4670\n",
      "Epoch 371/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.6998 - accuracy: 0.6933 - val_loss: 42.3720 - val_accuracy: 0.5110\n",
      "Epoch 372/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.6552 - accuracy: 0.6726 - val_loss: 42.3003 - val_accuracy: 0.4615\n",
      "Epoch 373/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.5832 - accuracy: 0.6685 - val_loss: 42.6527 - val_accuracy: 0.4615\n",
      "Epoch 374/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.7701 - accuracy: 0.6616 - val_loss: 42.1117 - val_accuracy: 0.4835\n",
      "Epoch 375/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.5403 - accuracy: 0.7043 - val_loss: 42.2596 - val_accuracy: 0.4890\n",
      "Epoch 376/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.4550 - accuracy: 0.6919 - val_loss: 42.1844 - val_accuracy: 0.5000\n",
      "Epoch 377/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.5373 - accuracy: 0.6671 - val_loss: 41.8979 - val_accuracy: 0.4505\n",
      "Epoch 378/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.3613 - accuracy: 0.6946 - val_loss: 42.1105 - val_accuracy: 0.4505\n",
      "Epoch 379/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 41.3460 - accuracy: 0.7001 - val_loss: 41.9104 - val_accuracy: 0.4890\n",
      "Epoch 380/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.3263 - accuracy: 0.6850 - val_loss: 42.0642 - val_accuracy: 0.4670\n",
      "Epoch 381/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.3847 - accuracy: 0.6878 - val_loss: 42.0613 - val_accuracy: 0.4505\n",
      "Epoch 382/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 41.2708 - accuracy: 0.6974 - val_loss: 41.9969 - val_accuracy: 0.4835\n",
      "Epoch 383/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.3007 - accuracy: 0.6740 - val_loss: 42.0583 - val_accuracy: 0.4780\n",
      "Epoch 384/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.1876 - accuracy: 0.6905 - val_loss: 41.8609 - val_accuracy: 0.4945\n",
      "Epoch 385/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.1346 - accuracy: 0.7125 - val_loss: 41.6600 - val_accuracy: 0.5000\n",
      "Epoch 386/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 41.0671 - accuracy: 0.7139 - val_loss: 41.5865 - val_accuracy: 0.4725\n",
      "Epoch 387/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 40.9439 - accuracy: 0.7263 - val_loss: 41.8002 - val_accuracy: 0.5110\n",
      "Epoch 388/400\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 41.1490 - accuracy: 0.6933 - val_loss: 42.0285 - val_accuracy: 0.4615\n",
      "Epoch 389/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.0527 - accuracy: 0.7084 - val_loss: 41.9322 - val_accuracy: 0.4286\n",
      "Epoch 390/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.9801 - accuracy: 0.7084 - val_loss: 42.0886 - val_accuracy: 0.4231\n",
      "Epoch 391/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 41.0715 - accuracy: 0.6933 - val_loss: 41.7298 - val_accuracy: 0.4451\n",
      "Epoch 392/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.8407 - accuracy: 0.7056 - val_loss: 41.4870 - val_accuracy: 0.5000\n",
      "Epoch 393/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 40.8680 - accuracy: 0.7029 - val_loss: 41.4396 - val_accuracy: 0.4945\n",
      "Epoch 394/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 40.8064 - accuracy: 0.7180 - val_loss: 41.8425 - val_accuracy: 0.3736\n",
      "Epoch 395/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.8378 - accuracy: 0.6946 - val_loss: 41.2026 - val_accuracy: 0.4945\n",
      "Epoch 396/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.5741 - accuracy: 0.7276 - val_loss: 41.4960 - val_accuracy: 0.4560\n",
      "Epoch 397/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 40.8267 - accuracy: 0.6891 - val_loss: 41.3951 - val_accuracy: 0.4725\n",
      "Epoch 398/400\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 40.5761 - accuracy: 0.7276 - val_loss: 41.4487 - val_accuracy: 0.4560\n",
      "Epoch 399/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.5204 - accuracy: 0.7414 - val_loss: 41.3637 - val_accuracy: 0.5110\n",
      "Epoch 400/400\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 40.7046 - accuracy: 0.7221 - val_loss: 41.4538 - val_accuracy: 0.4451\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 15s 610ms/step - loss: 39.8253 - accuracy: 0.7400 - val_loss: 40.0904 - val_accuracy: 0.4945\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 39.1525 - accuracy: 0.7978 - val_loss: 39.8545 - val_accuracy: 0.4780\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.9734 - accuracy: 0.7840 - val_loss: 39.7252 - val_accuracy: 0.4945\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.8354 - accuracy: 0.8212 - val_loss: 39.6695 - val_accuracy: 0.5110\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.8176 - accuracy: 0.7978 - val_loss: 39.6466 - val_accuracy: 0.4945\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.7788 - accuracy: 0.7950 - val_loss: 39.5930 - val_accuracy: 0.4890\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.7667 - accuracy: 0.7909 - val_loss: 39.5871 - val_accuracy: 0.4780\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.7074 - accuracy: 0.8157 - val_loss: 39.5778 - val_accuracy: 0.4890\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.6961 - accuracy: 0.8047 - val_loss: 39.5399 - val_accuracy: 0.5110\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.6655 - accuracy: 0.8033 - val_loss: 39.5503 - val_accuracy: 0.4945\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.6867 - accuracy: 0.7950 - val_loss: 39.5147 - val_accuracy: 0.4890\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.6069 - accuracy: 0.8377 - val_loss: 39.5211 - val_accuracy: 0.5110\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.6196 - accuracy: 0.8226 - val_loss: 39.5537 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.6145 - accuracy: 0.8267 - val_loss: 39.5334 - val_accuracy: 0.4670\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.6306 - accuracy: 0.8074 - val_loss: 39.5312 - val_accuracy: 0.5055\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.6113 - accuracy: 0.8047 - val_loss: 39.5315 - val_accuracy: 0.4725\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.6251 - accuracy: 0.8033 - val_loss: 39.5164 - val_accuracy: 0.4945\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.5325 - accuracy: 0.8349 - val_loss: 39.5088 - val_accuracy: 0.5220\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.5701 - accuracy: 0.8226 - val_loss: 39.5048 - val_accuracy: 0.5055\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.5964 - accuracy: 0.8184 - val_loss: 39.5047 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.5716 - accuracy: 0.8253 - val_loss: 39.4852 - val_accuracy: 0.5220\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.5517 - accuracy: 0.8184 - val_loss: 39.4938 - val_accuracy: 0.5110\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.5560 - accuracy: 0.8198 - val_loss: 39.4975 - val_accuracy: 0.5110\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.5636 - accuracy: 0.8212 - val_loss: 39.4938 - val_accuracy: 0.5165\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.5452 - accuracy: 0.8267 - val_loss: 39.5075 - val_accuracy: 0.4945\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.5551 - accuracy: 0.8047 - val_loss: 39.5109 - val_accuracy: 0.4945\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.5168 - accuracy: 0.8308 - val_loss: 39.4925 - val_accuracy: 0.5055\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 14s 620ms/step - loss: 38.5324 - accuracy: 0.8239 - val_loss: 39.4987 - val_accuracy: 0.5165\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.5351 - accuracy: 0.8253 - val_loss: 39.4647 - val_accuracy: 0.5165\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.5310 - accuracy: 0.8129 - val_loss: 39.4839 - val_accuracy: 0.4780\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.4964 - accuracy: 0.8253 - val_loss: 39.4713 - val_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.5354 - accuracy: 0.8198 - val_loss: 39.4484 - val_accuracy: 0.5165\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.4925 - accuracy: 0.8226 - val_loss: 39.4815 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4666 - accuracy: 0.8377 - val_loss: 39.4822 - val_accuracy: 0.4835\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.4934 - accuracy: 0.8198 - val_loss: 39.4694 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4938 - accuracy: 0.8226 - val_loss: 39.4759 - val_accuracy: 0.4890\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.4791 - accuracy: 0.8363 - val_loss: 39.4757 - val_accuracy: 0.4890\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.5142 - accuracy: 0.8006 - val_loss: 39.4716 - val_accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.4818 - accuracy: 0.8336 - val_loss: 39.4617 - val_accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.4531 - accuracy: 0.8336 - val_loss: 39.4466 - val_accuracy: 0.5055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.4480 - accuracy: 0.8253 - val_loss: 39.4643 - val_accuracy: 0.5165\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4755 - accuracy: 0.8336 - val_loss: 39.4503 - val_accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4571 - accuracy: 0.8226 - val_loss: 39.4588 - val_accuracy: 0.5055\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4088 - accuracy: 0.8597 - val_loss: 39.4555 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.4428 - accuracy: 0.8322 - val_loss: 39.4804 - val_accuracy: 0.5055\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 14s 612ms/step - loss: 38.4290 - accuracy: 0.8418 - val_loss: 39.4217 - val_accuracy: 0.5220\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 38.4111 - accuracy: 0.8501 - val_loss: 39.4475 - val_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 14s 607ms/step - loss: 38.4572 - accuracy: 0.8239 - val_loss: 39.4608 - val_accuracy: 0.5055\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 14s 617ms/step - loss: 38.4179 - accuracy: 0.8418 - val_loss: 39.4369 - val_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 38.4373 - accuracy: 0.8212 - val_loss: 39.4758 - val_accuracy: 0.4890\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 38.4512 - accuracy: 0.8198 - val_loss: 39.4309 - val_accuracy: 0.5055\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 38.4323 - accuracy: 0.8212 - val_loss: 39.4314 - val_accuracy: 0.5055\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 14s 603ms/step - loss: 38.4056 - accuracy: 0.8349 - val_loss: 39.4341 - val_accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 38.3856 - accuracy: 0.8336 - val_loss: 39.4687 - val_accuracy: 0.5055\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 14s 610ms/step - loss: 38.3816 - accuracy: 0.8184 - val_loss: 39.4443 - val_accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 14s 610ms/step - loss: 38.3560 - accuracy: 0.8542 - val_loss: 39.4588 - val_accuracy: 0.5110\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 14s 608ms/step - loss: 38.3884 - accuracy: 0.8294 - val_loss: 39.4720 - val_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 38.3814 - accuracy: 0.8473 - val_loss: 39.4075 - val_accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 38.3710 - accuracy: 0.8569 - val_loss: 39.4631 - val_accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 38.3535 - accuracy: 0.8542 - val_loss: 39.4239 - val_accuracy: 0.4945\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 14s 615ms/step - loss: 38.3873 - accuracy: 0.8336 - val_loss: 39.4476 - val_accuracy: 0.4945\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 14s 628ms/step - loss: 38.3796 - accuracy: 0.8391 - val_loss: 39.4228 - val_accuracy: 0.5165\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 14s 618ms/step - loss: 38.3837 - accuracy: 0.8487 - val_loss: 39.4320 - val_accuracy: 0.5055\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 38.3928 - accuracy: 0.8226 - val_loss: 39.4502 - val_accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 38.3625 - accuracy: 0.8349 - val_loss: 39.4265 - val_accuracy: 0.5165\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 38.3968 - accuracy: 0.8129 - val_loss: 39.4048 - val_accuracy: 0.5110\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 14s 617ms/step - loss: 38.3702 - accuracy: 0.8446 - val_loss: 39.3889 - val_accuracy: 0.4945\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 14s 605ms/step - loss: 38.3284 - accuracy: 0.8583 - val_loss: 39.4003 - val_accuracy: 0.5055\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.3279 - accuracy: 0.8556 - val_loss: 39.4092 - val_accuracy: 0.4945\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.3564 - accuracy: 0.8294 - val_loss: 39.4321 - val_accuracy: 0.5055\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.3314 - accuracy: 0.8432 - val_loss: 39.4051 - val_accuracy: 0.5110\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.3347 - accuracy: 0.8514 - val_loss: 39.3861 - val_accuracy: 0.5110\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.3189 - accuracy: 0.8349 - val_loss: 39.3671 - val_accuracy: 0.5110\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.3256 - accuracy: 0.8404 - val_loss: 39.3535 - val_accuracy: 0.5165\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.3628 - accuracy: 0.8267 - val_loss: 39.3670 - val_accuracy: 0.5220\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 14s 612ms/step - loss: 38.3120 - accuracy: 0.8404 - val_loss: 39.3364 - val_accuracy: 0.5220\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 14s 614ms/step - loss: 38.3022 - accuracy: 0.8501 - val_loss: 39.3470 - val_accuracy: 0.5275\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.2891 - accuracy: 0.8446 - val_loss: 39.3901 - val_accuracy: 0.4945\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.3039 - accuracy: 0.8336 - val_loss: 39.3681 - val_accuracy: 0.5055\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.2986 - accuracy: 0.8308 - val_loss: 39.3900 - val_accuracy: 0.5110\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.2828 - accuracy: 0.8501 - val_loss: 39.3838 - val_accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.2725 - accuracy: 0.8542 - val_loss: 39.3714 - val_accuracy: 0.5165\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.3017 - accuracy: 0.8446 - val_loss: 39.3728 - val_accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.2799 - accuracy: 0.8528 - val_loss: 39.3935 - val_accuracy: 0.5165\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 14s 600ms/step - loss: 38.2872 - accuracy: 0.8446 - val_loss: 39.3866 - val_accuracy: 0.5220\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.2860 - accuracy: 0.8446 - val_loss: 39.3600 - val_accuracy: 0.5220\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 38.2991 - accuracy: 0.8391 - val_loss: 39.3894 - val_accuracy: 0.4890\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 14s 621ms/step - loss: 38.2506 - accuracy: 0.8583 - val_loss: 39.3548 - val_accuracy: 0.5275\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 38.2502 - accuracy: 0.8611 - val_loss: 39.3626 - val_accuracy: 0.5220\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 38.2458 - accuracy: 0.8473 - val_loss: 39.3749 - val_accuracy: 0.5110\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 14s 606ms/step - loss: 38.2698 - accuracy: 0.8198 - val_loss: 39.3760 - val_accuracy: 0.5165\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.2595 - accuracy: 0.8432 - val_loss: 39.3546 - val_accuracy: 0.5110\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.2261 - accuracy: 0.8556 - val_loss: 39.3174 - val_accuracy: 0.5110\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.2468 - accuracy: 0.8446 - val_loss: 39.3629 - val_accuracy: 0.5275\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 14s 599ms/step - loss: 38.2103 - accuracy: 0.8569 - val_loss: 39.3457 - val_accuracy: 0.5275\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 38.2456 - accuracy: 0.8363 - val_loss: 39.3352 - val_accuracy: 0.5055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "23/23 [==============================] - 14s 602ms/step - loss: 38.2156 - accuracy: 0.8446 - val_loss: 39.3181 - val_accuracy: 0.5110\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 14s 598ms/step - loss: 38.2038 - accuracy: 0.8652 - val_loss: 39.3403 - val_accuracy: 0.5055\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 14s 601ms/step - loss: 38.2183 - accuracy: 0.8404 - val_loss: 39.3248 - val_accuracy: 0.5055\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 38.1977 - accuracy: 0.8583"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train_videos,train_labels,test_size=0.25,shuffle=True)\n",
    "\n",
    "input_shape =(32,16,112,112,3)\n",
    "output_shape = 20\n",
    "\n",
    "tf.keras.regularizers.L1L2(\n",
    "    l1=0.01, l2=0.1\n",
    ")\n",
    "\n",
    "RGBmodel = Sequential()\n",
    "\n",
    "# model.add(RandomFlip(\"horizontal\"))\n",
    "# model.add(RandomRotation(0.1))\n",
    "# RGBmodel.add(Rescaling(1.0 / 255,input_shape=input_shape[1:]))\n",
    "RGBmodel.add(Conv3D(64,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same',input_shape=input_shape[1:]))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(1, 2, 2),strides =(1,2,2)))    #maxPool-1\n",
    "\n",
    "RGBmodel.add(Conv3D(128,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-2\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3a\n",
    "RGBmodel.add(Conv3D(256,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-3b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-4b\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5a\n",
    "RGBmodel.add(Conv3D(512,kernel_size=(3,3,3),strides=(1, 1, 1),activation='relu',padding = 'same')) #conv3d-5b\n",
    "\n",
    "RGBmodel.add(ZeroPadding3D(padding=(0,1,1)))\n",
    "RGBmodel.add(MaxPooling3D(pool_size=(2, 2, 2),strides =(2,2,2)))\n",
    "RGBmodel.add(Flatten())\n",
    "\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l2'))\n",
    "RGBmodel.add(Dropout(rate=0.5))\n",
    "RGBmodel.add(Dense(units=4096, activation='relu', kernel_regularizer='l1'))\n",
    "\n",
    "\n",
    "RGBmodel.add(Dense(units = output_shape, activation = 'softmax'))\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 400, validation_split = 0.2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.0001)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Print the summary of the model\n",
    "RGBmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGBmodel.save('RGBStream.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGBmodel.ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.0005)\n",
    "RGBmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = opt, metrics = ['accuracy'])\n",
    "RGBmodel.fit(X_train,y_train,batch_size=32,epochs = 50, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
